{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BjbuBM8L_fr_"
   },
   "source": [
    "# **PREDICTING SUCCESS RATE OF ADVERTISEMENTS - TERM PROJECT**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Table Of Contents**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Dataset and Problem Statement](#section1)<br>\n",
    "2. [Importing libraries](#section2)<br>\n",
    "3. [Reading Data](#section3)<br>\n",
    "4. [Pre-profiling of Data](#section4)<br>\n",
    "5. [Profiling using Sweetviz package](#section5)<br>\n",
    "6. [Analysis using Autoviz](#section6)<br>\n",
    "7. [Statistical Analysis of Data](#section7)<br>\n",
    "8. [Exploratory Analysis of Data](#section8)<br>\n",
    "  - 8.1 [Advertisements counts based on relationship status](#section801)<br>\n",
    "  - 8.2 [Advertisements counts for each industry](#section802)<br>\n",
    "  - 8.3 [Advertisements counts for each genre](#section803)<br>\n",
    "  - 8.4 [Advertisements counts gender-wise](#section804)<br>\n",
    "  - 8.5 [Advertisements depending on time of air](#section805)<br>\n",
    "  - 8.6 [Advertisements based on price class](#section806)<br>\n",
    "  - 8.7 [Advertisements based on money back guarantee](#section807)<br>\n",
    "  - 8.8 [Advertisements based on location](#section808)<br>\n",
    "\t- 8.8.1 [Based on genre and runtime](#section8081)<br>\n",
    "\t- 8.8.2 [Based on industry and runtime](#section8082)<br>\n",
    "\t- 8.8.3 [Based on genre and ratings](#section8083)<br>\n",
    "\t- 8.8.4 [Based on industry and ratings](#section8084)<br>\n",
    "  - 8.9 [Advertisements count based on runtime and genre](#section809)<br>\n",
    "  - 8.10 [Advertisements count based on runtime and industry](#section810)<br>\n",
    "  - 8.11 [Advertisements count based on ratings and genre](#section811)<br>\n",
    "  - 8.12 [Advertisements count based on ratings and indstry](#section812)<br>\n",
    "  - 8.13 [Ratings distribution for each genre](#section813)<br>\n",
    "  - 8.14 [Ratings distribution for each industry](#section814)<br>\n",
    "  - 8.15 [Runtime distribution for each genre](#section815)<br>\n",
    "  - 8.16 [Runtime distribution for each industry](#section816)<br>\n",
    "  - 8.17 [Relationship between ratings and runtime](#section817)<br>\n",
    "  - 8.18 [Analysis of columns with respect to target column - netgain](#section818)<br>\n",
    "\t- 8.18.1 [Profitable records analysis](#section8181)<br>\n",
    "\t- 8.18.2 [Loss records analysis](#section8182)<br>\n",
    "9. [Checking data distribution of output variable](#section9)<br>\n",
    "10. [Feature Engineering and Data Transformation](#section10)<br>\n",
    "\t- 10.1 [Combining feature values](#section101)<br>\n",
    "\t- 10.2 [Label Encoding and One Hot Encoding](#section102)<br>\n",
    "\t- 10.3 [Creating bins for the runtime values](#section103)<br>\n",
    "\t- 10.4 [Checking for outliers](#section104)<br>\n",
    "\t\t- 10.4.1 [Using Boxplot](#section10401)<br>\n",
    "\t\t- 10.4.2 [Analyzing using IQR](#section10402)<br>\n",
    "\t\t- 10.4.3 [Analyzing approaches on data imbalance issue](#section10403)<br>\n",
    "\t\t- 10.4.4 [LDA and PCA Transformation](#section10404)<br>\n",
    "11. [Feature Selection](#section11)<br>\n",
    "  - 11.1 [Using Model](#section1101)<br>\n",
    "  - 11.2 [Using selectKBes](#section1102)<br>\n",
    "12. [Pycaret - to analyze the best models on the dataset and features before actual modelling](#section12)<br>\n",
    "13. [Checking the distribution of continuos columns to decide on normalization](#section13)<br>\n",
    "14. [Machine learning - Analyzing baseline models](#section14)<br>\n",
    "  - 14.1 [Modelling on basic dataset - adv_data with stratified - False while splitting the data for test/train](#section1401)<br>\n",
    "  - 14.2 [Modelling on basic dataset - adv_data with stratified - True while splitting the data for test/train](#section1402)<br>\n",
    "  - 14.3 [Modelling on adv_data_1 with stratified - False while splitting the data for test/train](#section1403)<br>\n",
    "  - 14.4 [Modelling on adv_data_1 with stratified - True while splitting the data for test/train](#section1404)<br>\n",
    "  - 14.5 [Modelling on adv_data_2 with stratified - False while splitting the data for test/train](#section1405)<br>\n",
    "  - 14.6 [Modelling on adv_data_2 with stratified - True while splitting the data for test/train](#section1406)<br>\n",
    "  - 14.7 [Summary on baseline model results](#section1407)<br>\n",
    "15. [Machine learning - Analyzing tuned models (and Ensemble models)](#section15)<br>\n",
    "  - 15.1 [Hyper-parameter tuning](#section1501)<br>\n",
    "\t- 15.1.1 [Hyper parameter tuning in Random Forest](#section15011)<br>\n",
    "\t- 15.1.2 [Hyper paramete tuning in Gradient Boosting Classifier](#section15012)<br>\n",
    "\t- 15.1.3 [Hyper parameter tuning for Xtreme Gradient Boosting Classifier](#section15013)<br>\n",
    "\t- 15.1.4 [Hyper parameter tuning for LightGBM Classifier](#section15014)<br>\n",
    "  - 15.2 [Ensemble Techniques](#section1502)<br>\n",
    "\t- 15.2.1 [Voting Classifier](#section15021)<br>\n",
    "\t- 15.2.2 [Stacking](#section15022)<br>\n",
    "16. [Deep Learning](#section16)<br>\n",
    "  - 16.1 [Check the data distribution and if the data is linearly seperable based on output class](#section1601)<br>\n",
    "  - 16.2 [Function to Normalize the data](#section1602)<br>\n",
    "  - 16.3 [Create simple Neural Network first and evaluation with all datasets generated above](#section1603)<br>\n",
    "\t- 16.3.1 [Training the model with 1000 EPOCHS](#section16031)<br>\n",
    "\t- 16.3.2 [Training the model with 500 EPOCHS](#section16032)<br>\n",
    "\t- 16.3.3 [Training the model with 2000 EPOCHS](#section16032)<br>\n",
    "\t- 16.3.4 [Summary of highest accuracies obtained for all 3 epoch values](#section16034)<br>\n",
    "\t- 16.3.5 [Analyzing validation loss for various data generated above](#section16035)<br>\n",
    "  - 16.4 [Creating deep neural networks with hyper parameter optimization](#section1604)<br>\n",
    "  - 16.5 [Experiment with selected configuration from hyper parameter tuning](#section1605)<br>\n",
    "  - 16.6 [Batch Normalization and Weight initializer on models selected above along with different optimizers](#section1606)<br>\n",
    "\t- 16.6.1 [Using Batch Normalization and early stopping](#section16061)<br>\n",
    "\t- 16.6.2 [Applying Kernel initializers](#section16062)<br>\n",
    "  - 16.7 [Analyzing with various Optimizers](#section1607)<br>\n",
    "  - 16.8 [Selecting the final parameters](#section1608)<br>\n",
    "  - 16.9 [Final Model](#section1609)<br>\n",
    "17. [Comparison of Machine Learning Model results and Deep Learning results](#section17)<br>\n",
    "18. [Conclusion](#section18)<br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LBZvOLaVAR2-"
   },
   "source": [
    "<a id=section1></a>\n",
    "## **1. Dataset and Problem Statement**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains information about various advertisements.\n",
    "\n",
    "It is a collection of approximately 26,000 different instances of advertisements of different products aired in different countries.\n",
    "\n",
    "**Input fields**\n",
    "\n",
    "Column|Description\n",
    "---|---\n",
    "id|Unique id for each row\n",
    "relationship_status|The relationship status of the most responsive customers to the advertisement\n",
    "industry|The industry to which the product belonged\n",
    "genre|The type of advertisement\n",
    "targeted_sex|Sex that was mainly targeted for the advertisement\n",
    "averageruntime(minutesper_week)|Minutes per week the advertisement was aired\n",
    "airtime|Time when the advertisement was aired\n",
    "airlocation|Country of origin\n",
    "ratings|Metric out of 1 which represents how much of the targeted demographic watched the advertisement\n",
    "expensive|A general measure of how expensive the product or service is that the ad is discussing\n",
    "moneybackguarantee|Whether or not the product offers a refund in the case of customer dissatisfaction\n",
    "\n",
    "\n",
    "**Target column**\n",
    "\n",
    "Column|Description\n",
    "---|---\n",
    "netgain|Whether the ad will incur a gain or not when sold\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Probelem Staetement -**\n",
    "\n",
    "Using the above dataset, we need to classify whether an ad will be profitable or not\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "So, to accomplish above, will train the data using both Machiner Learning models and Deep learning models and compare which can be better in this case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GLptruYs_5sw"
   },
   "source": [
    "<a id=section2></a>\n",
    "## **2. Importing libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "-iN484QvyYd-",
    "outputId": "abab647b-c9a9-496f-a235-5d69c6db507d"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pandas\n",
    "!pip install --upgrade numpy\n",
    "!pip install --upgrade folium\n",
    "!pip install --upgrade requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TXTooVAR_dq-"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from sklearn.svm import LinearSVC\n",
    "from imblearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier, Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly\n",
    "from plotly.offline import iplot\n",
    "import plotly.graph_objs as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enable_plotly_in_cell():\n",
    "  import IPython\n",
    "  from plotly.offline import init_notebook_mode\n",
    "  display(IPython.core.display.HTML('''<script src=\"/static/components/requirejs/require.js\"></script>'''))\n",
    "  init_notebook_mode(connected=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "-uFBO9Fy-kTN",
    "outputId": "a20d866f-c668-4ad1-dd9c-0696b08f431b"
   },
   "outputs": [],
   "source": [
    "!pip install pandas-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aPudAOZL-my-"
   },
   "outputs": [],
   "source": [
    "from pandas_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "_6LvOyK2kIqZ",
    "outputId": "f41cf3d2-488e-426e-d5c5-01d343a08692"
   },
   "outputs": [],
   "source": [
    "!pip install pycaret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "colab_type": "code",
    "id": "Pa3wghQ4kRkP",
    "outputId": "4774b2ef-dd93-4c37-8351-314b061c6bc6"
   },
   "outputs": [],
   "source": [
    "!pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "colab_type": "code",
    "id": "QlUPvlz1nGiK",
    "outputId": "40c6938d-ba8c-4b8b-daea-39f5b7ad6547"
   },
   "outputs": [],
   "source": [
    "!pip install sweetviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q6VPSUgnotlR"
   },
   "outputs": [],
   "source": [
    "import sweetviz as sv     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 918
    },
    "colab_type": "code",
    "id": "PbAPWiIZ0xmJ",
    "outputId": "a3e8b147-9a38-4767-bebd-2af6eb4bc527"
   },
   "outputs": [],
   "source": [
    "!pip install autoviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V8_Vb8Cz00du"
   },
   "outputs": [],
   "source": [
    "from autoviz.AutoViz_Class import AutoViz_Class\n",
    "\n",
    "AV=AutoViz_Class()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/tensorflow/docs\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras import regularizers\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.plots\n",
    "import tensorflow_docs.modeling\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PxqFGqE1_csy"
   },
   "source": [
    "<a id=section3></a>\n",
    "## **3. Reading Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "vlSiZcJ5VyBi",
    "outputId": "a3c0696b-c0dd-4e9b-a632-5e127cab9ecc"
   },
   "outputs": [],
   "source": [
    "#adv_data=pd.read_csv('/content/advertisement_success.csv')\n",
    "adv_data=pd.read_csv('advertisement_success.csv')\n",
    "adv_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L9HRjtjokC9b"
   },
   "source": [
    "<a id=section4></a>\n",
    "## **4. Pre-profiling of Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install config-with-yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 409,
     "referenced_widgets": [
      "7cd30bb9688c45ffbfb20ed8efe440e1",
      "b895a13004874030a8a3781281aba775",
      "16a78573eba04e49bfda26b1f715e094",
      "b5e4f2dfea4b40af8759e0aa5c1bf11c",
      "eef2d6095fa9421cbf0ce5552813f431",
      "6fd4b7911d0d4d68ba8868218ffd8b9b",
      "a2669a858bea40ab9e1930f2a3910f5d",
      "cf76cbb90caa4c0eb9cce0287eaccf2e",
      "4e13cfe86fe44d259c0d620caee39111",
      "d0a4ad0446ac455f820fa2779125aecd",
      "ec57249851504bccabb6ce0e9eb039e6",
      "e23ffb8f230a4bd28ec02c9f0585aef3",
      "619ab18a8c1e4eb6a3613716843f7c7e",
      "7ec735aad1d24157917eb4413d659617",
      "80189249d21e422e96e8496dfa0d6946",
      "b1dac2ceeb154a0ca1b92ee6a5774cd8",
      "99611965e4a3426bb6f033df5604fd0e",
      "e907020be5fe4c1391b88e6bfc8faec3",
      "551ddbd09e284d289cd30c66baea1060",
      "5cf2a3734f6a450dbfdbfe463c7415f6",
      "56eec5d6999b435eaa8da5d5df6193a4",
      "0e7d1aa19a7641c6beacc8340bca6c42",
      "bbb800ebd2294c499c6234d7909ebce7",
      "8bd7a420269a467dab4d7f683412d768",
      "40d9e82b3546430cbc8833bf61f21f4f",
      "fcb0833e9fc74835bcd1edfe6bf347c4",
      "a82bbb213a894ce38735d416be25a3d9",
      "d0daa37114584d3b98a0c748fe6162ae",
      "c1725ba4121c4fa8a20e2a6bcd4c5c6a",
      "28ff5fec54ef4e06bf4e8926ca7e75fc",
      "5ebc77b88fc047b4bb996e5c95c8e454",
      "2b2ac5bc569d49a189ac28d82a67d2db",
      "617fbb675a724e32ba2aa590200eda4e",
      "cfd7637676f84eb58d8261dca47c1c04",
      "baf2f8fb4d304de8b6f88e72174e95ae",
      "be1d9fc483454222bbf26b79c386bb09",
      "857e1e236fc0460ca3b2a75ef3deebdd",
      "73d1aa4431014e5fa9c760868e400646",
      "e1f206db22574dfabad4a8ccd36a3102",
      "61c6c2b1717a4024a3dd2706860e19af",
      "f0e83e483d55455587416a0989538944",
      "efa59afe025f4e39baec14ae175dd78d",
      "ceb4385988f947a88532afc7c2583255",
      "68f61c364f91484f99af74adf868f6b8",
      "3f99c7d20557409f936d051222fd029e",
      "985f06a377f3487c87c0aacfe0e19977",
      "132696e0e57d4d8a8fca1859584f08df",
      "30a7391ec5d249a8b535e9df16f135d7",
      "baeec67fc5fd458d9ee6b1e89a7e6023",
      "9470e3ca2e64488a98c709e11cd76ae1",
      "dd2b9f16df4f4376998696337d726459",
      "f37f866eddd3421e91096ae56a77f760",
      "2fb13bc8ffed4a9e985f5c6aeaf04e5f",
      "1762321b21344e2ea404531105d3d203",
      "3cd9cc14e44043e68ac6cc89fb8e9410",
      "6238b75d5d8c45b1a4e1220308868316",
      "c042f8b98ebe45eebddc03a6d251af50",
      "610f8e5bece34ed8800850eb9db18b90",
      "3ac11be1e9104601a5174c4bd17036a5",
      "0030ad715ac24635b2d555a75c3be88c",
      "3c7745aed29147c4ad9c8a82649ffa23",
      "b19ea9c1fbd74a418bd08ba6c7e98f9f",
      "bea86f53d46245e5b391e966d5482c3c",
      "bbcb610b29194b608f10eb6df74603b2"
     ]
    },
    "colab_type": "code",
    "id": "fM7y2HCKkH7V",
    "outputId": "ed9c0e3b-36bc-4f90-9a5a-5f926540c52f"
   },
   "outputs": [],
   "source": [
    "prof_adv=ProfileReport(adv_data)\n",
    "prof_adv.to_file(output_file='Advertising_data_profiling.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L6s2IrzMkaQd"
   },
   "source": [
    " - No missing values.\n",
    " - realtionship_status is categorcial column has 7 distinct values with 'Married-civ-spouse' and 'Never-married' have majority of the records.\n",
    " - industry is categorical column has 6 distinct values with 'Pharma' and 'Auto' is the most common values.\n",
    " - genre is categorical column has 5 distinct values with 'Comedy' is the most common value and has majority of the data while other values have very few records.\n",
    " - targeted_sex is categorical column has 2 values - 'Male' and 'Female' where 'Male' has majority of the values.\n",
    " - runtime is numerical continuos values where bin range of 30-40 mins have most of the records and almost normal distribution.\n",
    " - airtime is categorical column and 'Primetime' is most common value.\n",
    " - airlocation is categorical column and 'United States' has majority of the records.\n",
    " - ratings is the numerical continuos column  - positively skewed, high value of kurotsis as well - means some values have extreme high count.\n",
    " - expensive is the categorical column with \"low expensive\" have majority of the records.\n",
    " - money_bak_guarantee has boolean values - Yes/No with almost equal percenatge of records for both.\n",
    " - netgain is boolean record - True/False where True has major records  - data imbalance issue which needs to be taken care.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BL6izv-QqT_E"
   },
   "source": [
    "<a id=section5></a>\n",
    "## **5. Profiling using Sweetviz package**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "4o_6H0dCnFg4",
    "outputId": "6f9b12f7-f110-4110-be59-4d9750118ef1"
   },
   "outputs": [],
   "source": [
    "advert_report = sv.analyze(adv_data)\n",
    "advert_report.show_html('Advertising.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wEY0WSzergzf"
   },
   "source": [
    "Sweetviz provided information on correlation among fields\n",
    " - relationship has high correlation with indsutry\n",
    " - targeted_sex has high correlation with industry\n",
    " - netgain  - the output variable is not highly correlated with any field but slightly dependent on netgain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gMvGbfIv0Yfc"
   },
   "source": [
    "<a id=section6></a>\n",
    "## **6. Analysis using Autoviz**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "5CGC1epo0dwv",
    "outputId": "1fd6dec9-e810-4a3b-a76f-604e231b2c11"
   },
   "outputs": [],
   "source": [
    "aftrain=AV.AutoViz('advertisement_success.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yz_Ze21Q9yS8"
   },
   "source": [
    "1. Data imbalance issue in the output dataset.\n",
    "2. Most of the advertisements are low cost categories with high cost is distant second which is almost half of low cost.\n",
    "3. Most common average rating is 40 mins/week followed by 50 mins and 60 mins but their count is almost negligible when compared whem compared to 40 mins/week.\n",
    "4. Comedy is the most common genre and dominates the datatset, it is followed by Informercial which is very low in count as compared to comedy.\n",
    "5. Pharma is the most common industry for Advertisements followed by Auto , Political and Entertainment, with Poitical and Entertainment almost half of Pharma.\n",
    "6. Most of the advertisements are intended for Primetime followed by Morning time whihc is half og Primetime count, and Dayime has very less count.\n",
    "7. The ratings below 0.2 are most common.\n",
    "8. In case of targeted genders - Male category is almost double of Female.\n",
    "9. In case of relationship status, 'Married-civ-spouse' is most common, followed by 'Never Married'.\n",
    "10. Most of the records belong to United States.\n",
    "11. Money back guarantee have equal distribution - for money back and not back category.\n",
    "12. While people from various categroes majorly given average rating below 0.4, but people under \"Married-civ-spouse\" have given rating to Pharma above than 0.4.\n",
    "13. Drama genre seems to have higher average ratings than other genres, it seems to have ratings above 0.5 while others have below than 0.4.\n",
    "14. Daytime advertisements have better average rating than Primetime advertisements, even if Daytime count is least in the airtime category and Primetime has majority.\n",
    "15. Asian countries like Japan, Thailand,India have given overall better average ratings than European countries and United States.\n",
    "16. Average ratings given by Make are higher than Females, but if we consider the record count of Female and Male in target audience, the ratings seems to be given by Feamle seems better.\n",
    "17. The average rating based on netgain, the profitable records to have rating aorund 0.07, while non-profitable are around 0.03."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w-lgajZuB3tH"
   },
   "source": [
    "<a id=section7></a>\n",
    "## **7. Statistical Analysis of Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tjDv7BfwlzP-"
   },
   "source": [
    "Taking the backup of original data and removed the \"id\" field from the data as it holds no significance in the analysis.\n",
    "\n",
    "Also, the column name has been trimmed to \"runtime\" from original \"average_runtime(minutes_per_week)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "xheS-EyIkNil",
    "outputId": "923d8108-eed4-4607-d9a6-8a4fbb4f25c2"
   },
   "outputs": [],
   "source": [
    "adv_data_orig=adv_data.copy()\n",
    "adv_data.drop('id', axis=1, inplace=True)\n",
    "adv_data.rename(columns={\"average_runtime(minutes_per_week)\":\"runtime\"}, inplace=True)\n",
    "adv_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3Td7JQ2GVyBn",
    "outputId": "6b438254-a468-4c64-d2c8-0c74eea06b28"
   },
   "outputs": [],
   "source": [
    "adv_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "glVHOM3RDXb1"
   },
   "source": [
    "The dataset is having 26048 rows and 11 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "Z0Rl81PYDcMF",
    "outputId": "a144d320-dc4f-4d1f-aeb4-9f4f9d2aba98"
   },
   "outputs": [],
   "source": [
    "adv_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-x0EgXU9DjAF"
   },
   "source": [
    "The dataset contains no null values.\n",
    "\n",
    "Most of the columns are of type object which needs to be converted to numerical values before feeding the data to various ML/AI models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "colab_type": "code",
    "id": "s3wa0LlvVyBp",
    "outputId": "4b6cf7d5-ebaf-40ea-b54c-ef95ce88470d"
   },
   "outputs": [],
   "source": [
    "adv_data.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x4w1r2esEHGN"
   },
   "source": [
    "Above is the summary of all the columns present in a dataset - whether numeric or object type.\n",
    "\n",
    " - No Null values\n",
    " - 2 numeric columns - airtime and ratings\n",
    " - Both the columns seems to have outliers \n",
    " - runtime has almost value of 40 mins/week for around 50% of data and maximum is 99 minutes/week, so it means very few advertisements have runtime of 99 minutes, while most of them are limited to 40 mins.\n",
    " - ratings seems to be slighly positive skewed, with outlier value of rating 1, while majority of the advertisements are having ratings of around 0.027, but very few with 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lXS_bautGk72"
   },
   "source": [
    "**Checking the unique values for categorical columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "LKC_5Ub0EGRV",
    "outputId": "caeb0ead-5ee1-4778-f728-2f47e977175d"
   },
   "outputs": [],
   "source": [
    "print(adv_data.realtionship_status.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LloRQuM9oGRw"
   },
   "source": [
    "'Married-spouse-absent' 'Married-civ-spouse' and 'Married-AF-spouse' can be merged later into single category - \"Married\"\n",
    "\n",
    "Divorced', 'Separated' can be merged later into single category - \"Divorced\"\n",
    "\n",
    "'Separated', 'Never-married' 'Widowed' can be merged later into single category - \"Single\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fW1RRFGwGkEG",
    "outputId": "cefd5f52-37a6-4a9d-c320-dfb5bac92e80"
   },
   "outputs": [],
   "source": [
    "print(adv_data.industry.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ixv0jbIBGuCH",
    "outputId": "e823ee75-c2c5-4191-ee03-6cbd6a2cb557"
   },
   "outputs": [],
   "source": [
    "print(adv_data.genre.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "sVuk-VfcGwdl",
    "outputId": "5d688bdb-3318-4342-9064-19e71999363a"
   },
   "outputs": [],
   "source": [
    "print(adv_data.targeted_sex.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "GvRH40CkG1WN",
    "outputId": "c6ffbc75-ccbc-4e49-d93f-fd513a8f4f05"
   },
   "outputs": [],
   "source": [
    "print(adv_data.airtime.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "vo536pR3G4H1",
    "outputId": "9673f46b-d69c-403e-9cd4-197799d9be62"
   },
   "outputs": [],
   "source": [
    "print(adv_data.airlocation.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "anD6ConTG9dH",
    "outputId": "699491c4-395e-4dd6-fb9b-788fe6c4400c"
   },
   "outputs": [],
   "source": [
    "print(adv_data.expensive.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YwTxOIAuG_rY",
    "outputId": "1e2fdaae-a3a3-4fdc-d9ba-a14e353ce2b6"
   },
   "outputs": [],
   "source": [
    "print(adv_data.money_back_guarantee.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "TjLNPT7fHDnG",
    "outputId": "b5a334f3-26ca-496b-d256-00d02cb74c98"
   },
   "outputs": [],
   "source": [
    "print(adv_data.netgain.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kbn1d4_tq6Th"
   },
   "source": [
    "All the above values will be label encoded to numeric values later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZXyYbBi3CHUm"
   },
   "source": [
    "<a id=section8></a>\n",
    "## **8. Exploratory Analysis of Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6OcxnOVrcbdq"
   },
   "source": [
    "Understanding the relation in the data by plotting various graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0mhsUA3IRki0"
   },
   "outputs": [],
   "source": [
    "def plot_countplot(df, column, color, description):\n",
    "  plt.figure(figsize=(15,8))\n",
    "  sns.countplot(x=column,color=color, data=df)\n",
    "  plt.title(description)\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3hNkW_ODZyXq"
   },
   "outputs": [],
   "source": [
    "def plot_barplot(df, col1, col2, col3, loc, description, w=20, h=8):\n",
    "  plt.figure(figsize=(w,h))\n",
    "  sns.barplot(x=col1, y=col2, hue=col3, data=df)\n",
    "  plt.legend(loc=loc)\n",
    "  plt.title(description)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0vZevMuml6k5"
   },
   "source": [
    "<a id=section801></a>\n",
    "### **8.1 Advertisements counts based on relationship status**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 700
    },
    "colab_type": "code",
    "id": "mqbu7I6omAl_",
    "outputId": "af47bde5-c2cf-4b00-eca1-214d409a1837"
   },
   "outputs": [],
   "source": [
    "plot_countplot(adv_data, 'realtionship_status', 'red', 'Advertisements count based on relationship status')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ETzJi9epmXly"
   },
   "source": [
    "'Married-civ-spouse' and 'Never married' are the major followers of advertisements followed by 'Divorced'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dHKagVKncmjD"
   },
   "source": [
    "<a id=section802></a>\n",
    "### **8.2 Advertisements counts for each industry**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "ZqAnr0YtR4UM",
    "outputId": "51adf15e-30b6-4d28-b2dc-da38e7899766"
   },
   "outputs": [],
   "source": [
    "plot_countplot(adv_data, 'industry', 'orange', 'Advertisements count per industry')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QATuD-JMn12o"
   },
   "source": [
    "Majority of the advertisements belonged to Pharma industry followed by Auto industry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iYLtTfhUrF9R"
   },
   "source": [
    "<a id=section803></a>\n",
    "### **8.3 Advertisements counts for each genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "eUs0lUpRVyBs",
    "outputId": "9f27bb11-bf0c-4cab-a61a-a3180879f39a"
   },
   "outputs": [],
   "source": [
    "plot_countplot(adv_data, 'genre', 'green', 'Advertisements count per genre')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FRqxDwZVrPHv"
   },
   "source": [
    "Comedy genre has the major count for Advertisements as comedy genre can attract the viewers/listeners easily, so this could be the reason, this genre preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IoVH6ihPrqNO"
   },
   "source": [
    "<a id=section804></a>\n",
    "### **8.4 Advertisements counts gender-wise**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 514
    },
    "colab_type": "code",
    "id": "pRKZXfduVyBu",
    "outputId": "458eee93-4dbf-46b6-c079-a09ace2b0341"
   },
   "outputs": [],
   "source": [
    "plot_countplot(adv_data, 'targeted_sex', 'violet', 'Advertisements count based on gender')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e3xyXVtPrvjo"
   },
   "source": [
    "Most of the advertisements are targeted to Male, and about half are targeted for Female gender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rIkPtaYlsDme"
   },
   "source": [
    "<a id=section805></a>\n",
    "### **8.5 Advertisements depending on time of air**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "QERzqQOAVyBw",
    "outputId": "cabc9ccc-812a-4738-be0d-bb0589a0bef7"
   },
   "outputs": [],
   "source": [
    "plot_countplot(adv_data, 'airtime', 'blue', 'Advertisements count based on time of air')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wybEWzIrsKj6"
   },
   "source": [
    "Most of the advertisements are favoured to run at prime time, followed by morning time.\n",
    "\n",
    "Very few have day time slot as most of the people might be out for work, so less audience for the advertrisements hence low count preferred for this time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8pmOo6hxte3v"
   },
   "source": [
    "<a id=section806></a>\n",
    "### **8.6 Advertisements based on price class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "H7eje_9XVyBy",
    "outputId": "8039c9e3-dec4-4b88-b986-e6127c1073eb"
   },
   "outputs": [],
   "source": [
    "plot_countplot(adv_data, 'expensive', 'yellow', 'Advertisements count based on price range')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QDYAW9OvuEmG"
   },
   "source": [
    "Most of the advertisements belonged to low price range of products as most of the advertisements are targeted for higher range of population which could be middle class and would prefer for low cost products.\n",
    "\n",
    "It is followed by High price range of products could be for huge spenders or could be products are popular among people irrespective of high price.\n",
    "\n",
    "The Medium has the lowest count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D8ceMQixvaC_"
   },
   "source": [
    "<a id=section807></a>\n",
    "### **8.7 Advertisements based on money back guarantee**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 514
    },
    "colab_type": "code",
    "id": "gjQkQDLgVyB0",
    "outputId": "6dfed662-469a-42e3-fb5f-4d0b4498215f"
   },
   "outputs": [],
   "source": [
    "plot_countplot(adv_data, 'money_back_guarantee', 'pink', 'Advertisements count based on money back guarantee')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7B3c1Ajavm0m"
   },
   "source": [
    "The count is almost similar, as some big manufactureres can consider the customer satisfaction has a high priority, but many low/medium cost manufactureres producing in bulk may not be able to follow this process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Li3-rT49qeCV"
   },
   "source": [
    "<a id=section808></a>\n",
    "### **8.8 Advertisements based on location**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ob_5OP-ccnfQ"
   },
   "source": [
    "<a id=section80801></a>\n",
    "#### **8.8.1 Based on genre and runtime**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "0ExfoTWGzQmx",
    "outputId": "49cee116-1254-418a-9793-4f2877bb428a"
   },
   "outputs": [],
   "source": [
    "df_location=pd.DataFrame(adv_data.groupby(['airlocation','genre','runtime'])['runtime'].count().nlargest(100))\n",
    "df_location.rename(columns={\"runtime\":\"count\"}, inplace=True)\n",
    "df_location.reset_index(inplace=True)\n",
    "df_location=df_location.sort_values(by='runtime', ascending=False)\n",
    "df_location.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448
    },
    "colab_type": "code",
    "id": "NTw4XU8EqoLP",
    "outputId": "1251e1fd-01d6-4e36-93d8-c9809ef5626f"
   },
   "outputs": [],
   "source": [
    "plot_barplot(df_location, 'airlocation', 'runtime', 'genre', 'upper right', 'Advertisements count based on location and genre', 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hdt-o0Rlrdik"
   },
   "source": [
    "It seems like all genres are across the countries have generally 40 mins/week but Direct marginally more in United states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OLZ9_HRKcq0f"
   },
   "source": [
    "<a id=section80802></a>\n",
    "#### **8.8.2 Based on industry and runtime**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "NbSjuv7dctxt",
    "outputId": "80c2ae2e-88b9-426d-80a8-7c99ebfd644f"
   },
   "outputs": [],
   "source": [
    "df_location=pd.DataFrame(adv_data.groupby(['airlocation','industry','runtime'])['runtime'].count().nlargest(100))\n",
    "df_location.rename(columns={\"runtime\":\"count\"}, inplace=True)\n",
    "df_location.reset_index(inplace=True)\n",
    "df_location=df_location.sort_values(by='runtime', ascending=False)\n",
    "df_location.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448
    },
    "colab_type": "code",
    "id": "d-E7BQPycwPI",
    "outputId": "a87929f6-95fd-4da8-c608-572a3d575f3f"
   },
   "outputs": [],
   "source": [
    "plot_barplot(df_location, 'airlocation', 'runtime', 'industry', 'upper right', 'Advertisements count based on location and industry', 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m9xudhCWf7nZ"
   },
   "source": [
    "All industries across countries are 40 mins/week, but political is less than others in runtime in United States"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UMBdd6Gpgq2x"
   },
   "source": [
    "<a id=section80803></a>\n",
    "#### **8.8.3 Based on genre and ratings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "vuUG5998greo",
    "outputId": "50bb7061-7025-4fc4-8da8-1e171e752022"
   },
   "outputs": [],
   "source": [
    "df_location=pd.DataFrame(adv_data.groupby(['airlocation','genre','ratings'])['ratings'].count().nlargest(50))\n",
    "df_location.rename(columns={\"ratings\":\"count\"}, inplace=True)\n",
    "df_location.reset_index(inplace=True)\n",
    "df_location=df_location.sort_values(by='ratings', ascending=False)\n",
    "df_location.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 446
    },
    "colab_type": "code",
    "id": "kdPWziZEgtBY",
    "outputId": "8aaec7ab-542a-469f-b1f9-3233461480b6"
   },
   "outputs": [],
   "source": [
    "plot_barplot(df_location, 'airlocation', 'ratings', 'genre', 'upper right', 'Advertisements count based on location and genre', 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CQp0l7_bjktL"
   },
   "source": [
    "All the advertisements have rating below .05 on an average, but Comedy has some better rating in United states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uPM0JX2onAff"
   },
   "source": [
    "<a id=section80804></a>\n",
    "#### **8.8.4 Based on industry and ratings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "YB5IMKzKnB0K",
    "outputId": "241721dd-71cc-48a1-b66b-00a101d826c7"
   },
   "outputs": [],
   "source": [
    "df_location=pd.DataFrame(adv_data.groupby(['airlocation','industry','ratings'])['ratings'].count().nlargest(100))\n",
    "df_location.rename(columns={\"ratings\":\"count\"}, inplace=True)\n",
    "df_location.reset_index(inplace=True)\n",
    "df_location=df_location.sort_values(by='ratings', ascending=False)\n",
    "df_location.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 446
    },
    "colab_type": "code",
    "id": "uz34c7HOm-Y5",
    "outputId": "9a600b13-40bc-47e4-a2d8-b77dacd98811"
   },
   "outputs": [],
   "source": [
    "plot_barplot(df_location, 'airlocation', 'ratings', 'industry', 'upper right', 'Advertisements count based on location and industry', 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p9pc3oXF2iru"
   },
   "source": [
    "United states have majority of Auto industry records, followed by Pharma.\n",
    "\n",
    "In other countries, no particular industry has major share of records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n2ZUQDH7wTF_"
   },
   "source": [
    "<a id=section809></a>\n",
    "### **8.9 Advertisements count based on runtime and genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "Isw2s77gesKF",
    "outputId": "80df4042-aa16-4141-b49e-2a35ac0b6074"
   },
   "outputs": [],
   "source": [
    "df_genre_runtime=pd.DataFrame(adv_data.groupby(['runtime','genre'])['runtime'].count().nlargest(50))\n",
    "df_genre_runtime.rename(columns={\"runtime\":\"count\"}, inplace=True)\n",
    "df_genre_runtime.reset_index(inplace=True)\n",
    "df_genre_runtime=df_genre_runtime.sort_values(by='runtime', ascending=False)\n",
    "df_genre_runtime.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "vFu9VJvqVyB1",
    "outputId": "cd6348e1-ebac-40d3-9f34-cee818b9a06a"
   },
   "outputs": [],
   "source": [
    "plot_barplot(df_genre_runtime, 'runtime', 'count', 'genre', 'upper left', 'Runtime by genres', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F-hAiAP-sa7S"
   },
   "source": [
    " - Advertisements based on comedy genre has 40 mins/week as the most common duration.\n",
    " - Informercial advertisements are also mostly 40 mins/week.\n",
    " - Comedy advertisements have more values like 45 mins, 50 mins, 60 mins with significant count of records.\n",
    " - Remaining genres have very few reocds on other duration apart from 40 mins/week.\n",
    "\n",
    "So, it is assumed that 40 mins/week is the most preferred and cost effective duration for companies to showcase their advertisements.\n",
    "\n",
    "Comedy genre has more different duration values as compared to other genres, could be the reason that they are more popular among people and profitable for companies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gJeDTsZww--J"
   },
   "source": [
    "<a id=section810></a>\n",
    "### **8.10 Advertisements count based on runtime and industry**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "jL7XjZ4NxKEy",
    "outputId": "550c33ca-682d-4722-d984-5df463330223"
   },
   "outputs": [],
   "source": [
    "df_industry_runtime=pd.DataFrame(adv_data.groupby(['runtime','industry'])['runtime'].count().nlargest(100))\n",
    "df_industry_runtime.rename(columns={\"runtime\":\"count\"}, inplace=True)\n",
    "df_industry_runtime.reset_index(inplace=True)\n",
    "df_industry_runtime=df_industry_runtime.sort_values(by='runtime', ascending=False)\n",
    "df_industry_runtime.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "HD7nKRy5xN_4",
    "outputId": "233be905-1525-4cda-fff1-138dc637c651"
   },
   "outputs": [],
   "source": [
    "plot_barplot(df_industry_runtime, 'runtime', 'count', 'industry', 'upper right', 'Runtime by industry', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2909GXGVu8dR"
   },
   "source": [
    "- Pharma industry has most varied duration values with majority are at 40 mins/week but it has other values as well with significant records.\n",
    "\n",
    "- Auto is the second most commmon industry with the number of records and has vaarious different duration values.\n",
    "\n",
    "- Political and Entertainment also has varied values apart from 40m mins/week, but at other durations, the records are quite low in count.\n",
    "\n",
    "- Remaining industries are having major count of advertisements at 40 mins/week but negligible for other duration values.\n",
    "\n",
    "So Pharma and Auto industries have more count of records as compared to other industries and also have adveritsements of different duration apart from most commonn duration of 40 mins/week.\n",
    "\n",
    "So, 40 mins/week is the most preferred duration as observed in the previous graph as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gAvqa5Ryzpa3"
   },
   "source": [
    "<a id=section811></a>\n",
    "### **8.11 Advertisements count based on ratings and genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "mmFVV4ViztiA",
    "outputId": "48601430-c13d-4292-ef92-7b3345cb9bdc"
   },
   "outputs": [],
   "source": [
    "df_genre_ratings=pd.DataFrame(adv_data.groupby(['ratings','genre'])['ratings'].count().nlargest(100))\n",
    "df_genre_ratings.rename(columns={\"ratings\":\"count\"}, inplace=True)\n",
    "df_genre_ratings.reset_index(inplace=True)\n",
    "df_genre_ratings=df_genre_ratings.sort_values(by='ratings', ascending=False)\n",
    "df_genre_ratings['ratings']=df_genre_ratings['ratings'].map('{:,.2f}'.format)\n",
    "df_genre_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "HYq2fZPs0JtX",
    "outputId": "039ed11e-d0a7-4d3c-cb00-d7ffbf4f3c19"
   },
   "outputs": [],
   "source": [
    "  plot_barplot(df_genre_ratings, 'ratings', 'count', 'genre', 'upper right', 'Ratings by genres', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qnVNofrfxYRN"
   },
   "source": [
    "- Since Comedy genre is most common, so in the ratings scale, it is most common and spread accross various values.\n",
    "\n",
    "- The most common values for ratings for comedy and Informercial genre is around less than 0.03\n",
    "\n",
    "- For drama and informercial and other category as well, it is around 0.03."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UHwxn7HSzt4N"
   },
   "source": [
    "<a id=section812></a>\n",
    "### **8.12 Advertisements count based on ratings and indstry**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "AjkmhBdZlMIG",
    "outputId": "01c8e235-bad9-421b-9490-6526d1f07b2d"
   },
   "outputs": [],
   "source": [
    "df_industry_ratings=pd.DataFrame(adv_data.groupby(['ratings','industry'])['ratings'].count().nlargest(100))\n",
    "df_industry_ratings.rename(columns={\"ratings\":\"count\"}, inplace=True)\n",
    "df_industry_ratings.reset_index(inplace=True)\n",
    "df_industry_ratings=df_industry_ratings.sort_values(by='ratings', ascending=False)\n",
    "df_industry_ratings['ratings']=df_industry_ratings['ratings'].map('{:,.2f}'.format)\n",
    "df_industry_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "BQdVNmb0lO8w",
    "outputId": "84b54f58-1216-45a9-d3ff-435f770bdc9b"
   },
   "outputs": [],
   "source": [
    "plot_barplot(df_industry_ratings, 'ratings', 'count', 'industry', 'upper right', 'Ratings by industry', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tqc1wr2S3CTf"
   },
   "source": [
    "As observed before Pharma has the most count of advertisements with maxmum rating count of 0.17 followed by 0.10 and then 1.\n",
    "\n",
    "Auto has most records with ratings of 0.11 followed by 0.16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i-GGONnn21he"
   },
   "source": [
    "<a id=section813></a>\n",
    "### **8.13 Ratings distribution for each genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "m4Nz-2ZU2V8g",
    "outputId": "71953f5c-d122-4626-f1c3-9d5b7c84c7eb"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "sns.stripplot(x='genre', y='ratings',  data=adv_data)\n",
    "plt.title('Ratings distribution for each genre')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h9NYT91xJx6v"
   },
   "source": [
    "The ratings genre is not uniform, rather, no ratings provided from .5 to .9 range for any genre based advertisements.\n",
    "\n",
    "Comedy and Informercial have some advertisements with ratings around .3 and .4 but other genres have ratings not more than .2 and then at 1\n",
    "\n",
    "Most of the advertisements across all genres have most common rating between .1 and .2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lc-m4q3f3IGW"
   },
   "source": [
    "<a id=section814></a>\n",
    "### **8.14 Ratings distribution for each industry**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "jOL9U1Xu2tPg",
    "outputId": "7512bf9c-acc1-4944-99b5-f38709c27321"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "sns.stripplot(x='industry', y='ratings',  data=adv_data)\n",
    "plt.title('Ratings dstribution for each industry')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tckO0O3aKwQm"
   },
   "source": [
    "Similar to genre, ratings based on industry has same distribution with Political and Pharma have few around .4.\n",
    "\n",
    "No ratings between .5 to .9\n",
    "\n",
    "Auto and Pharma have most of the counts in the graph and most common count like previous graph is around .1 to .2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WTcA9M1Q3ZVm"
   },
   "source": [
    "<a id=section815></a>\n",
    "### **8.15 Runtime distribution for each genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "VKlKGoQS5_Rg",
    "outputId": "48fccefb-9806-4cf1-ba03-332dec016a97"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "sns.stripplot(x='genre', y='runtime',  data=adv_data)\n",
    "plt.title('Runtime dstribution for each genre')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VrcPEQXbLXI0"
   },
   "source": [
    "Runtime seems to be almost well distributed accross all genres.\n",
    "\n",
    "For Comedy, comparatively most of the advertisements are below 60 mins but dispersed data later on.\n",
    "\n",
    "For other genres as well, count is less after 60 mins.\n",
    "\n",
    "Comedy has most of the count at extreme values of more than 90 mins as compared to other genres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rWvHzPZ63slN"
   },
   "source": [
    "<a id=section816></a>\n",
    "### **8.16 Runtime distribution for each industry**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "qQ822fl96dL6",
    "outputId": "790c293c-e2ff-497d-ac78-382cc76cc774"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "sns.stripplot(x='industry', y='runtime',  data=adv_data)\n",
    "plt.title('Runtime distribution for each industry')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-TtFKnpnMPxk"
   },
   "source": [
    "Pharma has most of the records and distributed across various run time values with most of them below 60 mins.\n",
    "\n",
    "Pharama is followed by Auto and Political and Entertainment in the count, where most of the count is below 60 mins and few above.\n",
    "\n",
    "Pharama has most count at extreme run time values of more than 90 mins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aZdIG-9v6Wc1"
   },
   "source": [
    "<a id=section817></a>\n",
    "### **8.17 Relationship between ratings and runtime**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "OYqN7JsD6dm9",
    "outputId": "ed971cc3-2aa6-43df-a0c4-8a2571fc514d"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "sns.scatterplot(x='ratings', y='runtime',  data=adv_data)\n",
    "plt.title('Ratings based on runtime')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P0qz4f0VObUF"
   },
   "source": [
    "No relation between ratings and runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cGvnotMTHgfT"
   },
   "source": [
    "**Summary**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P8cNNbZnHiMI"
   },
   "source": [
    "- Most of the audience belong to 'Married-civ-spouse' category or 'Never Married' category while other categories have nominal count.\n",
    "- Pharma is the most common industry than Auto.\n",
    "- Comedy is the most common genre and other genres have negligible count comparatively.\n",
    "- Most of the advertisements belong to Low cost categories.\n",
    "- With respect to air location - Asian countries prefer Drama while European countries prefer Comedy and United States are into all genres.\n",
    "- Most common duration among genres is 40 mins/Week.\n",
    "- Majority of the advertisements have rating around or below 0.03 with Comedy is most common and other genres have negligible count, and in terms of Indiustry Pharma is most common followed by Auto and Political.\n",
    "\n",
    "Since Comedy is the most common genre and Pharma and Auto are the most common industries and also other genres have  very low count comparatively, but in terms of Industries,apart from Pharma and Auto, others too have significant count.\n",
    "\n",
    "So, most of the industries prefer creating advertisements in comedy genre as it is more popular.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bd72nZZdQFcj"
   },
   "source": [
    "<a id=section818></a>\n",
    "### **8.18 Analysis of columns with respect to target column - netgain**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_LhD_gtYnmkI"
   },
   "source": [
    "<a id=section8181></a>\n",
    "#### **8.18.1 Profitable records analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4a-aG8RnMuEU"
   },
   "source": [
    "##### **Advertisements count per industry**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "tlvbO95uQPyh",
    "outputId": "0d883cbf-0083-40bc-8f43-cef996c76ad5"
   },
   "outputs": [],
   "source": [
    "profit_data=adv_data[adv_data.netgain==True]\n",
    "profit_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ro-DMl-rVGB0",
    "outputId": "98f925fe-6826-4138-f949-23bc0678bd34"
   },
   "outputs": [],
   "source": [
    "profit_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aX_yCTVaQoX2"
   },
   "source": [
    "##### **Advertisements count based on relationship status**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 512
    },
    "colab_type": "code",
    "id": "THBSWhXwQlwP",
    "outputId": "a674a5ec-8fca-4cad-e71c-d18093222de4"
   },
   "outputs": [],
   "source": [
    "plot_countplot(profit_data, 'realtionship_status', 'red', 'Advertisements count based on relationship status')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NbnSMFmtQ6T4"
   },
   "source": [
    "Since 'Married-civ-spouse' has maximum count in overall dataset, so it is exptec to be most common in profit dataset as well.\n",
    "\n",
    "But, 'Never Married' count has decreased drastically from overall dataset to profit datatset, so it means most of the advertisements for Never Married category not profitable for the industry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "MnBZX_VClBy4",
    "outputId": "fd4ca909-fb3c-4b54-ca4c-ae176b77496d"
   },
   "outputs": [],
   "source": [
    "plot_countplot(profit_data, 'industry', 'orange', 'Advertisements count per industry')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6JUbBUphS3VS"
   },
   "source": [
    "Pharma has the majority of the profitable shares, but the count is reduced drastically from above 10000 to around 5000 are profitable.\n",
    "\n",
    "But, for Auto, in the overall dataset, the industry has huge count, but very few records are profitable, overall count is more than 6000, but profitable count is around 800.\n",
    "\n",
    "All the industries have low count of profitable advertisements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LRcWVS0jM2CN"
   },
   "source": [
    "##### **Advertisements count per genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "AHo5ySo9lCRA",
    "outputId": "3627f930-715c-4e41-d013-03390daf7e1b"
   },
   "outputs": [],
   "source": [
    "plot_countplot(profit_data, 'genre', 'green', 'Advertisements count per genre')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wAqBr7YeVb3M"
   },
   "source": [
    "Comedy genre as expected has highest count of profitable advertisements, but its overall count is around 20000 and profitable is only around 6000.\n",
    "\n",
    "Other genre categories have very few count of profitable advertisements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z_MseJOlM4-L"
   },
   "source": [
    "##### **Advertisements count based on gender**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 514
    },
    "colab_type": "code",
    "id": "HTpmq157lEY_",
    "outputId": "ca90c4c2-ce58-4384-b7c6-7bd031af9315"
   },
   "outputs": [],
   "source": [
    "plot_countplot(profit_data, 'targeted_sex', 'violet', 'Advertisements count based on gender')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fK70zQpgWAvw"
   },
   "source": [
    "Male oriented advertisements have better count as profitable advertisements, that is also because in the original dataset male oriented advertisements have higher counts, though profitable count (5000) is quite less when compared to overall count(17500).\n",
    "\n",
    "Female oriented advertisements also reduced from 8000 to 1000 when consider profitability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dbzC_p9WM7iy"
   },
   "source": [
    "##### **Advertisements count based on time of air**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "2glTAU5YlFpA",
    "outputId": "983a51ed-a941-4c9c-9b35-119c12329d76"
   },
   "outputs": [],
   "source": [
    "plot_countplot(profit_data, 'airtime', 'blue', 'Advertisements count based on time of air')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KZRSsclUXXHJ"
   },
   "source": [
    "Prime time advertisements are more profitable, but that is also because, it has most count in the overall dataset.\n",
    "\n",
    "But count is decreased from overall 16000 to 5000.\n",
    "\n",
    "Daytime has the lowest count in the overall dataset, but the profitable count is almost same as Morning data and no drastic decrease also (2000 overall to around 600 in profit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ESGqk6VdM-VT"
   },
   "source": [
    "##### **Advertisements count based on price range**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "hSU1mNDjlG_O",
    "outputId": "d572d0e6-5312-4ff0-ea14-4e7a51dd37bb"
   },
   "outputs": [],
   "source": [
    "plot_countplot(profit_data, 'expensive', 'yellow', 'Advertisements count based on price range')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cuwkDipdYEIZ"
   },
   "source": [
    "Low being the most common in overall dataset is expted to have most count here as well, but the count has sharp decline - from 16000 in overall dataset to around 4000 is profitable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U9Jvts4KNAk0"
   },
   "source": [
    "##### **Advertisements count based on money back guarantee**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 514
    },
    "colab_type": "code",
    "id": "DlGJHNJglIRo",
    "outputId": "e7e62e80-6211-4718-ecaa-8c26e687ab67"
   },
   "outputs": [],
   "source": [
    "plot_countplot(profit_data, 'money_back_guarantee', 'pink', 'Advertisements count based on money back guarantee')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y2XR2daVYcUS"
   },
   "source": [
    "Here also money back guarantee has almost same share as in overall dataset, so money back guarantee is not a major factor in making advertisements profitable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Ll3ScrcNEsW"
   },
   "source": [
    "##### **Ratings distribution for each genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "kyc34cE0l8r7",
    "outputId": "e015b51f-c3bf-4e79-f481-d5dddd0837e6"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "sns.stripplot(x='genre', y='ratings',  data=profit_data)\n",
    "plt.title('Ratings distribution for each genre')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oIbVI-He4fRe"
   },
   "source": [
    "Most of the profits earning advertisements for each genre have not very good ratings, infact very few with rating 1 is profitable.\n",
    "\n",
    "Most of the advertisements with rating below or around .2 are most profitable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6T2sVxs2NG5m"
   },
   "source": [
    "##### **Ratings dstribution for each industry**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "jCDQFJUfl9Q7",
    "outputId": "a2383847-322b-49dc-da7f-e8ceb2818014"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "sns.stripplot(x='industry', y='ratings',  data=profit_data)\n",
    "plt.title('Ratings dstribution for each industry')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HrgjuFz55JFE"
   },
   "source": [
    "Most of the profits earning advertisements for each industry have not very good ratings, infact very few with rating 1 is profitable.\n",
    "\n",
    "Most of the advertisements with rating below or around .2 are most profitable.\n",
    "\n",
    "Pharma and Auto have major share."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0GiA1WUZNLbK"
   },
   "source": [
    "##### **Runtime dstribution for each genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "fCp5VV_7l-7-",
    "outputId": "ad8c761c-b174-4256-8291-38f9be60440b"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "sns.stripplot(x='genre', y='runtime',  data=profit_data)\n",
    "plt.title('Runtime dstribution for each genre')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MC4QC-mZ5mu0"
   },
   "source": [
    "Runtime is spread across all values, so no particular range of runtime is more profitable than others, BUT the the advertisements between the range of runtime of 30 to 60 have major share."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sd_iGnGbNN9q"
   },
   "source": [
    "##### **Runtime distribution for each industry**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "TtCcmqqpmAf4",
    "outputId": "86269bc1-340a-4ab7-ee78-ec26ae5d4604"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "sns.stripplot(x='industry', y='runtime',  data=profit_data)\n",
    "plt.title('Runtime distribution for each industry')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lp0NTOAA5cP1"
   },
   "source": [
    "Runtime is spread across all values, so no particular range of runtime is more profitable than others, BUT the the advertisements between the range of runtime of 30 to 60 have major share."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_g09faYzNRAb"
   },
   "source": [
    "##### **Ratings based on runtime**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "xLQMMwX1mBmB",
    "outputId": "93006afa-6b69-43d2-ef1b-69362e06b968"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "sns.scatterplot(x='ratings', y='runtime',  data=profit_data)\n",
    "plt.title('Ratings based on runtime')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dbembh6a6Cou"
   },
   "source": [
    "As observed before advertisements around rating .2 or below have major share "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g25mzVqVS4lT"
   },
   "source": [
    "##### **Advertisements count based on industry and relationship status**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "A3yQCSSzTAci",
    "outputId": "b8ba9a9c-18d5-43e0-e915-e5c49498b124"
   },
   "outputs": [],
   "source": [
    "df_indsrel=pd.DataFrame(profit_data.groupby(['industry','realtionship_status'])['netgain'].count())\n",
    "df_indsrel.rename(columns={\"netgain\":\"count\"}, inplace=True)\n",
    "df_indsrel.reset_index(inplace=True)\n",
    "df_indsrel=df_indsrel.sort_values(by='count', ascending=False)\n",
    "df_indsrel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 512
    },
    "colab_type": "code",
    "id": "gGYfATvwTCw-",
    "outputId": "13142a60-d3e5-4cf3-b244-8a3a49bb4048"
   },
   "outputs": [],
   "source": [
    "plot_barplot(df_indsrel, 'industry', 'count', 'realtionship_status', 'upper right', 'Advertisements count based on industry and relationship status', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DifpuT7dWBYN"
   },
   "source": [
    " - Pharma is most profitable for 'Married-civ-spouse' category.\n",
    " - Auto insdustry is mainly targeted to 'Never married/Divorced' category.\n",
    " - Entertainment category is mostly for 'Divorced'\n",
    " - Political for 'Never Married'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6jY16GMDVe0I"
   },
   "source": [
    "##### **Advertisements count based on industry and gender**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "9d-QMNdLVeSd",
    "outputId": "9b6f097a-0575-4ff9-8520-5eb27b6928c7"
   },
   "outputs": [],
   "source": [
    "df_indsrel=pd.DataFrame(profit_data.groupby(['industry','targeted_sex'])['netgain'].count())\n",
    "df_indsrel.rename(columns={\"netgain\":\"count\"}, inplace=True)\n",
    "df_indsrel.reset_index(inplace=True)\n",
    "df_indsrel=df_indsrel.sort_values(by='count', ascending=False)\n",
    "df_indsrel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 512
    },
    "colab_type": "code",
    "id": "hspy-YdzVoS4",
    "outputId": "3838e49a-1285-4ed5-9e29-e5136aa01fb3"
   },
   "outputs": [],
   "source": [
    "plot_barplot(df_indsrel, 'industry', 'count', 'targeted_sex', 'upper right', 'Advertisements count based on industry and gender', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8zxji6SGWtil"
   },
   "source": [
    " - Pharma is quite popular with Male\n",
    " - Auto is more targeted towards Male but comparatively significant count for Female also.\n",
    " - Female preferred Other category with no Male count for that.\n",
    " - Entertainment also have similar count for Male and Female."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SGWkbCXpa0QY"
   },
   "source": [
    "##### **Advertisements count based on genre and relationship status**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "VlE2f7u5a12z",
    "outputId": "8cdc8142-faf0-4034-c638-2931d406efed"
   },
   "outputs": [],
   "source": [
    "df_genrerel=pd.DataFrame(profit_data.groupby(['genre','realtionship_status'])['netgain'].count())\n",
    "df_genrerel.rename(columns={\"netgain\":\"count\"}, inplace=True)\n",
    "df_genrerel.reset_index(inplace=True)\n",
    "df_genrerel=df_genrerel.sort_values(by='count', ascending=False)\n",
    "df_genrerel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 512
    },
    "colab_type": "code",
    "id": "445gT5Zoa4AT",
    "outputId": "bcf42ddd-1d5c-47c6-b252-1b89108056f0"
   },
   "outputs": [],
   "source": [
    "plot_barplot(df_genrerel, 'genre', 'count', 'realtionship_status', 'upper right', 'Advertisements count based on genre and relationship status', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nqTY90iRa5lM"
   },
   "source": [
    " - Comedy is most popular with category - Married-civ-spouse, followed by Never Married and Divorced.\n",
    " - Informercial and Drama are also more popular with Married-civ-spouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eMMwkLfNa6CP"
   },
   "source": [
    "##### **Advertisements count based on genre and gender**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "CaEB_xyUa77t",
    "outputId": "4a0bc122-6101-415a-d09b-2b8db9b85ccb"
   },
   "outputs": [],
   "source": [
    "df_genrerel=pd.DataFrame(profit_data.groupby(['genre','targeted_sex'])['netgain'].count())\n",
    "df_genrerel.rename(columns={\"netgain\":\"count\"}, inplace=True)\n",
    "df_genrerel.reset_index(inplace=True)\n",
    "df_genrerel=df_genrerel.sort_values(by='count', ascending=False)\n",
    "df_genrerel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 512
    },
    "colab_type": "code",
    "id": "pUe3nBgOa9ek",
    "outputId": "711f3bea-cfba-4b70-c5fa-79cae9907af9"
   },
   "outputs": [],
   "source": [
    "plot_barplot(df_genrerel, 'genre', 'count', 'targeted_sex', 'upper right', 'Advertisements count based on genre and gender', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TgVlItT2a--r"
   },
   "source": [
    "In all the genre, Male count is more, and so because Male count is more in overall dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5xP30fLqNUfk"
   },
   "source": [
    "##### **Advertisements count based on location,runtime and genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "7hcmcm6Knv6i",
    "outputId": "2a86ba4d-43fd-4435-979f-24f2b6f25357"
   },
   "outputs": [],
   "source": [
    "df_location=pd.DataFrame(profit_data.groupby(['airlocation','genre','runtime'])['runtime'].count().nlargest(100))\n",
    "df_location.rename(columns={\"runtime\":\"count\"}, inplace=True)\n",
    "df_location.reset_index(inplace=True)\n",
    "df_location=df_location.sort_values(by='runtime', ascending=False)\n",
    "df_location.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448
    },
    "colab_type": "code",
    "id": "6xXVsK1zn57h",
    "outputId": "2be438b4-4139-4f96-81bb-97e6b9d23bae"
   },
   "outputs": [],
   "source": [
    "plot_barplot(df_location, 'airlocation', 'runtime', 'genre', 'upper right', 'Advertisements count based on location, runtime and genre', 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zz0hVUxo6fbN"
   },
   "source": [
    "United states dont have any particualr genre with major share of records in profitable data, but Comedy genre is most common across the multiple locations followed by Drama.\n",
    "\n",
    "In European, Drama seems to be in majority while in Aisan countries, drama is most profitable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tRtuU3XPNZjM"
   },
   "source": [
    "##### **Advertisements count based on location,runtime and industry**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "aZvkEJQ0n7Tf",
    "outputId": "58616ea4-50b8-4f24-f014-ca4bdc22cd02"
   },
   "outputs": [],
   "source": [
    "df_location=pd.DataFrame(profit_data.groupby(['airlocation','industry','runtime'])['runtime'].count().nlargest(100))\n",
    "df_location.rename(columns={\"runtime\":\"count\"}, inplace=True)\n",
    "df_location.reset_index(inplace=True)\n",
    "df_location=df_location.sort_values(by='runtime', ascending=False)\n",
    "df_location.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448
    },
    "colab_type": "code",
    "id": "FETTIatbn8iA",
    "outputId": "51787b7f-c291-4967-ce1d-2de0c728103c"
   },
   "outputs": [],
   "source": [
    "plot_barplot(df_location, 'airlocation', 'runtime', 'industry', 'upper right', 'Advertisements count based on location,runtime and industry', 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G_uf6OOx7V01"
   },
   "source": [
    "United States have no particualr industry common but Pharma is most common across the countries overall.\n",
    "\n",
    "And all of them have runtime approx 40 - 50 mins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jS2nkQMcNdt7"
   },
   "source": [
    "##### **Advertisements count based on location,ratings and genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "QCwmYdBfn-BA",
    "outputId": "4db10fcf-9f90-4547-f348-05b65aaabac7"
   },
   "outputs": [],
   "source": [
    "df_location=pd.DataFrame(profit_data.groupby(['airlocation','genre','ratings'])['ratings'].count().nlargest(50))\n",
    "df_location.rename(columns={\"ratings\":\"count\"}, inplace=True)\n",
    "df_location.reset_index(inplace=True)\n",
    "df_location=df_location.sort_values(by='ratings', ascending=False)\n",
    "df_location.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 446
    },
    "colab_type": "code",
    "id": "livgB0tjn_0Y",
    "outputId": "72bc8f86-208b-45d7-adc8-f5bd1dad2cbf"
   },
   "outputs": [],
   "source": [
    "plot_barplot(df_location, 'airlocation', 'ratings', 'genre', 'upper right', 'Advertisements count based on location,ratings and genre', 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MzSmzlqw77dV"
   },
   "source": [
    "Comedy is the most common genre, with ratings of around 0.02 except in United States where it is around 0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f6MSPhnWNhI6"
   },
   "source": [
    "**Advertisements count based on location,ratings and industry**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "S7dt9QgNoBE4",
    "outputId": "9a03044b-bc1a-499c-f176-fc9760cdba3d"
   },
   "outputs": [],
   "source": [
    "df_location=pd.DataFrame(profit_data.groupby(['airlocation','industry','ratings'])['ratings'].count().nlargest(200))\n",
    "df_location.rename(columns={\"ratings\":\"count\"}, inplace=True)\n",
    "df_location.reset_index(inplace=True)\n",
    "df_location=df_location.sort_values(by='ratings', ascending=False)\n",
    "df_location.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "colab_type": "code",
    "id": "ux4MKO52oDCb",
    "outputId": "774bc387-4001-49e0-e5c4-ecd1bb8c8b4c"
   },
   "outputs": [],
   "source": [
    "plot_barplot(df_location, 'airlocation', 'ratings', 'industry', 'upper right', 'Advertisements count based on location,ratings and industry', 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lmlp1KWB8srI"
   },
   "source": [
    "Pharma is the most common with rating around 0.1 with phillipines having rating around 0.5\n",
    "\n",
    "In United states unlike other countries, Entertainment and Political industry is more common than Pharma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JRl77giQNlIl"
   },
   "source": [
    "##### **Runtime by genres**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "Y9B9QFdCoE_z",
    "outputId": "024c5ee8-5342-41d2-f459-64376186a4e3"
   },
   "outputs": [],
   "source": [
    "df_genre_runtime=pd.DataFrame(profit_data.groupby(['runtime','genre'])['runtime'].count())\n",
    "df_genre_runtime.rename(columns={\"runtime\":\"count\"}, inplace=True)\n",
    "df_genre_runtime.reset_index(inplace=True)\n",
    "df_genre_runtime=df_genre_runtime.sort_values(by='runtime', ascending=False)\n",
    "df_genre_runtime.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "Ncx_SmvuoGV1",
    "outputId": "262fab56-8660-4e2c-c871-9fd08496cf36"
   },
   "outputs": [],
   "source": [
    "plot_barplot(df_genre_runtime, 'runtime', 'count', 'genre', 'upper left', 'Runtime by genres', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "exn9Dw57NqCm"
   },
   "source": [
    "Comedy genre with runtime around 40 mins has most of the records in profitable dataset, followed by 50 mins and 45 mins.\n",
    "\n",
    "Other genres have comparatively very less entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uuCG7bsjNqee"
   },
   "source": [
    "##### **Runtime by industry**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "AoWVSqhpoHhw",
    "outputId": "8ff08ded-547a-41ba-d1c3-49988a99bf6b"
   },
   "outputs": [],
   "source": [
    "df_industry_runtime=pd.DataFrame(profit_data.groupby(['runtime','industry'])['runtime'].count())\n",
    "df_industry_runtime.rename(columns={\"runtime\":\"count\"}, inplace=True)\n",
    "df_industry_runtime.reset_index(inplace=True)\n",
    "df_industry_runtime=df_industry_runtime.sort_values(by='runtime', ascending=False)\n",
    "df_industry_runtime.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "72I4BbvQoJwI",
    "outputId": "2eb0bab1-6f4c-47c9-fb36-1a53e44ef5b2"
   },
   "outputs": [],
   "source": [
    "plot_barplot(df_industry_runtime, 'runtime', 'count', 'industry', 'upper right', 'Runtime by industry', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XDm666HrNtd1"
   },
   "source": [
    "Pharma has most of the share with runtime of 40 mins, followed by 50 mins and 45 mins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hoJZ8ZPHNtxC"
   },
   "source": [
    "##### **Ratings by genres**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "ICG7yE_WoL6e",
    "outputId": "c3d274b0-1235-4c96-c117-ee042b482d90"
   },
   "outputs": [],
   "source": [
    "df_genre_ratings=pd.DataFrame(profit_data.groupby(['ratings','genre'])['ratings'].count())\n",
    "df_genre_ratings.rename(columns={\"ratings\":\"count\"}, inplace=True)\n",
    "df_genre_ratings.reset_index(inplace=True)\n",
    "df_genre_ratings=df_genre_ratings.sort_values(by='ratings', ascending=False)\n",
    "df_genre_ratings['ratings']=df_genre_ratings['ratings'].map('{:,.2f}'.format)\n",
    "df_genre_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "3UkYUcvgoNPi",
    "outputId": "d0640876-6281-463f-fafa-1d712a42d054"
   },
   "outputs": [],
   "source": [
    "plot_barplot(df_genre_ratings, 'ratings', 'count', 'genre', 'upper right', 'Ratings by genres', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pKdvdabcN1LC"
   },
   "source": [
    "Comedy is the most common with ratings around 0.3 followed by rating of around 0.17 and 1.\n",
    "\n",
    "This means that good rating does not always profitable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r2r7r0F2N1hy"
   },
   "source": [
    "##### **Ratings by industry**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "ziWq5yU6oOdw",
    "outputId": "583e41b0-75fd-4d8a-8669-205d6b054c80"
   },
   "outputs": [],
   "source": [
    "df_industry_ratings=pd.DataFrame(profit_data.groupby(['ratings','industry'])['ratings'].count().nlargest(100))\n",
    "df_industry_ratings.rename(columns={\"ratings\":\"count\"}, inplace=True)\n",
    "df_industry_ratings.reset_index(inplace=True)\n",
    "df_industry_ratings=df_industry_ratings.sort_values(by='ratings', ascending=False)\n",
    "df_industry_ratings['ratings']=df_industry_ratings['ratings'].map('{:,.2f}'.format)\n",
    "df_industry_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "5Fhc7JgXoPoa",
    "outputId": "ccf57aee-b7ba-408d-e692-71d4b111a705"
   },
   "outputs": [],
   "source": [
    "plot_barplot(df_industry_ratings, 'ratings', 'count', 'industry', 'upper right', 'Ratings by industry', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TxLhPbWrN6Hb"
   },
   "source": [
    "Pharma is the most common with around 0.03 rating followed by 0.17 and 0.10\n",
    "\n",
    "Same as prevous graph, the advertisements with low ratings are more profitable with very few with high ratings.\n",
    "\n",
    "But the advertisements with high ratings are overall very less in the dataset, so this is the reason, low count in the profitable data as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5qrNWdYDjo39"
   },
   "source": [
    "##### **Analyzing Profit data using AutoViz**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EQJZwkAZikM8"
   },
   "outputs": [],
   "source": [
    "profit_data.to_csv('Profit_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "uRoNAm8SjmGW",
    "outputId": "a52af898-3dc0-4a38-90b6-3467a2fab985"
   },
   "outputs": [],
   "source": [
    "aftrain=AV.AutoViz('Profit_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2p3FcAFSt7DK"
   },
   "source": [
    " - Pharma is the most common record, followed by Auto and Other.\n",
    " - Comedy is the most common genre.\n",
    " - 0.00 to 0.02 is the most common rating.\n",
    " - Most of the records belong to United States\n",
    " - Male have majority \n",
    " - Married-civ-spouse is the most common relationship status and have given better average ratings.\n",
    " - Primetime has most of the records\n",
    " - Auto industry based advertisements have got the best rating followed by Entertainment.\n",
    " - Drama has got the best rating.\n",
    " - Daytime based advertisements got the best rating.\n",
    " - Advertisements with run time of 51 mins and 66 mins have best ratings.\n",
    " - Ratings almost similar given by Male and Female.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tcJ8lIWmt_Wz"
   },
   "source": [
    "##### **Summary**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-3YdP9IZuBv6"
   },
   "source": [
    " - Pharma is most common in profit dataset, which is similar as in overall dataset, but Auto haas very less records when compared to overall record, So Auto industry based advertisements are not much profitable.\n",
    " - 'Married-civ-spouse' has maximum count as in overall dataset, but 'Never married' has huge decrease count as comapred to overall dataset, so that means advertisements targeted to Never Married are not much profitable.\n",
    " - Comedy genre is most profitable and for 'Married-civ-spouse' and primetime is the most profitable.\n",
    " - Pharma is most profitable for 'Married-civ-souse' while Auto mostly profiatble with 'Never Married' or 'Divorced' and Entertainment is most profitable with 'Divorced' and 'Never married' audience.\n",
    " - Pharma is mostly profitable for Male audience while Auto and Entertainment for both Male and Female.\n",
    " - Similarly Comedy genre is most profitable with 'Married-civ-souse'.\n",
    " - Comedy and Drama with runtime of 40-50 mins is more profitable across the World, but for all categoeries have significant count.\n",
    " - Pharma is the most profitable insdustry across the World, but in US, Auto and Entertainment are profitable.\n",
    " - Average profitable rating -  0.02 to 0.03 across the World, but in US. Comedy and Informercial has rating above 0.10.\n",
    " - Most profitable run time for Comedy/Drama is 40 mins, followed by 50 mins, 45 mins and 60 mins.\n",
    "\n",
    "\n",
    "\n",
    " So, we can deduce that Pharma is quite profitable and could be the reason it is most common in overall dataset, but Auto is not much profitable but still good count in overall dataset.\n",
    "\n",
    "Most of the profit comes from 'Marrid-civ-spouse' category audience and followed by Divorced and Never Married"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Xqix8EPnsWg"
   },
   "source": [
    "<a id=section8182></a>\n",
    "#### **8.18.2 Loss records analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "J1VyKBlfQxP7",
    "outputId": "8a16b56b-d0b3-42ad-9a92-5be91e9e9c1c"
   },
   "outputs": [],
   "source": [
    "loss_data=adv_data[adv_data.netgain==False]\n",
    "loss_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "o7UdI1tHU_fZ",
    "outputId": "8e090c01-6ce5-43b4-d10b-e2b9947f52f1"
   },
   "outputs": [],
   "source": [
    "loss_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uzv7yTegUlVE"
   },
   "source": [
    "##### **Advertisements count per industry**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "Sm8fI2EOlXQJ",
    "outputId": "a1fa2fc6-77e6-4273-cdb4-61731b924ac4"
   },
   "outputs": [],
   "source": [
    "plot_countplot(loss_data, 'industry', 'orange', 'Advertisements count per industry')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SUmR5rQjYunL"
   },
   "source": [
    "Auto has most of the count in loss dataset, that means most of advertisements for Auto industry are not much profitable, followed by Pharam and Political.\n",
    "\n",
    "Though Auto and Pharma has most of the records in dataset, so they have most of the count in profit and loss dataset as well.\n",
    "\n",
    "But, Pharma is most common in profit dataset while Auto in loss dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DYsPXnRLUiXr"
   },
   "source": [
    "##### **Advertisements count per genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "IC8dtIIWlX1f",
    "outputId": "b9b12601-2bdf-4a86-a908-83aeca31732d"
   },
   "outputs": [],
   "source": [
    "plot_countplot(loss_data, 'genre', 'green', 'Advertisements count per genre')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "orGY5StmaN0M"
   },
   "source": [
    "Comeady has the most of the share in loss dataset, because it is most common in the overall dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-3lJTEdDUolM"
   },
   "source": [
    "##### **Advertisements count based on gender**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 514
    },
    "colab_type": "code",
    "id": "IgNpd2CjlZPn",
    "outputId": "87d8eba7-d4df-43d0-edce-5901ac923388"
   },
   "outputs": [],
   "source": [
    "plot_countplot(loss_data, 'targeted_sex', 'violet', 'Advertisements count based on gender')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j6VWRuT6ab-W"
   },
   "source": [
    "Male being most common in the total dataset, have high count in loss and profitable dataset but the female count is quite high in loss dataset as compared to profit dataset.\n",
    "\n",
    "That means, many advertisements targeted towards feamle may not be much profitable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fUYCZbcIUqPE"
   },
   "source": [
    "##### **Advertisements count based on time of air**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "hNrq9YfIlang",
    "outputId": "8d718dba-d68d-4c0c-db70-5defe1b78b04"
   },
   "outputs": [],
   "source": [
    "plot_countplot(loss_data, 'airtime', 'blue', 'Advertisements count based on time of air')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4i0XyAOHa8Om"
   },
   "source": [
    "Since most of the records in Prime time in overall dataset, so it has high count in loss datasset as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OZoafXYlUtBL"
   },
   "source": [
    "##### **Advertisements count based on price range**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "_3xeQouElb2y",
    "outputId": "83ebe099-5bd7-464c-871e-6add4d96752c"
   },
   "outputs": [],
   "source": [
    "plot_countplot(loss_data, 'expensive', 'yellow', 'Advertisements count based on price range')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "58iyTNAtbKiM"
   },
   "source": [
    "Since most of the records to low cost producst, so they have most of the share in loss dataset also followed by High cost products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NIEhzPuOUvsB"
   },
   "source": [
    "##### **Advertisements count based on money back guarantee**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 514
    },
    "colab_type": "code",
    "id": "rhIxWtIhldn3",
    "outputId": "42466a21-698b-4575-9884-492de18d5eaa"
   },
   "outputs": [],
   "source": [
    "plot_countplot(loss_data, 'money_back_guarantee', 'pink', 'Advertisements count based on money back guarantee')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0LTihsTpbZgq"
   },
   "source": [
    "Money back guarantee is almost simialr in both profit and loss dataset, so not much impacting the revenue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7bhLV0uDUyeC"
   },
   "source": [
    "##### **Ratings distribution for each genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "5dupFAyOmNYg",
    "outputId": "3e0184e1-1223-4a67-a095-2c5acc6e4c47"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "sns.stripplot(x='genre', y='ratings',  data=loss_data)\n",
    "plt.title('Ratings distribution for each genre')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rD6nazNvcmaE"
   },
   "source": [
    "Vey least advertrisements with ratings 1 in loss dataset as compared to profit dataset and majority have ratings below 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7rH8daulU67l"
   },
   "source": [
    "##### **Ratings dstribution for each industry**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "fhdCLnI7mN0Q",
    "outputId": "b83905dd-dd5a-4dca-ce59-f864dd1c3363"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "sns.stripplot(x='industry', y='ratings',  data=loss_data)\n",
    "plt.title('Ratings dstribution for each industry')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GQdv8ofTc4aJ"
   },
   "source": [
    "Same as previous graph, the advertisements with ratings 1 irrespective of industry have very less count in loss dataset, while most of the records have ratings around 0.2 or below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gu4g1ofZVyWE"
   },
   "source": [
    "##### **Runtime dstribution for each genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "-bx9aTopmPxf",
    "outputId": "8b745e86-cc30-4cf3-a235-e09a7e46fcbf"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "sns.stripplot(x='genre', y='runtime',  data=loss_data)\n",
    "plt.title('Runtime dstribution for each genre')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IAIgXUPL02K8"
   },
   "source": [
    "Comedy genre has most of the records in loss datastet, it is evident as Comedy has most of the records in overall dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8gVdiog_V1Oc"
   },
   "source": [
    "##### **Runtime distribution for each industry**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "pzC1TF-YmQ4R",
    "outputId": "884ad267-b8be-409e-f37f-870c9e1deea4"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "sns.stripplot(x='industry', y='runtime',  data=loss_data)\n",
    "plt.title('Runtime distribution for each industry')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7b5k-H5l2I7-"
   },
   "source": [
    "Auto and Pharma with runtime 40-60 mins have most entires in loss dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tzDYyNTfV3I6"
   },
   "source": [
    "##### **Ratings based on runtime**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "1o97Cn8SmSTo",
    "outputId": "28dd4706-74fd-4e40-d5ee-46a8e1043589"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "sns.scatterplot(x='ratings', y='runtime',  data=loss_data)\n",
    "plt.title('Ratings based on runtime')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g1DDEvlv2T4Y"
   },
   "source": [
    "As seen in earlier grapg, most of the advertisements with ratings 1 are profitable and very less have entries in loss dataset while most of  the advertisements with ratings less than 0.2 are non-profitable.\n",
    "\n",
    "No specific runtime though, advertisements with all runtimes are present in profitable and loss dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KYV51q_-V6dU"
   },
   "source": [
    "##### **Advertisements count based on location,runtime and genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "K4031ZC9oc1l",
    "outputId": "be38e90a-847b-493b-81be-c16a803191f7"
   },
   "outputs": [],
   "source": [
    "df_location=pd.DataFrame(loss_data.groupby(['airlocation','genre','runtime'])['runtime'].count().nlargest(200))\n",
    "df_location.rename(columns={\"runtime\":\"count\"}, inplace=True)\n",
    "df_location.reset_index(inplace=True)\n",
    "df_location=df_location.sort_values(by='runtime', ascending=False)\n",
    "df_location.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "colab_type": "code",
    "id": "YWbZ_-neodf8",
    "outputId": "75f7d7a6-ede6-4293-e063-d6fd1799df41"
   },
   "outputs": [],
   "source": [
    "plot_barplot(df_location, 'airlocation', 'runtime', 'genre', 'upper right', 'Advertisements count based on location,runtime and genre', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y8Q55nyu3iH8"
   },
   "source": [
    "Comedy and Drama have most of the entries along the world, while United States and neighboring locations have Informercial,Direct and others as most common entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P7QFSXHVWbye"
   },
   "source": [
    "##### **Advertisements count based on location,runtime and industry**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "mAO7FraCofQP",
    "outputId": "1eb67c90-a6e8-4856-ccaf-bcc6f27c9864"
   },
   "outputs": [],
   "source": [
    "df_location=pd.DataFrame(loss_data.groupby(['airlocation','industry','runtime'])['runtime'].count().nlargest(200))\n",
    "df_location.rename(columns={\"runtime\":\"count\"}, inplace=True)\n",
    "df_location.reset_index(inplace=True)\n",
    "df_location=df_location.sort_values(by='runtime', ascending=False)\n",
    "df_location.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "y1mTjRYqogZn",
    "outputId": "5b9681bb-2cd3-4f0f-a096-724d1d39c8cf"
   },
   "outputs": [],
   "source": [
    "plot_barplot(df_location, 'airlocation', 'runtime', 'industry', 'upper right', 'Advertisements count based on location,runtime and industry', 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Oi2orN4c5f5Y"
   },
   "source": [
    "Auto and Pharma are the most common around the World, but in US and neighbouring locations, almost all industries have huge count of records.\n",
    "\n",
    "This could be the due to as most of the records belong to United states and nearby locations as compared to other locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wOOgGLPQWjO-"
   },
   "source": [
    "##### **Advertisements count based on location,ratings and genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "2XZ9gV2Woh5w",
    "outputId": "598a6acf-806c-4df7-91b1-9ebd4620488e"
   },
   "outputs": [],
   "source": [
    "df_location=pd.DataFrame(loss_data.groupby(['airlocation','genre','ratings'])['ratings'].count().nlargest(100))\n",
    "df_location.rename(columns={\"ratings\":\"count\"}, inplace=True)\n",
    "df_location.reset_index(inplace=True)\n",
    "df_location=df_location.sort_values(by='ratings', ascending=False)\n",
    "df_location.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "colab_type": "code",
    "id": "Q4i8u3Ydoi_h",
    "outputId": "d4917244-d7dc-4159-b6c8-0241be2d9b38"
   },
   "outputs": [],
   "source": [
    "plot_barplot(df_location, 'airlocation', 'ratings', 'genre', 'upper right', 'Advertisements count based on location,ratings and genre', 45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LOYJ2N9d6MTC"
   },
   "source": [
    "In US, the comedy genre even with somewhat higher rating has goood count, while other genre have average rating aorund just below 0.03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DcXJ-Nl5W6bK"
   },
   "source": [
    "##### **Advertisements count based on location,ratings and industry**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "W7bADbTiokUv",
    "outputId": "70d35f4d-e214-4e91-9fd2-c2cb51b58993"
   },
   "outputs": [],
   "source": [
    "df_location=pd.DataFrame(loss_data.groupby(['airlocation','industry','ratings'])['ratings'].count().nlargest(100))\n",
    "df_location.rename(columns={\"ratings\":\"count\"}, inplace=True)\n",
    "df_location.reset_index(inplace=True)\n",
    "df_location=df_location.sort_values(by='ratings', ascending=False)\n",
    "df_location.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 446
    },
    "colab_type": "code",
    "id": "iWFRzMUrolZY",
    "outputId": "d9736809-1f6a-46c4-bad1-ac505350a237"
   },
   "outputs": [],
   "source": [
    "plot_barplot(df_location, 'airlocation', 'ratings', 'industry', 'upper right', 'Advertisements count based on location,rating and industry', 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fwgZUjcF6oUI"
   },
   "source": [
    "Though all industry have significant count of records but Pharma has majority across the various locations in the world, But in US ,,Pharma and Auto has most of the records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r-d-cgksW-1N"
   },
   "source": [
    "##### **Runtime by genres**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "klX3FLg6oms-",
    "outputId": "8edf59f3-70a3-4be5-be97-222cc2e6b0bd"
   },
   "outputs": [],
   "source": [
    "df_genre_runtime=pd.DataFrame(loss_data.groupby(['runtime','genre'])['runtime'].count().nlargest(100))\n",
    "df_genre_runtime.rename(columns={\"runtime\":\"count\"}, inplace=True)\n",
    "df_genre_runtime.reset_index(inplace=True)\n",
    "df_genre_runtime=df_genre_runtime.sort_values(by='runtime', ascending=False)\n",
    "df_genre_runtime.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "L-dNrqz-ooC1",
    "outputId": "ae1ba9aa-ed3c-49a7-c082-b550050c6624"
   },
   "outputs": [],
   "source": [
    "plot_barplot(df_genre_runtime, 'runtime', 'count', 'genre', 'upper left', 'Runtime by genres', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TTM1FEt7meIO"
   },
   "source": [
    "Comedy is most common, with runtime of 40 mins have majority of the records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v8w4_EXiXFEG"
   },
   "source": [
    "##### **Runtime by industry**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "EJ18GQ2bopOx",
    "outputId": "5fc1c53a-1378-41db-f120-4bb831a8c00e"
   },
   "outputs": [],
   "source": [
    "df_industry_runtime=pd.DataFrame(loss_data.groupby(['runtime','industry'])['runtime'].count().nlargest(100))\n",
    "df_industry_runtime.rename(columns={\"runtime\":\"count\"}, inplace=True)\n",
    "df_industry_runtime.reset_index(inplace=True)\n",
    "df_industry_runtime=df_industry_runtime.sort_values(by='runtime', ascending=False)\n",
    "df_industry_runtime.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "jk1ZuUZooqbT",
    "outputId": "208fbc31-0c64-480f-8e1e-ed02d6dd0837"
   },
   "outputs": [],
   "source": [
    "plot_barplot(df_industry_runtime, 'runtime', 'count', 'industry', 'upper right', 'Runtime by industry', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uBOtCMVctv8f"
   },
   "source": [
    "Most of the industries have maximum records at 40 mins followed by 45 and 50 mins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ac2OAUaBXIpy"
   },
   "source": [
    "##### **Ratings by genres**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "O7znfvoHosBK",
    "outputId": "e7ee45ef-551f-42f6-c563-8a9eb531ab70"
   },
   "outputs": [],
   "source": [
    "df_genre_ratings=pd.DataFrame(loss_data.groupby(['ratings','genre'])['ratings'].count().nlargest(100))\n",
    "df_genre_ratings.rename(columns={\"ratings\":\"count\"}, inplace=True)\n",
    "df_genre_ratings.reset_index(inplace=True)\n",
    "df_genre_ratings=df_genre_ratings.sort_values(by='ratings', ascending=False)\n",
    "df_genre_ratings['ratings']=df_genre_ratings['ratings'].map('{:,.2f}'.format)\n",
    "df_genre_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "fm-1znZhott1",
    "outputId": "5200eaf0-651c-48af-ead6-3bc25fee65ec"
   },
   "outputs": [],
   "source": [
    "plot_barplot(df_genre_ratings, 'ratings', 'count', 'genre', 'upper right', 'Ratings by genres', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jdm8IuLAt_5j"
   },
   "source": [
    "The genres have most non-profitable with ratings around 0.03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cgsuMK-aXNY0"
   },
   "source": [
    "##### **Ratings by industry**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "cXEmu9QEovZv",
    "outputId": "0591fab5-97b7-4de1-cfaf-cc5df0f8d458"
   },
   "outputs": [],
   "source": [
    "df_industry_ratings=pd.DataFrame(loss_data.groupby(['ratings','industry'])['ratings'].count().nlargest(100))\n",
    "df_industry_ratings.rename(columns={\"ratings\":\"count\"}, inplace=True)\n",
    "df_industry_ratings.reset_index(inplace=True)\n",
    "df_industry_ratings=df_industry_ratings.sort_values(by='ratings', ascending=False)\n",
    "df_industry_ratings['ratings']=df_industry_ratings['ratings'].map('{:,.2f}'.format)\n",
    "df_industry_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "aPTYE6nrowmS",
    "outputId": "b25079c8-004d-4756-b094-5741c41fe3a4"
   },
   "outputs": [],
   "source": [
    "plot_barplot(df_industry_ratings, 'ratings', 'count', 'industry', 'upper right', 'Ratings by industry', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_8s44d6luVZk"
   },
   "source": [
    "Most of the indstry have non-profitable advertisements at the rating of 0.03 with Pharma is the most common, followed by Auto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Advertisements count based on genre and relationship status**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_genrerel=pd.DataFrame(loss_data.groupby(['genre','realtionship_status'])['netgain'].count())\n",
    "df_genrerel.rename(columns={\"netgain\":\"count\"}, inplace=True)\n",
    "df_genrerel.reset_index(inplace=True)\n",
    "df_genrerel=df_genrerel.sort_values(by='count', ascending=False)\n",
    "df_genrerel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_barplot(df_genrerel, 'genre', 'count', 'realtionship_status', 'upper right', 'Advertisements count based on genre and relationship status', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Never married have most common count for Comedy genre followed by Married-civ-spouse abd Divorced.\n",
    " - Other Genre too have similar seggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Advertisements count based on genre and gender**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_genrerel=pd.DataFrame(loss_data.groupby(['genre','targeted_sex'])['netgain'].count())\n",
    "df_genrerel.rename(columns={\"netgain\":\"count\"}, inplace=True)\n",
    "df_genrerel.reset_index(inplace=True)\n",
    "df_genrerel=df_genrerel.sort_values(by='count', ascending=False)\n",
    "df_genrerel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_barplot(df_genrerel, 'genre', 'count', 'targeted_sex', 'upper right', 'Advertisements count based on genre and gender', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Most of the non-profitable Comedy genre advertisements were for Male gender but female have good count, that means most of the female oriented advertisements are not profitable\n",
    "- Informercial and Drama too have almost similar count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Advertisements count based on industry and relationship status**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indsrel=pd.DataFrame(loss_data.groupby(['industry','realtionship_status'])['netgain'].count())\n",
    "df_indsrel.rename(columns={\"netgain\":\"count\"}, inplace=True)\n",
    "df_indsrel.reset_index(inplace=True)\n",
    "df_indsrel=df_indsrel.sort_values(by='count', ascending=False)\n",
    "df_indsrel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_barplot(df_indsrel, 'industry', 'count', 'realtionship_status', 'upper right', 'Advertisements count based on industry and relationship status', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pharma has most count for Married-civ-spouse same as in profit data\n",
    "- In other categories  - Never married and Divorced have more count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Advertisements count based on industry and gender**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indsrel=pd.DataFrame(loss_data.groupby(['industry','targeted_sex'])['netgain'].count())\n",
    "df_indsrel.rename(columns={\"netgain\":\"count\"}, inplace=True)\n",
    "df_indsrel.reset_index(inplace=True)\n",
    "df_indsrel=df_indsrel.sort_values(by='count', ascending=False)\n",
    "df_indsrel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_barplot(df_indsrel, 'industry', 'count', 'targeted_sex', 'upper right', 'Advertisements count based on industry and gender', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Pharma has only Male record\n",
    " - Auto and political have almost similar records for females\n",
    " - Entertainment has more records for females than male."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Analyzing Loss data using AutoViz**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_data.to_csv('Loss_data.csv')\n",
    "aftrain=AV.AutoViz('Loss_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Summary**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Auto has more loss data than Pharma and in overall data, Auto has less count than Pharma That means, Auto has huge count of non-profitable data.\n",
    "- Comedy genre is most common in loss data as well\n",
    "- Feamle count is less than Male count, but still high in count as comapared to profit data set, that means most of the advertisements targeted to Female are not much profitable.\n",
    "- There are only couple of records with rating=1 in loss data set, that means most of the advertisements with high ratings are profitable.\n",
    "- Runtime seems to have no impact on loss and profit as similar distribution.\n",
    "- Most of the advertisements with loss are having ratings around 0.01.\n",
    "- Genre distribution has more Auto records than Pharma.\n",
    "- Profit dataset have more records for \"Married-civ-spouse' category while loss dataset has for 'Never Married' with 'Married-civ-spouse' closely followed.\n",
    "- Loss data has comparatively more count ration for females as compared to the ratio in overall dataset and profit dataset, that means most of the advertisements for feamle not earning much profit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "riMxC2Tvm5C-"
   },
   "source": [
    "<a id=section9></a>\n",
    "## **9. Checking data distribution of output variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "colab_type": "code",
    "id": "SomiKiIckjAB",
    "outputId": "468792b1-ad01-4908-d111-8f58f5c47f90"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "sns.countplot(x='netgain', data=adv_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nR6HOO-SlGTp"
   },
   "source": [
    "Data is imbalanced, so need to impute dummy values to minority calss to make it balanced before applying any ML/DL algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EolY42kwdmjq"
   },
   "source": [
    "<a id=section10></a>\n",
    "## **10. Feature Engineering and Data Transformation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section101></a>\n",
    "### **10.1 Combining feature values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_data.realtionship_status.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_data.industry.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_data.genre.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_data.targeted_sex.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_data.expensive.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_data.airtime.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_data.airlocation.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the EDA above, observed that realtionship_status['Never-married'] and realtionship_status['Divorced'] have similar values wrt other columns.\n",
    "\n",
    "So, can combine both of these to new value  - 'single' and can check how ML works on this dataset.\n",
    "\n",
    "\n",
    "Also, can combine 'Never-married','Divorced' ,'Widowed' and 'seperated' as well to anlyze further.\n",
    "\n",
    "SO, now have below dataset for further experiments\n",
    "\n",
    " - Original one\n",
    " - realtionship_status['Never-married'] and realtionship_status['Divorced'] merged into realtionship_status['Single']\n",
    " - realtionship_status['Never-married'], realtionship_status['Seperated'],  realtionship_status['Widowed'] and realtionship_status['Divorced'] merged into realtionship_status['Single']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_data_1=adv_data.copy()\n",
    "adv_data_2=adv_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_data_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_data_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_data_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_data_1.realtionship_status.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_data_1.realtionship_status=adv_data_1.realtionship_status.replace(['Never-married','Divorced'], 'Single')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_data_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_data_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_data_1.realtionship_status.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_data_2.realtionship_status=adv_data_2.realtionship_status.replace(['Never-married','Divorced','Separated', 'Widowed'], 'Single')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_data_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_data_2.realtionship_status.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 3 datasets\n",
    "- adv_data\n",
    "- adv_data_1\n",
    "- adv_data_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5gaZxiWbm2dG"
   },
   "source": [
    "<a id=section102></a>\n",
    "### **10.2 Label Encoding and One Hot Encoding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will do the OHE for\n",
    "- relationship_status\n",
    "- genre\n",
    "- industry\n",
    "- airtime\n",
    "- targeted_sex\n",
    "\n",
    "And will do label encoding for\n",
    "- expensive\n",
    "- net_gain\n",
    "- money_back_guarantee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "RZRW8c9Zvr0h",
    "outputId": "ae93d3b8-4617-4d46-9f35-95db1daec3ab"
   },
   "outputs": [],
   "source": [
    "adv_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iRKJZmAevwaQ"
   },
   "outputs": [],
   "source": [
    "adv_data_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_data_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to apply One hot encoding\n",
    "def apply_ohe(df):\n",
    "    df=pd.get_dummies(df, columns=['realtionship_status','industry','genre','targeted_sex','airtime','airlocation'], drop_first=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to apply Label Encoding\n",
    "def apply_le(df):\n",
    "    le = LabelEncoder()\n",
    "    df['expensive']=le.fit_transform(df['expensive'])\n",
    "    df['netgain']=le.fit_transform(df['netgain'])\n",
    "    df['money_back_guarantee']=le.fit_transform(df['money_back_guarantee'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wrapper to call OHE and LE on dataframe\n",
    "def apply_encoding(df):\n",
    "    df=apply_ohe(df)\n",
    "    df=apply_le(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_data=apply_encoding(adv_data)\n",
    "adv_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_data_1=apply_encoding(adv_data_1)\n",
    "adv_data_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_data_2=apply_encoding(adv_data_2)\n",
    "adv_data_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_data.to_csv('Encoded_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_data_1.to_csv('Encoded_data_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_data_2.to_csv('Encoded_data_2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section103></a>\n",
    "### **10.3 Creating bins for the runtime values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_data_3=adv_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "adv_data_3['runtime'].plot(kind='hist')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Histogram, it seems better to create bins of 10 intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_data_3['runtime'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Around half of the data is around 40 mins and 75% of data till 45 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_data_3['runtime_bins']=pd.cut(adv_data_3['runtime'], bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_data_3.drop('runtime', axis=1, inplace=True)\n",
    "adv_data_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_data_3.to_csv('Bin_data_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_data_4=adv_data_2.copy()\n",
    "adv_data_4['runtime_bins']=pd.cut(adv_data_4['runtime'], bins=10)\n",
    "adv_data_4.drop('runtime', axis=1, inplace=True)\n",
    "print(adv_data_4.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_data_4.to_csv('Encoded_Bin_data_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_data_5=adv_data_1.copy()\n",
    "adv_data_5['runtime_bins']=pd.cut(adv_data_5['runtime'], bins=10)\n",
    "adv_data_5.drop('runtime', axis=1, inplace=True)\n",
    "print(adv_data_5.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_data_5.to_csv('Encoded_Bin_data_2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 5 datasets with various combinations to analyze\n",
    " - adv_data : original data with label encoding\n",
    " - adv_data_1 and adv_data_2 - data with relationship values combined to create new category\n",
    " - adv_data_3,adv_data_4 and adv_data_5 - label encoding with bins defined for runtime column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section104></a>\n",
    "### **10.4 Checking for outliers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 continuos columns  - runtime and rating and both seem to have high count of outliers, it is better to observe if outliers need to be deleted or retain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section10401></a>\n",
    "#### **10.4.1 Using Boxplot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_outliers(df, col):\n",
    "    trace=[]\n",
    "    trace.append(go.Box(y=df[col],name=col))\n",
    "    data=trace\n",
    "    iplot({\"data\":data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enable_plotly_in_cell()\n",
    "plot_outliers(adv_data, 'runtime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many outliers in the column runtime, and the reason is most of the advertrisements are around 40 mins and few are with different runtimes, but cant remove those values as runtime is also important in determining netgain, so would be better to retain it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enable_plotly_in_cell()\n",
    "plot_outliers(adv_data, 'ratings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to runtime, ratings column also has most of the values around 0.02, so other values are extreme but they can't be ignored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section10402></a>\n",
    "#### **10.4.2 Analyzing using IQR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outlier_info(df):\n",
    "    Q1=df.quantile(0.25)\n",
    "    Q3=df.quantile(0.75)\n",
    "    IQR=Q3-Q1\n",
    "    #print (IQR)\n",
    "    return df[((df<(Q1-IQR*1.5)) | (df>(Q3+IQR*1.5))).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp=get_outlier_info(adv_data)\n",
    "print (tmp.shape)\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outlier_colinfo(df, col):\n",
    "    Q1=df[col].quantile(0.25)\n",
    "    Q3=df[col].quantile(0.75)\n",
    "    IQR=Q3-Q1\n",
    "    #print (IQR)\n",
    "    print (df[((df[col]<(Q1-IQR*1.5)) | (df[col]>(Q3+IQR*1.5)))][col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_outlier_colinfo(adv_data, 'runtime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_outlier_colinfo(adv_data, 'ratings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of outliers record if combine for runtime and ratings are around 8600, which is huge in terms of dataset with 26k records, so dropping those would cause losing information, so better to retain those\n",
    "\n",
    "But can try to use RobustScaler while modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section10403></a>\n",
    "### **10.4.3 Analyzing approaches on data imbalance issue**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can analuze various ways to reduce the data imbalance issue by oversampling the minority class\n",
    "\n",
    " - RandomOverSampler\n",
    " - SMOTE \n",
    " - ADASYN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can analyze all the techniques on basic SVC classifier and measure the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to split data into test and train\n",
    "def split_data(data, test_size):\n",
    "    X=data.drop('netgain', axis=1)\n",
    "    y=data['netgain']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=test_size, random_state=42)\n",
    "    \n",
    "    print(\"X_train shape - \", X_train.shape)\n",
    "    print(\"y_train shape - \", y_train.shape)\n",
    "    print(\"X_test shape - \", X_test.shape)\n",
    "    print(\"y_test shape - \", y_test.shape)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test=split_data(adv_data, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std=StandardScaler()\n",
    "X_train_std=std.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**With imbalance data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=SVC()\n",
    "clf.fit(X_train_std, y_train)\n",
    "clf.score(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using Random OverSampler**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(RandomOverSampler(random_state=0), SVC())\n",
    "pipe.fit(X_train_std, y_train)\n",
    "pipe.score(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using SMOTE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(SMOTE(random_state=0), SVC())\n",
    "pipe.fit(X_train_std, y_train)\n",
    "pipe.score(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using ADASYN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(ADASYN(random_state=0), SVC())\n",
    "pipe.fit(X_train_std, y_train)\n",
    "pipe.score(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy got impacted from various sampling techniques, but accuracy can never be measure of model and especially in highly imbalanced dataset like this, so can include the sampling methods in ML pipeline to experiment with various approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section10404></a>\n",
    "### **10.4.4 LDA and PCA Transformation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Using LDA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_n_components(var_ratio, goal_var):\n",
    "    total_variance=0\n",
    "    n_comp=0\n",
    "\n",
    "    for explained_variance in var_ratio:\n",
    "        total_variance  += explained_variance\n",
    "\n",
    "        n_comp += 1\n",
    "\n",
    "        if total_variance>=goal_var:\n",
    "            break\n",
    "\n",
    "    return n_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lda_comp(X, y):\n",
    "    lda=LDA(n_components=None)\n",
    "\n",
    "    X_lda=lda.fit(X, y)\n",
    "\n",
    "    lda_var_ratios=lda.explained_variance_ratio_\n",
    "    \n",
    "    print(select_n_components(lda_var_ratios, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=adv_data.drop('netgain', axis=1)\n",
    "y=adv_data['netgain']\n",
    "std=StandardScaler()\n",
    "X_std=std.fit_transform(X)\n",
    "get_lda_comp(X_std, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like only 1 component wll explain 99% of the variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Using PCA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_std.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.99)\n",
    "X_pca = pca.fit_transform(X_std)\n",
    "X_pca.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems 58 components explain 99% of the variance using PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in the pipeline, it would be better to expriment with both"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section11></a>\n",
    "## **11. Feature Selection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section1101></a>\n",
    "### **11.1 Using Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "model=ExtraTreesClassifier()\n",
    "model.fit(X,y)\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "feat_importances=pd.Series(model.feature_importances_, index=X.columns)\n",
    "feat_importances.nlargest(20).plot(kind='barh')\n",
    "plt.title('Important Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section1102></a>\n",
    "### **11.2 Using selectKBest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestfeatures=SelectKBest(score_func=chi2, k=20)\n",
    "fit=bestfeatures.fit(X,y)\n",
    "dfscores=pd.DataFrame(fit.scores_)\n",
    "dfcolumns=pd.DataFrame(X.columns)\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "featureScores=pd.concat([dfcolumns, dfscores], axis=1)\n",
    "featureScores.columns=['Features', 'Score']\n",
    "sns.barplot(x='Score', y='Features', data=featureScores.sort_values(by='Score', ascending=False).head(20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestfeatures=SelectKBest(score_func=f_classif, k=20)\n",
    "fit=bestfeatures.fit(X,y)\n",
    "dfscores=pd.DataFrame(fit.scores_)\n",
    "dfcolumns=pd.DataFrame(X.columns)\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "featureScores=pd.concat([dfcolumns, dfscores], axis=1)\n",
    "featureScores.columns=['Features', 'Score']\n",
    "sns.barplot(x='Score', y='Features', data=featureScores.sort_values(by='Score', ascending=False).head(20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above analysis, it is observed that below features are most important, so during experiment, can try to model on the data with only these features (using selectKBest in pipeline with same k value)\n",
    "\n",
    " - ratings\n",
    " - runtime\n",
    " - relationship_status_Married-civ-spouse\n",
    " - industry_Pharma\n",
    " - relationship_status_Never-married\n",
    " - airtime_Morning\n",
    " - airtime_Primetime\n",
    " - expensive\n",
    " - targeted_sex_Male\n",
    " - industry_Political\n",
    " - industry_Entertainment\n",
    " - industry_Other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section12></a>\n",
    "## **12. Pycaret - to analyze the best models on the dataset and features before actual modelling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Will run and analyze Pycaret for adv_data to analyze the best models and important features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Running for adv_data with feature_selection and fix_imbalance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.classification import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = setup(adv_data, target = 'netgain', session_id=123, log_experiment=False, experiment_name='adv1',  feature_selection=True, fix_imbalance=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " base_models = compare_models(exclude=['nb','qda'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catb=create_model('catboost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc=create_model('gbc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm=create_model('lightgbm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb=create_model('xgboost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada=create_model('ada')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blender_specific_soft= blend_models(estimator_list = [catb,gbc,xgb,lgbm,ada], method = 'soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blender_specific_hard= blend_models(estimator_list = [catb,gbc,xgb,lgbm,ada], method = 'hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacker = stack_models(estimator_list = [catb,gbc,lgbm,ada], meta_model = xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret_model(catb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret_model(gbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret_model(lgbm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Running for adv_data with feature_selection and fix_imbalance and normalize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = setup(adv_data, target = 'netgain', session_id=124, log_experiment=False, experiment_name='adv1',  feature_selection=True, fix_imbalance=True, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " base_models = compare_models(exclude=['nb','qda','lda'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catb=create_model('catboost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc=create_model('gbc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm=create_model('lightgbm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb=create_model('xgboost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada=create_model('ada')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blender_specific_soft= blend_models(estimator_list = [catb,gbc,xgb,lgbm,ada], method = 'soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blender_specific_hard= blend_models(estimator_list = [catb,gbc,xgb,lgbm,ada], method = 'hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacker = stack_models(estimator_list = [catb,gbc,lgbm,ada], meta_model = xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret_model(catb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret_model(gbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret_model(lgbm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, while modelling the data, can use the above features to check the performace on the best features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "Below models seems to be best performing\n",
    "\n",
    "- Logistic Regression\n",
    "- Random Forest Classifier\n",
    "- Random Forest Classifier\n",
    "- Ada Boost Classifier\n",
    "- XGBoost Classifier\n",
    "- Extreme Gradient Boosting Classifier\n",
    "- Light Gradient Boosting Classifier\n",
    "- Catboost\n",
    "\n",
    "\n",
    "Also below features seems to be most important as seen before\n",
    "\n",
    " - ratings\n",
    " - runtime\n",
    " - relationship_status_Married-civ-spouse\n",
    " - industry_Pharma\n",
    " - relationship_status_Never-married\n",
    " - airtime_Morning\n",
    " - airtime_Primetime\n",
    " - expensive\n",
    " - targeted_sex_Male\n",
    " - industry_Political\n",
    " - industry_Entertainment\n",
    " - industry_Other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section13></a>\n",
    "## **13. Checking the distribution of continuos columns to decide on normalization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to analyze if runtime and ratings are normally distributed using various methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basic methods using describe()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_data.runtime.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_data.ratings.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyzing using KDE plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.kdeplot(data=adv_data.runtime)\n",
    "plt.title('Runtime Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.kdeplot(data=adv_data.ratings)\n",
    "plt.title('Ratings Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QQ Plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from scipy.stats import norm\n",
    "import pylab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.qqplot(adv_data.runtime, line='45')\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.qqplot(adv_data.ratings, line='45')\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like both the values need to normalized/scaled, so will experiment with that in ML sklearn pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section14></a>\n",
    "## **14. Machine learning - Analyzing baseline models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From PyCaret we observed that following models provided the satisfactory results,\n",
    "\n",
    "- Logistic Regression\n",
    "- Random Forest Classifier\n",
    "- Random Forest Classifier\n",
    "- Ada Boost Classifier\n",
    "- XGBoost Classifier\n",
    "- Extreme Gradient Boosting Classifier\n",
    "- Light Gradient Boosting Classifier\n",
    "- Catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also since get the idea of best features in the dataset, so can ignore including PCA and LDA in experimentation as LDA returning only 1 compoenent, so can lose information and PCA returning  58 features which is close to actual number of features - 63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fucntion to create pipeline of various modesl with various scaling and sampling techniques and return the best model parameters\n",
    "#Will be called for different dataset\n",
    "\n",
    "MLA_columns = ['MLA Name', 'MLA Parameters','MLA Train Accuracy Mean', 'MLA Test Accuracy Mean', 'MLA Recall', 'MLA Precision', 'MLA F1 Score']\n",
    "MLA_compare = pd.DataFrame(columns = MLA_columns)\n",
    "\n",
    "def train_predict_default_ml(data, stratified, modelfilename):\n",
    "    \"\"\"\n",
    "    data - Input dataframe\n",
    "    stratified - boolean - whether to set stratify flag ON or OFF while splitting test and train data for model\n",
    "    modelfilename - pickle filename to dump the best model so that can be load and used later\n",
    "    \"\"\"\n",
    "\n",
    "    row_index=0\n",
    "    best_accuracy=0\n",
    "    best_recall=0\n",
    "    best_precision=0\n",
    "    best_f1=0\n",
    "\n",
    "    X=data.drop('netgain', axis=1)\n",
    "    y=data['netgain']\n",
    "    \n",
    "    if (True==stratified):\n",
    "        X_tr, X_ts, y_tr, y_ts = train_test_split(X, y, test_size = .20, random_state = 0, stratify=y, shuffle=True)\n",
    "    else:\n",
    "        X_tr, X_ts, y_tr, y_ts = train_test_split(X, y, test_size = .20, random_state = 0)\n",
    "              \n",
    "    #preprocess_step=FeatureUnion([('kbest1', SelectKBest(score_func=chi2, k=15)), ('kbest2', SelectKBest(score_func=f_classif, k=15))])\n",
    "    preprocess_step=FeatureUnion([('kbest', SelectKBest(score_func=f_classif, k=15))])\n",
    "\n",
    "    #Logistic Regression CV pipeline with Data imbalance and Scaling fucntions\n",
    "    pipe_lrv_1=Pipeline([('lrvscaling1', StandardScaler()),\n",
    "                        ('lrvimbalance1', RandomOverSampler(random_state=0)),\n",
    "                        ('lrv_preprocess1', preprocess_step),\n",
    "                        ('lrvclassifer1', LogisticRegressionCV(cv=5, max_iter=500, random_state=0))])\n",
    "    pipe_lrv_2=Pipeline([('lrvscaling2', StandardScaler()),\n",
    "                        ('lrvimbalance2', SMOTE(random_state=0)),\n",
    "                        ('lrv_preprocess2', preprocess_step),\n",
    "                        ('lrvclassifer2', LogisticRegressionCV(cv=5, max_iter=500, random_state=0))])\n",
    "    pipe_lrv_3=Pipeline([('lrvscaling3', StandardScaler()),\n",
    "                        ('lrvimbalance3', ADASYN(random_state=0)),\n",
    "                        ('lrv_preprocess3', preprocess_step),\n",
    "                        ('lrvclassifer3', LogisticRegressionCV(cv=5, max_iter=500, random_state=0))])\n",
    "    pipe_lrv_4=Pipeline([('lrvscaling4', Normalizer()),\n",
    "                        ('lrvimbalance4', RandomOverSampler(random_state=0)),\n",
    "                        ('lrv_preprocess4', preprocess_step),\n",
    "                        ('lrvclassifer4', LogisticRegressionCV(cv=5, max_iter=500, random_state=0))])\n",
    "    pipe_lrv_5=Pipeline([('lrvscaling5', Normalizer()),\n",
    "                        ('lrvimbalance5', SMOTE(random_state=0)),\n",
    "                        ('lrv_preprocess5', preprocess_step),\n",
    "                        ('lrvclassifer5', LogisticRegressionCV(cv=5, max_iter=500, random_state=0))])\n",
    "    pipe_lrv_6=Pipeline([('lrvscaling6', Normalizer()),\n",
    "                        ('lrvimbalance6', ADASYN(random_state=0)),\n",
    "                        ('lrv_preprocess6', preprocess_step),\n",
    "                        ('lrvclassifer6', LogisticRegressionCV(cv=5, max_iter=500, random_state=0))])\n",
    "    pipe_lrv_7=Pipeline([('lrvscaling7', MinMaxScaler()),\n",
    "                        ('lrvimbalance7', RandomOverSampler(random_state=0)),\n",
    "                        ('lrv_preprocess7', preprocess_step),\n",
    "                        ('lrvclassifer7', LogisticRegressionCV(cv=5, max_iter=500, random_state=0))])\n",
    "    pipe_lrv_8=Pipeline([('lrvscaling8', MinMaxScaler()),\n",
    "                        ('lrvimbalance8', SMOTE(random_state=0)),\n",
    "                        ('lrv_preprocess8', preprocess_step),\n",
    "                        ('lrvclassifer8', LogisticRegressionCV(cv=5, max_iter=500, random_state=0))])\n",
    "    pipe_lrv_9=Pipeline([('lrvscaling9', MinMaxScaler()),\n",
    "                        ('lrvimbalance9', ADASYN(random_state=0)),\n",
    "                        ('lrv_preprocess9', preprocess_step),\n",
    "                        ('lrvclassifer9', LogisticRegressionCV(cv=5, max_iter=500, random_state=0))])\n",
    "    pipe_lrv_10=Pipeline([('lrvscaling10', RobustScaler()),\n",
    "                        ('lrvimbalance10', RandomOverSampler(random_state=0)),\n",
    "                        ('lrv_preprocess10', preprocess_step),\n",
    "                        ('lrvclassifer10', LogisticRegressionCV(cv=5, max_iter=500, random_state=0))])\n",
    "    pipe_lrv_11=Pipeline([('lrvscaling11', RobustScaler()),\n",
    "                        ('lrvimbalance11', SMOTE(random_state=0)),\n",
    "                        ('lrv_preprocess11', preprocess_step),\n",
    "                        ('lrvclassifer11', LogisticRegressionCV(cv=5, max_iter=500, random_state=0))])\n",
    "    pipe_lrv_12=Pipeline([('lrvscaling11', RobustScaler()),\n",
    "                        ('lrvimbalance12', ADASYN(random_state=0)),\n",
    "                        ('lrv_preprocess12', preprocess_step),\n",
    "                        ('lrvclassifer12', LogisticRegressionCV(cv=5, max_iter=500, random_state=0))])\n",
    "    \n",
    "    \n",
    "    #Logistic Regression pipeline with Data imbalance and Scaling fucntions\n",
    "    pipe_lr_1=Pipeline([('lrscaling1', StandardScaler()),\n",
    "                        ('lrimbalance1', RandomOverSampler(random_state=0)),\n",
    "                        ('lr_preprocess1', preprocess_step),\n",
    "                        ('lrclassifer1', LogisticRegression(max_iter=500, random_state=0))])\n",
    "    pipe_lr_2=Pipeline([('lrscaling2', StandardScaler()),\n",
    "                        ('lrimbalance2', SMOTE(random_state=0)),\n",
    "                        ('lr_preprocess2', preprocess_step),\n",
    "                        ('lrclassifer2', LogisticRegression(max_iter=500, random_state=0))])\n",
    "    pipe_lr_3=Pipeline([('lrscaling3', StandardScaler()),\n",
    "                        ('lrimbalance3', ADASYN(random_state=0)),\n",
    "                        ('lr_preprocess3', preprocess_step),\n",
    "                        ('lrclassifer3', LogisticRegression(max_iter=500, random_state=0))])\n",
    "    pipe_lr_4=Pipeline([('lrscaling4', Normalizer()),\n",
    "                        ('lrimbalance4', RandomOverSampler(random_state=0)),\n",
    "                        ('lr_preprocess4', preprocess_step),\n",
    "                        ('lrclassifer4', LogisticRegression(max_iter=500, random_state=0))])\n",
    "    pipe_lr_5=Pipeline([('lrscaling5', Normalizer()),\n",
    "                        ('lrimbalance5', SMOTE(random_state=0)),\n",
    "                        ('lr_preprocess5', preprocess_step),\n",
    "                        ('lrclassifer5', LogisticRegression(max_iter=500, random_state=0))])\n",
    "    pipe_lr_6=Pipeline([('lrscaling6', Normalizer()),\n",
    "                        ('lrimbalance6', ADASYN(random_state=0)),\n",
    "                        ('lr_preprocess6', preprocess_step),\n",
    "                        ('lrclassifer6', LogisticRegression(max_iter=500, random_state=0))])\n",
    "    pipe_lr_7=Pipeline([('lrvscaling7', MinMaxScaler()),\n",
    "                        ('lrimbalance7', RandomOverSampler(random_state=0)),\n",
    "                        ('lr_preprocess7', preprocess_step),\n",
    "                        ('lrclassifer7', LogisticRegression(max_iter=500, random_state=0))])\n",
    "    pipe_lr_8=Pipeline([('lrscaling8', MinMaxScaler()),\n",
    "                        ('lrvimbalance8', SMOTE(random_state=0)),\n",
    "                        ('lr_preprocess8', preprocess_step),\n",
    "                        ('lrclassifer8', LogisticRegression(max_iter=500, random_state=0))])\n",
    "    pipe_lr_9=Pipeline([('lrscaling9', MinMaxScaler()),\n",
    "                        ('lrimbalance9', ADASYN(random_state=0)),\n",
    "                        ('lr_preprocess9', preprocess_step),\n",
    "                        ('lrclassifer9', LogisticRegression(max_iter=500, random_state=0))])\n",
    "    pipe_lr_10=Pipeline([('lrvscaling10', RobustScaler()),\n",
    "                        ('lrimbalance10', RandomOverSampler(random_state=0)),\n",
    "                        ('lr_preprocess10', preprocess_step),\n",
    "                        ('lrclassifer10', LogisticRegression(max_iter=500, random_state=0))])\n",
    "    pipe_lr_11=Pipeline([('lrscaling11', RobustScaler()),\n",
    "                        ('lrvimbalance11', SMOTE(random_state=0)),\n",
    "                        ('lr_preprocess11', preprocess_step),\n",
    "                        ('lrclassifer11', LogisticRegression(max_iter=500, random_state=0))])\n",
    "    pipe_lr_12=Pipeline([('lrscaling12', RobustScaler()),\n",
    "                        ('lrimbalance12', ADASYN(random_state=0)),\n",
    "                        ('lr_preprocess12', preprocess_step),\n",
    "                        ('lrclassifer12', LogisticRegression(max_iter=500, random_state=0))])\n",
    "     \n",
    "    \n",
    "    #Random Forest pipeline with Data imbalance and Scaling fucntions\n",
    "    pipe_rf_1=Pipeline([('rfscaling1', StandardScaler()),\n",
    "                        ('rfimbalance1', RandomOverSampler(random_state=0)),\n",
    "                        ('rf_preprocess1', preprocess_step),\n",
    "                        ('rfclassifer1', RandomForestClassifier(random_state=0, criterion='gini'))])\n",
    "    pipe_rf_2=Pipeline([('rfscaling2', StandardScaler()),\n",
    "                        ('rfimbalance2', SMOTE(random_state=0)),\n",
    "                        ('rf_preprocess2', preprocess_step),\n",
    "                        ('rfclassifer2', RandomForestClassifier(random_state=0, criterion='gini'))])\n",
    "    pipe_rf_3=Pipeline([('rfscaling3', StandardScaler()),\n",
    "                        ('rfimbalance3', ADASYN(random_state=0)),\n",
    "                        ('rf_preprocess3', preprocess_step),\n",
    "                        ('rfclassifer3', RandomForestClassifier(random_state=0, criterion='gini'))])\n",
    "    pipe_rf_4=Pipeline([('rfscaling4', Normalizer()),\n",
    "                        ('rfimbalance4', RandomOverSampler(random_state=0)),\n",
    "                        ('rf_preprocess4', preprocess_step),\n",
    "                        ('rfclassifer4', RandomForestClassifier(random_state=0, criterion='gini'))])\n",
    "    pipe_rf_5=Pipeline([('rfscaling5', Normalizer()),\n",
    "                        ('rfimbalance5', SMOTE(random_state=0)),\n",
    "                        ('rf_preprocess5', preprocess_step),\n",
    "                        ('rfclassifer5', RandomForestClassifier(random_state=0, criterion='gini'))])\n",
    "    pipe_rf_6=Pipeline([('rfscaling6', Normalizer()),\n",
    "                        ('rfimbalance6', ADASYN(random_state=0)),\n",
    "                        ('rf_preprocess6', preprocess_step),\n",
    "                        ('rfclassifer6', RandomForestClassifier(random_state=0, criterion='gini'))])\n",
    "    pipe_rf_7=Pipeline([('rfscaling7', MinMaxScaler()),\n",
    "                        ('rfimbalance7', RandomOverSampler(random_state=0)),\n",
    "                        ('rf_preprocess7', preprocess_step),\n",
    "                        ('rfclassifer7', RandomForestClassifier(random_state=0, criterion='gini'))])\n",
    "    pipe_rf_8=Pipeline([('rfscaling8', MinMaxScaler()),\n",
    "                        ('rfimbalance8', SMOTE(random_state=0)),\n",
    "                        ('rf_preprocess8', preprocess_step),\n",
    "                        ('rfclassifer8', RandomForestClassifier(random_state=0, criterion='gini'))])\n",
    "    pipe_rf_9=Pipeline([('rfscaling9', MinMaxScaler()),\n",
    "                        ('rfimbalance9', ADASYN(random_state=0)),\n",
    "                        ('rf_preprocess9', preprocess_step),\n",
    "                        ('rfclassifer9', RandomForestClassifier(random_state=0, criterion='gini'))])\n",
    "    pipe_rf_10=Pipeline([('rfscaling10', RobustScaler()),\n",
    "                        ('rfimbalance10', RandomOverSampler(random_state=0)),\n",
    "                        ('rf_preprocess10', preprocess_step),\n",
    "                        ('rfclassifer10', RandomForestClassifier(random_state=0, criterion='gini'))])\n",
    "    pipe_rf_11=Pipeline([('rfscaling11', RobustScaler()),\n",
    "                        ('rfimbalance11', SMOTE(random_state=0)),\n",
    "                        ('rf_preprocess11', preprocess_step),\n",
    "                        ('rfclassifer11', RandomForestClassifier(random_state=0, criterion='gini'))])\n",
    "    pipe_rf_12=Pipeline([('rfscaling12', RobustScaler()),\n",
    "                        ('rfimbalance12', ADASYN(random_state=0)),\n",
    "                        ('rf_preprocess12', preprocess_step),\n",
    "                        ('rfclassifer12', RandomForestClassifier(random_state=0, criterion='gini'))])\n",
    "\n",
    "    #Gradient Boosting Classifier pipeline with Data imbalance and Scaling fucntions\n",
    "    pipe_gbc_1=Pipeline([('gbcscaling1', StandardScaler()),\n",
    "                        ('gbcimbalance1', RandomOverSampler(random_state=0)),\n",
    "                        ('gbc_preprocess1', preprocess_step),\n",
    "                        ('gbcclassifer1', GradientBoostingClassifier(random_state=0))])\n",
    "    pipe_gbc_2=Pipeline([('gbcscaling2', StandardScaler()),\n",
    "                        ('gbcimbalance2', SMOTE(random_state=0)),\n",
    "                        ('gbc_preprocess2', preprocess_step),\n",
    "                        ('gbcclassifer2', GradientBoostingClassifier(random_state=0))])\n",
    "    pipe_gbc_3=Pipeline([('gbcscaling3', StandardScaler()),\n",
    "                        ('gbcimbalance3', ADASYN(random_state=0)),\n",
    "                        ('gbc_preprocess3', preprocess_step),\n",
    "                        ('gbcclassifer3', GradientBoostingClassifier(random_state=0))])\n",
    "    pipe_gbc_4=Pipeline([('gbcscaling4', Normalizer()),\n",
    "                        ('gbcimbalance4', RandomOverSampler(random_state=0)),\n",
    "                        ('gbc_preprocess4', preprocess_step),\n",
    "                        ('gbcclassifer4', GradientBoostingClassifier(random_state=0))])\n",
    "    pipe_gbc_5=Pipeline([('gbcscaling5', Normalizer()),\n",
    "                        ('gbcimbalance5', SMOTE(random_state=0)),\n",
    "                        ('gbc_preprocess5', preprocess_step),\n",
    "                        ('gbcclassifer5', GradientBoostingClassifier(random_state=0))])\n",
    "    pipe_gbc_6=Pipeline([('gbcscaling6', Normalizer()),\n",
    "                        ('gbcimbalance6', ADASYN(random_state=0)),\n",
    "                        ('gbc_preprocess6', preprocess_step),\n",
    "                        ('gbcclassifer6', GradientBoostingClassifier(random_state=0))])\n",
    "    pipe_gbc_7=Pipeline([('gbcscaling7', MinMaxScaler()),\n",
    "                        ('gbcimbalance7', RandomOverSampler(random_state=0)),\n",
    "                        ('gbc_preprocess7', preprocess_step),\n",
    "                        ('gbcclassifer7', GradientBoostingClassifier(random_state=0))])\n",
    "    pipe_gbc_8=Pipeline([('gbcscaling8', MinMaxScaler()),\n",
    "                        ('gbcimbalance8', SMOTE(random_state=0)),\n",
    "                        ('gbc_preprocess8', preprocess_step),\n",
    "                        ('gbcclassifer8', GradientBoostingClassifier(random_state=0))])\n",
    "    pipe_gbc_9=Pipeline([('gbcscaling9', MinMaxScaler()),\n",
    "                        ('gbcimbalance9', ADASYN(random_state=0)),\n",
    "                        ('gbc_preprocess9', preprocess_step),\n",
    "                        ('gbcclassifer9', GradientBoostingClassifier(random_state=0))]) \n",
    "    pipe_gbc_10=Pipeline([('gbcscaling10', RobustScaler()),\n",
    "                        ('gbcimbalance10', RandomOverSampler(random_state=0)),\n",
    "                        ('gbc_preprocess10', preprocess_step),\n",
    "                        ('gbcclassifer10', GradientBoostingClassifier(random_state=0))])\n",
    "    pipe_gbc_11=Pipeline([('gbcscaling11', RobustScaler()),\n",
    "                        ('gbcimbalance11', SMOTE(random_state=0)),\n",
    "                        ('gbc_preprocess11', preprocess_step),\n",
    "                        ('gbcclassifer11', GradientBoostingClassifier(random_state=0))])\n",
    "    pipe_gbc_12=Pipeline([('gbcscaling12', RobustScaler()),\n",
    "                        ('gbcimbalance12', ADASYN(random_state=0)),\n",
    "                        ('gbc_preprocess12', preprocess_step),\n",
    "                        ('gbcclassifer12', GradientBoostingClassifier(random_state=0))])  \n",
    "\n",
    "    #Light Gradient Boosting Classifier pipeline with Data imbalance and Scaling fucntions\n",
    "    pipe_lgbm_1=Pipeline([('lgbmscaling1', StandardScaler()),\n",
    "                        ('lgbmimbalance1', RandomOverSampler(random_state=0)),\n",
    "                        ('lgbm_preprocess1', preprocess_step),\n",
    "                        ('lgbmclassifer1', LGBMClassifier(random_state=0))])\n",
    "    pipe_lgbm_2=Pipeline([('lgbmscaling2', StandardScaler()),\n",
    "                        ('lgbmimbalance2', SMOTE(random_state=0)),\n",
    "                        ('lgbm_preprocess2', preprocess_step),\n",
    "                        ('lgbmclassifer2', LGBMClassifier(random_state=0))])\n",
    "    pipe_lgbm_3=Pipeline([('lgbmscaling3', StandardScaler()),\n",
    "                        ('lgbmimbalance3', ADASYN(random_state=0)),\n",
    "                        ('lgbm_preprocess3', preprocess_step),\n",
    "                        ('lgbmclassifer3', LGBMClassifier(random_state=0))])\n",
    "    pipe_lgbm_4=Pipeline([('lgbmscaling4', Normalizer()),\n",
    "                        ('lgbmimbalance4', RandomOverSampler(random_state=0)),\n",
    "                        ('lgbm_preprocess4', preprocess_step),\n",
    "                        ('lgbmclassifer4', LGBMClassifier(random_state=0))])\n",
    "    pipe_lgbm_5=Pipeline([('lgbmscaling5', Normalizer()),\n",
    "                        ('lgbmimbalance5', SMOTE(random_state=0)),\n",
    "                        ('lgbm_preprocess5', preprocess_step),\n",
    "                        ('lgbmclassifer5', LGBMClassifier(random_state=0))])\n",
    "    pipe_lgbm_6=Pipeline([('lgbmscaling6', Normalizer()),\n",
    "                        ('lgbmimbalance6', ADASYN(random_state=0)),\n",
    "                        ('lgbm_preprocess6', preprocess_step),\n",
    "                        ('lgbmclassifer6', LGBMClassifier(random_state=0))])\n",
    "    pipe_lgbm_7=Pipeline([('lgbmscaling7', MinMaxScaler()),\n",
    "                        ('lgbmimbalance7', RandomOverSampler(random_state=0)),\n",
    "                        ('lgbm_preprocess7', preprocess_step),\n",
    "                        ('lgbmclassifer7', LGBMClassifier(random_state=0))])\n",
    "    pipe_lgbm_8=Pipeline([('lgbmscaling8', MinMaxScaler()),\n",
    "                        ('lgbmimbalance8', SMOTE(random_state=0)),\n",
    "                        ('lgbm_preprocess8', preprocess_step),\n",
    "                        ('lgbmclassifer8', LGBMClassifier(random_state=0))])\n",
    "    pipe_lgbm_9=Pipeline([('lgbmscaling9', MinMaxScaler()),\n",
    "                        ('lgbmimbalance9', ADASYN(random_state=0)),\n",
    "                        ('lgbm_preprocess9', preprocess_step),\n",
    "                        ('lgbmclassifer9', LGBMClassifier(random_state=0))])   \n",
    "    pipe_lgbm_10=Pipeline([('lgbmscaling10', RobustScaler()),\n",
    "                        ('lgbmimbalance10', RandomOverSampler(random_state=0)),\n",
    "                        ('lgbm_preprocess10', preprocess_step),\n",
    "                        ('lgbmclassifer10', LGBMClassifier(random_state=0))])\n",
    "    pipe_lgbm_11=Pipeline([('lgbmscaling11', RobustScaler()),\n",
    "                        ('lgbmimbalance11', SMOTE(random_state=0)),\n",
    "                        ('lgbm_preprocess11', preprocess_step),\n",
    "                        ('lgbmclassifer11', LGBMClassifier(random_state=0))])\n",
    "    pipe_lgbm_12=Pipeline([('lgbmscaling12', RobustScaler()),\n",
    "                        ('lgbmimbalance12', ADASYN(random_state=0)),\n",
    "                        ('lgbm_preprocess12', preprocess_step),\n",
    "                        ('lgbmclassifer12', LGBMClassifier(random_state=0))])   \n",
    "    \n",
    "    \n",
    "  \n",
    "    #Xtreme Gradient Boosting Classifier pipeline with Data imbalance and Scaling fucntions\n",
    "    pipe_xgb_1=Pipeline([('xgbscaling1', StandardScaler()),\n",
    "                        ('xgbimbalance1', RandomOverSampler(random_state=0)),\n",
    "                        ('xgb_preprocess1', preprocess_step),\n",
    "                        ('xgbclassifer1', XGBClassifier(random_state=0))])\n",
    "    pipe_xgb_2=Pipeline([('xgbscaling2', StandardScaler()),\n",
    "                        ('xgbimbalance2', SMOTE(random_state=0)),\n",
    "                        ('xgb_preprocess2', preprocess_step),\n",
    "                        ('xgbclassifer2', XGBClassifier(random_state=0))])\n",
    "    pipe_xgb_3=Pipeline([('xgbscaling3', StandardScaler()),\n",
    "                        ('xgbimbalance3', ADASYN(random_state=0)),\n",
    "                        ('xgb_preprocess3', preprocess_step),\n",
    "                        ('xgbclassifer3', XGBClassifier(random_state=0))])\n",
    "    pipe_xgb_4=Pipeline([('xgbscaling4', Normalizer()),\n",
    "                        ('xgbimbalance4', RandomOverSampler(random_state=0)),\n",
    "                        ('xgb_preprocess4', preprocess_step),\n",
    "                        ('xgbclassifer4', XGBClassifier(random_state=0))])\n",
    "    pipe_xgb_5=Pipeline([('xgbscaling5', Normalizer()),\n",
    "                        ('xgbimbalance5', SMOTE(random_state=0)),\n",
    "                        ('xgb_preprocess5', preprocess_step),\n",
    "                        ('xgbclassifer5', XGBClassifier(random_state=0))])\n",
    "    pipe_xgb_6=Pipeline([('xgbscaling6', Normalizer()),\n",
    "                        ('xgbimbalance6', ADASYN(random_state=0)),\n",
    "                        ('xgb_preprocess6', preprocess_step),\n",
    "                        ('xgbclassifer6', XGBClassifier(random_state=0))])\n",
    "    pipe_xgb_7=Pipeline([('xgbscaling7', MinMaxScaler()),\n",
    "                        ('xgbimbalance7', RandomOverSampler(random_state=0)),\n",
    "                        ('xgb_preprocess7', preprocess_step),\n",
    "                        ('xgbclassifer7', XGBClassifier(random_state=0))])\n",
    "    pipe_xgb_8=Pipeline([('xgbscaling8', MinMaxScaler()),\n",
    "                        ('xgbimbalance8', SMOTE(random_state=0)),\n",
    "                        ('xgb_preprocess8', preprocess_step),\n",
    "                        ('xgbclassifer8', XGBClassifier(random_state=0))])\n",
    "    pipe_xgb_9=Pipeline([('xgbscaling9', MinMaxScaler()),\n",
    "                        ('xgbimbalance9', ADASYN(random_state=0)),\n",
    "                        ('xgb_preprocess9', preprocess_step),\n",
    "                        ('xgbclassifer9', XGBClassifier(random_state=0))])   \n",
    "    pipe_xgb_10=Pipeline([('xgbscaling10', RobustScaler()),\n",
    "                        ('xgbimbalance10', RandomOverSampler(random_state=0)),\n",
    "                        ('xgb_preprocess10', preprocess_step),\n",
    "                        ('xgbclassifer10', XGBClassifier(random_state=0))])\n",
    "    pipe_xgb_11=Pipeline([('xgbscaling11', RobustScaler()),\n",
    "                        ('xgbimbalance11', SMOTE(random_state=0)),\n",
    "                        ('xgb_preprocess11', preprocess_step),\n",
    "                        ('xgbclassifer11', XGBClassifier(random_state=0))])\n",
    "    pipe_xgb_12=Pipeline([('xgbscaling12', RobustScaler()),\n",
    "                        ('xgbimbalance12', ADASYN(random_state=0)),\n",
    "                        ('xgb_preprocess12', preprocess_step),\n",
    "                        ('xgbclassifer12', XGBClassifier(random_state=0))])   \n",
    " \n",
    "    #Adaptive Boosting Classifier pipeline with Data imbalance and Scaling fucntions\n",
    "    pipe_ada_1=Pipeline([('adascaling1', StandardScaler()),\n",
    "                        ('adaimbalance1', RandomOverSampler(random_state=0)),\n",
    "                        ('ada_preprocess1', preprocess_step),\n",
    "                        ('adaclassifer1', AdaBoostClassifier(n_estimators=50, learning_rate=0.1, random_state=0))])\n",
    "    pipe_ada_2=Pipeline([('adascaling2', StandardScaler()),\n",
    "                        ('adaimbalance2', SMOTE(random_state=0)),\n",
    "                        ('ada_preprocess2', preprocess_step),\n",
    "                        ('adaclassifer2', AdaBoostClassifier(n_estimators=50, learning_rate=0.1, random_state=0))])\n",
    "    pipe_ada_3=Pipeline([('adascaling3', StandardScaler()),\n",
    "                        ('adaimbalance3', ADASYN(random_state=0)),\n",
    "                        ('ada_preprocess3', preprocess_step),\n",
    "                        ('adaclassifer3', AdaBoostClassifier(n_estimators=50, learning_rate=0.1, random_state=0))])\n",
    "    pipe_ada_4=Pipeline([('adascaling4', Normalizer()),\n",
    "                        ('adaimbalance4', RandomOverSampler(random_state=0)),\n",
    "                        ('ada_preprocess4', preprocess_step),\n",
    "                        ('adaclassifer4', AdaBoostClassifier(n_estimators=50, learning_rate=0.1, random_state=0))])\n",
    "    pipe_ada_5=Pipeline([('adascaling5', Normalizer()),\n",
    "                        ('adaimbalance5', SMOTE(random_state=0)),\n",
    "                        ('ada_preprocess5', preprocess_step),\n",
    "                        ('adaclassifer5', AdaBoostClassifier(n_estimators=50, learning_rate=0.1, random_state=0))])\n",
    "    pipe_ada_6=Pipeline([('adascaling6', Normalizer()),\n",
    "                        ('adaimbalance6', ADASYN(random_state=0)),\n",
    "                        ('ada_preprocess6', preprocess_step),\n",
    "                        ('adaclassifer6', AdaBoostClassifier(n_estimators=50, learning_rate=0.1, random_state=0))])\n",
    "    pipe_ada_7=Pipeline([('adascaling7', MinMaxScaler()),\n",
    "                        ('adaimbalance7', RandomOverSampler(random_state=0)),\n",
    "                        ('ada_preprocess7', preprocess_step),\n",
    "                        ('adaclassifer7', AdaBoostClassifier(n_estimators=50, learning_rate=0.1, random_state=0))])\n",
    "    pipe_ada_8=Pipeline([('adascaling8', MinMaxScaler()),\n",
    "                        ('adaimbalance8', SMOTE(random_state=0)),\n",
    "                        ('ada_preprocess8', preprocess_step),\n",
    "                        ('adaclassifer8', AdaBoostClassifier(n_estimators=50, learning_rate=0.1, random_state=0))])\n",
    "    pipe_ada_9=Pipeline([('adascaling9', MinMaxScaler()),\n",
    "                        ('adaimbalance9', ADASYN(random_state=0)),\n",
    "                        ('ada_preprocess9', preprocess_step),\n",
    "                        ('adaclassifer9', AdaBoostClassifier(n_estimators=50, learning_rate=0.1, random_state=0))])   \n",
    "    pipe_ada_10=Pipeline([('adascaling10', RobustScaler()),\n",
    "                        ('adaimbalance10', RandomOverSampler(random_state=0)),\n",
    "                        ('ada_preprocess10', preprocess_step),\n",
    "                        ('adaclassifer10', AdaBoostClassifier(n_estimators=50, learning_rate=0.1, random_state=0))])\n",
    "    pipe_ada_11=Pipeline([('adascaling11', RobustScaler()),\n",
    "                        ('adaimbalance11', SMOTE(random_state=0)),\n",
    "                        ('ada_preprocess11', preprocess_step),\n",
    "                        ('adaclassifer11', AdaBoostClassifier(n_estimators=50, learning_rate=0.1, random_state=0))])\n",
    "    pipe_ada_12=Pipeline([('adascaling12', RobustScaler()),\n",
    "                        ('adaimbalance12', ADASYN(random_state=0)),\n",
    "                        ('ada_preprocess12', preprocess_step),\n",
    "                        ('adaclassifer12', AdaBoostClassifier(n_estimators=50, learning_rate=0.1, random_state=0))])    \n",
    "\n",
    "\n",
    "    #Catboost Classifier pipeline with Data imbalance and Scaling fucntions\n",
    "    pipe_catb_1=Pipeline([('adascaling1', StandardScaler()),\n",
    "                        ('adaimbalance1', RandomOverSampler(random_state=0)),\n",
    "                        ('ada_preprocess1', preprocess_step),\n",
    "                        ('adaclassifer1', CatBoostClassifier(iterations=50, learning_rate=0.1, random_state=0, verbose=0))])\n",
    "    pipe_catb_2=Pipeline([('adascaling2', StandardScaler()),\n",
    "                        ('adaimbalance2', SMOTE(random_state=0)),\n",
    "                        ('ada_preprocess2', preprocess_step),\n",
    "                        ('adaclassifer2', CatBoostClassifier(iterations=50, learning_rate=0.1, random_state=0, verbose=0))])\n",
    "    pipe_catb_3=Pipeline([('adascaling3', StandardScaler()),\n",
    "                        ('adaimbalance3', ADASYN(random_state=0)),\n",
    "                        ('ada_preprocess3', preprocess_step),\n",
    "                        ('adaclassifer3', CatBoostClassifier(iterations=50, learning_rate=0.1, random_state=0, verbose=0))])\n",
    "    pipe_catb_4=Pipeline([('adascaling4', Normalizer()),\n",
    "                        ('adaimbalance4', RandomOverSampler(random_state=0)),\n",
    "                        ('ada_preprocess4', preprocess_step),\n",
    "                        ('adaclassifer4', CatBoostClassifier(iterations=50, learning_rate=0.1, random_state=0, verbose=0))])\n",
    "    pipe_catb_5=Pipeline([('adascaling5', Normalizer()),\n",
    "                        ('adaimbalance5', SMOTE(random_state=0)),\n",
    "                        ('ada_preprocess5', preprocess_step),\n",
    "                        ('adaclassifer5', CatBoostClassifier(iterations=50, learning_rate=0.1, random_state=0, verbose=0))])\n",
    "    pipe_catb_6=Pipeline([('adascaling6', Normalizer()),\n",
    "                        ('adaimbalance6', ADASYN(random_state=0)),\n",
    "                        ('ada_preprocess6', preprocess_step),\n",
    "                        ('adaclassifer6', CatBoostClassifier(iterations=50, learning_rate=0.1, random_state=0, verbose=0))])\n",
    "    pipe_catb_7=Pipeline([('adascaling7', MinMaxScaler()),\n",
    "                        ('adaimbalance7', RandomOverSampler(random_state=0)),\n",
    "                        ('ada_preprocess7', preprocess_step),\n",
    "                        ('adaclassifer7', CatBoostClassifier(iterations=50, learning_rate=0.1, random_state=0, verbose=0))])\n",
    "    pipe_catb_8=Pipeline([('adascaling8', MinMaxScaler()),\n",
    "                        ('adaimbalance8', SMOTE(random_state=0)),\n",
    "                        ('ada_preprocess8', preprocess_step),\n",
    "                        ('adaclassifer8', CatBoostClassifier(iterations=50, learning_rate=0.1, random_state=0, verbose=0))])\n",
    "    pipe_catb_9=Pipeline([('adascaling9', MinMaxScaler()),\n",
    "                        ('adaimbalance9', ADASYN(random_state=0)),\n",
    "                        ('ada_preprocess9', preprocess_step),\n",
    "                        ('adaclassifer9', CatBoostClassifier(iterations=50, learning_rate=0.1, random_state=0, verbose=0))])\n",
    "    pipe_catb_10=Pipeline([('adascaling10', RobustScaler()),\n",
    "                        ('adaimbalance10', RandomOverSampler(random_state=0)),\n",
    "                        ('ada_preprocess10', preprocess_step),\n",
    "                        ('adaclassifer10', CatBoostClassifier(iterations=50, learning_rate=0.1, random_state=0, verbose=0))])\n",
    "    pipe_catb_11=Pipeline([('adascaling11', RobustScaler()),\n",
    "                        ('adaimbalance11', SMOTE(random_state=0)),\n",
    "                        ('ada_preprocess11', preprocess_step),\n",
    "                        ('adaclassifer11', CatBoostClassifier(iterations=50, learning_rate=0.1, random_state=0, verbose=0))])\n",
    "    pipe_catb_12=Pipeline([('adascaling12', RobustScaler()),\n",
    "                        ('adaimbalance12', ADASYN(random_state=0)),\n",
    "                        ('ada_preprocess12', preprocess_step),\n",
    "                        ('adaclassifer12', CatBoostClassifier(iterations=50, learning_rate=0.1, random_state=0, verbose=0))])\n",
    "    \n",
    "    pipelines=[pipe_lrv_1,pipe_lrv_2,pipe_lrv_3,pipe_lrv_4,pipe_lrv_5,pipe_lrv_6,pipe_lrv_7,pipe_lrv_8,pipe_lrv_9,pipe_lrv_10,pipe_lrv_11,pipe_lrv_12,\n",
    "            pipe_lr_1,pipe_lr_2,pipe_lr_3,pipe_lr_4,pipe_lr_5,pipe_lr_6,pipe_lr_7,pipe_lr_8,pipe_lr_9,pipe_lr_10,pipe_lr_11,pipe_lr_12,\n",
    "            pipe_rf_1,pipe_rf_2,pipe_rf_3,pipe_rf_4,pipe_rf_5,pipe_rf_6,pipe_rf_7,pipe_rf_8,pipe_rf_9,pipe_rf_10,pipe_rf_11,pipe_rf_12,\n",
    "            pipe_gbc_1,pipe_gbc_2,pipe_gbc_3,pipe_gbc_4,pipe_gbc_5,pipe_gbc_6,pipe_gbc_7,pipe_gbc_8,pipe_gbc_9,pipe_gbc_10,pipe_gbc_11,pipe_gbc_12,\n",
    "            pipe_lgbm_1,pipe_lgbm_2,pipe_lgbm_3,pipe_lgbm_4,pipe_lgbm_5,pipe_lgbm_6,pipe_lgbm_7,pipe_lgbm_8,pipe_lgbm_9,pipe_lgbm_10,pipe_lgbm_11,pipe_lgbm_12,\n",
    "            pipe_xgb_1,pipe_xgb_2,pipe_xgb_3,pipe_xgb_4,pipe_xgb_5,pipe_xgb_6,pipe_xgb_7,pipe_xgb_8,pipe_xgb_9,pipe_xgb_10,pipe_xgb_11,pipe_xgb_12,\n",
    "            pipe_ada_1,pipe_ada_2,pipe_ada_3,pipe_ada_4,pipe_ada_5,pipe_ada_6,pipe_ada_7,pipe_ada_8,pipe_ada_9,pipe_ada_10,pipe_ada_11,pipe_ada_12,\n",
    "            pipe_catb_1,pipe_catb_2,pipe_catb_3,pipe_catb_4,pipe_catb_5,pipe_catb_6,pipe_catb_7,pipe_catb_8,pipe_catb_9,pipe_catb_10,pipe_catb_11,pipe_catb_12]\n",
    "\n",
    "    for pipe in pipelines:\n",
    "        pipe.fit(X_tr, y_tr)\n",
    "  \n",
    "    for i,model in enumerate(pipelines):\n",
    "        if (i>=0 and i<=11):\n",
    "            MLA_compare.loc[row_index, 'MLA Name']='LogRegCV'\n",
    "        elif (i>=12 and i<=23):\n",
    "            MLA_compare.loc[row_index, 'MLA Name']='LogReg'\n",
    "        elif (i>=24 and i<=35):\n",
    "            MLA_compare.loc[row_index, 'MLA Name']='RF'\n",
    "        elif (i>=36 and i<=47):\n",
    "            MLA_compare.loc[row_index, 'MLA Name']='GB'\n",
    "        elif (i>=48 and i<=59):\n",
    "            MLA_compare.loc[row_index, 'MLA Name']='LightGBM'\n",
    "        elif (i>=60 and i<=71):\n",
    "            MLA_compare.loc[row_index, 'MLA Name']='XGB'\n",
    "        elif (i>=72 and i<=83):\n",
    "            MLA_compare.loc[row_index, 'MLA Name']='ADA'\n",
    "        elif (i>=84 and i<=95):\n",
    "            MLA_compare.loc[row_index, 'MLA Name']='CATB'\n",
    "\n",
    "\n",
    "        MLA_compare.loc[row_index, 'MLA Parameters']=str(model.get_params())\n",
    "        MLA_compare.loc[row_index, 'MLA Train Accuracy Mean']=model.score(X_tr,y_tr)\n",
    "        MLA_compare.loc[row_index, 'MLA Test Accuracy Mean']=model.score(X_ts,y_ts)\n",
    "        y_pred = model.predict(X_ts)\n",
    "        MLA_compare.loc[row_index, 'MLA Recall']=metrics.recall_score(y_ts, y_pred, average='weighted')\n",
    "        MLA_compare.loc[row_index, 'MLA Precision']=metrics.precision_score(y_ts, y_pred,average='weighted')\n",
    "        MLA_compare.loc[row_index, 'MLA F1 Score']=metrics.f1_score(y_ts, y_pred, average='weighted')\n",
    "\n",
    "        row_index=row_index+1\n",
    "        \n",
    "\n",
    "        if (model.score(X_ts,y_ts)>best_accuracy):\n",
    "            best_accuracy=model.score(X_ts,y_ts,)\n",
    "            best_accpipeline=model\n",
    "\n",
    "        if (metrics.recall_score(y_ts, y_pred, average='weighted')>best_recall):\n",
    "            best_recall=metrics.recall_score(y_ts, y_pred, average='weighted')\n",
    "            bestrecpipeline=model\n",
    "\n",
    "        if (metrics.precision_score(y_ts, y_pred, average='weighted')>best_precision):\n",
    "            best_precision=metrics.precision_score(y_ts, y_pred, average='weighted')\n",
    "            bestprcpipeline=model\n",
    "\n",
    "        if (metrics.f1_score(y_ts, y_pred, average='weighted')>best_f1):\n",
    "            best_f1=metrics.f1_score(y_ts, y_pred, average='weighted')\n",
    "            bestf1pipeline=model\n",
    "\n",
    "        MLA_compare['Difference']= (MLA_compare['MLA Test Accuracy Mean']-MLA_compare['MLA Train Accuracy Mean'])*100\n",
    "\n",
    "    #Storing the model with best f1 score   \n",
    "    with open(modelfilename, 'wb') as file:  \n",
    "        pickle.dump(bestf1pipeline, file)\n",
    "\n",
    "    return MLA_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_test_accuracy(df):\n",
    "    tmp_df=df.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False)\n",
    "    tmp_df.reset_index(drop=True, inplace=True)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.barplot(x='MLA Name', y='MLA Test Accuracy Mean', data=tmp_df.head(10))\n",
    "    plt.title(\"Mean Test Accuracy of Models\", fontdict={'fontweight':'bold'})\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_accuracy(df):\n",
    "    tmp_df=df.sort_values(by = ['MLA Train Accuracy Mean'], ascending = False)\n",
    "    tmp_df.reset_index(drop=True, inplace=True)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.barplot(x='MLA Name', y='MLA Train Accuracy Mean', data=tmp_df.head(10))\n",
    "    plt.title(\"Mean Train Accuracy of Models\", fontdict={'fontweight':'bold'})\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_recall(df):\n",
    "    tmp_df=df.sort_values(by = ['MLA Recall'], ascending = False)\n",
    "    tmp_df.reset_index(drop=True, inplace=True)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.barplot(x='MLA Name', y='MLA Recall', data=tmp_df.head(10))\n",
    "    plt.title(\"Weighted Recall of Models\", fontdict={'fontweight':'bold'})\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision(df):\n",
    "    tmp_df=df.sort_values(by = ['MLA Precision'], ascending = False)\n",
    "    tmp_df.reset_index(drop=True, inplace=True)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.barplot(x='MLA Name', y='MLA Precision', data=tmp_df.head(10))\n",
    "    plt.title(\"Weighted Precision of Models\", fontdict={'fontweight':'bold'})\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_f1score(df):\n",
    "    tmp_df=df.sort_values(by = ['MLA F1 Score'], ascending = False)\n",
    "    tmp_df.reset_index(drop=True, inplace=True)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.barplot(x='MLA Name', y='MLA F1 Score', data=tmp_df.head(10))\n",
    "    plt.title(\"Weighted F1 Score of Models\", fontdict={'fontweight':'bold'})\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section1401></a>\n",
    "### **14.1 Modelling on basic dataset - adv_data with stratified - False while splitting the data for test/train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=train_predict_default_ml(adv_data, 0, 'model_adv_data_no_stratify').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_test_accuracy(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='MLA Test Accuracy Mean', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of Test accuracy, Xtreme Gradient Boosting, Light Gradient Boosting and Random Forest have perfornmed well than others.\n",
    "But if observed the top 5 results, XBG is the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_accuracy(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='MLA Train Accuracy Mean', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of Train accuracy, Random Forest,XGB are best models similar to test accuracy scenarios.\n",
    "But, here in tersms of Top 5 results, Random Forest is best model, that is expected, as Random Forest teds to overfit, so it may perform better on training data than testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_recall(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='MLA Recall', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of Recall, the result is similar, RF,XGB and LightGBM are best models with XGB has performed better of all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_precision(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='MLA Precision', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of Precision, Catboost Classifier has performed marginally better than other models, but LighGBM and XBG have also performed better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_f1score(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='MLA F1 Score', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of F1 score, XGB,LightGBM and Random Forest have performed better with XGB is better of all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section1402></a>\n",
    "### **14.2 Modelling on basic dataset - adv_data with stratified - True while splitting the data for test/train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=train_predict_default_ml(adv_data_1, 1, 'model_adv_data_stratify').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_test_accuracy(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.sort_values(by='MLA Test Accuracy Mean', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar models as in when stratified=False, but the test accuracy has been increrased with TRUE value for the flag.\n",
    "GB,XGB,Catb and LighGBM are better performed models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_accuracy(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.sort_values(by='MLA Train Accuracy Mean', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same models as in test accuracy scenario, here also when stratified flag is TRUE, train accuracy is increased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_recall(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.sort_values(by='MLA Recall', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar models  - XGB,GB,LightGBM and RF as in earlier cases, the recall is better with stratified=TRUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_precision(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.sort_values(by='MLA Precision', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When stratified is TRUE, AdaBoost has performed better in case of Precsion, but other values are comparatively low for it.\n",
    "Other models like LighGBM, LogisticRegressionCV and XGB have precision marginally low, but better values for other parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_f1score(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.sort_values(by='MLA F1 Score', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of F1 score, Gradient Boosting has performed well along with XGB,LightGBM and Random Forest, But F1 score is reduced marginally when compared to stratified=FALSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary for adv_data with baseline models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGB,LightGBM,Gradient Boosting and Random Forest are the best performing models either with stratified=TRUE or FALSE with CatBoost and Adaboost performed well when stratified=TRUE.\n",
    "All the parameter values have increased slighly with stratified=TRUE for top 5 models, but marginally low value for F1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section1403></a>\n",
    "### **14.3 Modelling on adv_data_1 with stratified - False while splitting the data for test/train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=train_predict_default_ml(adv_data_1, 0, 'model_adv_data_1_no_stratify').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_test_accuracy(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.sort_values(by='MLA Test Accuracy Mean', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar models as in case of adv_data, and test accuracy for top few results is better than what adv_data modelling had with stratified=FALSE, but values are lower when adv_data with stratified=FALSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_accuracy(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.sort_values(by='MLA Train Accuracy Mean', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same models as in case of adv_data with stratified=FALSE and train accuracy values are almost similar in both the cases, but lower when adv_data trained with stratified=FALSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_recall(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.sort_values(by='MLA Recall', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall has been decreased as compred to the results obtained from adv_data with or without stratified set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_precision(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.sort_values(by='MLA Precision', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall precision of the models decreased as compared to adv_data results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_f1score(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.sort_values(by='MLA F1 Score', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 score is almost similar as compared to adv_data results, though marginally high for few top results but overall not that difference.\n",
    "But, other values like precisoon,Recall and accuracy seems to have satisfactory results with better F1 in this case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "Recall and Precison values are marginally low as compared to adv_data results, but accuracy score and F1 score increrased marginally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section1404></a>\n",
    "### **14.4 Modelling on adv_data_1 with stratified - True while splitting the data for test/train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=train_predict_default_ml(adv_data_1, 1, 'model_adv_data_1_stratify').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_test_accuracy(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.sort_values(by='MLA Test Accuracy Mean', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test accuracy almost similar what we have in case of adv_data with stratified=TRUE with same models performing better - Gradient Boosting, XGB,Catboost and LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_accuracy(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.sort_values(by='MLA Train Accuracy Mean', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train accuracy is same a in adv_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_recall(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.sort_values(by='MLA Recall', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar value for Recall as well as compared to adv_data with sane models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_precision(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.sort_values(by='MLA Precision', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same value for precision as compared to adv_data results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_f1score(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.sort_values(by='MLA F1 Score', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 score is also same as in adv_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section1405></a>\n",
    "### **14.5 Modelling on adv_data_2 with stratified - False while splitting the data for test/train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4=train_predict_default_ml(adv_data_2, 0, 'model_adv_data_2_no_stratify').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_test_accuracy(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.sort_values(by='MLA Test Accuracy Mean', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test accuracies have been icnreased as compared to adv_data and adv_data_1 with stratidied=FLASE with same models - XBG,GB nd LighGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_accuracy(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.sort_values(by='MLA Train Accuracy Mean', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train accuracies have also increased with same models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_recall(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.sort_values(by='MLA Recall', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall too have increased with same Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_precision(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.sort_values(by='MLA Precision', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precison also increased as compared to adv_data and adv_data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_f1score(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.sort_values(by='MLA F1 Score', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision also increased with same models as compared to adv_data and adv_data_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "With adv_data_2, all the metrics values have increased, so can be considered for further experimentation over other 2 datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section1406></a>\n",
    "### **14.6 Modelling on adv_data_2 with stratified - True while splitting the data for test/train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5=train_predict_default_ml(adv_data_2, 1, 'model_adv_data_2_stratify').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_test_accuracy(df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.sort_values(by='MLA Test Accuracy Mean', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test accuracies have decreased as compared to adv_data and adv_data_1 with stratified=TRUE\n",
    "\n",
    "Also, adv_data_2 with stratified=FALSE has better accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_accuracy(df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.sort_values(by='MLA Train Accuracy Mean', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train accuracies have increased and marginally higher than other datasets with adv_data_2 with stratified=FALSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_recall(df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.sort_values(by='MLA Recall', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall is decreased as compared to other datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_precision(df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.sort_values(by='MLA Precision', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision too decrerased as comapred to other datasets and adv_data_2 with stratified=FALSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_f1score(df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.sort_values(by='MLA F1 Score', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 score also decreased as compared to previous results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section1407></a>\n",
    "### **14.7 Summary on baseline model results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - adv_data_2 with stratified=FALSE has given the best results\n",
    " - XGB,GB,LightGBM and RF are the best performing models \n",
    " \n",
    " \n",
    " **Below are the top 5 performing model configurations (considering F1 score as final evaluation parameter) - all are XGB or LightGBM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.sort_values(by='MLA F1 Score', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also noticed that, StandardScaler and RobustScaler have performed better  - could be due to below reasons-\n",
    " - StandardScaler - ratings and runtime values were in different scale (0-1 and 10-100 respectively), so moved all to same scale\n",
    " - RobustScaler took care of large number of outliers in both the columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section15></a>\n",
    "## **15. Machine learning - Analyzing tuned models (and Ensemble models)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section1501></a>\n",
    "### **15.1 Hyper-parameter tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will tune the parameters for Xtreme Gradient Boosting, LightGBM, Gradient Boosting, Random Forest along with scaling using - StandardScaler, RobustScaler and MinMaxScaler along with data imbalance resolving technique like  - RandomOverSampler, SMOTE and ADASYN\n",
    "\n",
    "Also, useing dataset - adv_data_2 with stratified=FALSE as this dataset provided the best results with baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section15011></a>\n",
    "#### **15.1.1 Hyper parameter tuning in Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to train Random Forest using hyper parameter tuning\n",
    "def process_rf(data,stratified):\n",
    "    \"\"\"\n",
    "    data - input data\n",
    "    stratified flag - wether to set the stratified flag to TRUE or FALSE (always FALSE in this case)\n",
    "    \"\"\"\n",
    "    X=data.drop('netgain', axis=1)\n",
    "    y=data['netgain']\n",
    "    \n",
    "    if (True==stratified):\n",
    "        X_tr, X_ts, y_tr, y_ts = train_test_split(X, y, test_size = .20, random_state = 0, stratify=y, shuffle=True)\n",
    "    else:\n",
    "        X_tr, X_ts, y_tr, y_ts = train_test_split(X, y, test_size = .20, random_state = 0)\n",
    "        \n",
    "    preprocess_step=FeatureUnion([('kbest', SelectKBest(score_func=f_classif, k=15))])\n",
    "    \n",
    "    pipe_1=Pipeline([('scaling', StandardScaler()),\n",
    "                    ('imbalance', RandomOverSampler(random_state=0)),\n",
    "                    ('preprocess', preprocess_step),\n",
    "                   ('classifier', RandomForestClassifier(random_state=0))])\n",
    "    pipe_2=Pipeline([('scaling', StandardScaler()),\n",
    "                    ('imbalance', SMOTE(random_state=0)),\n",
    "                    ('preprocess', preprocess_step),\n",
    "                   ('classifier', RandomForestClassifier(random_state=0))])\n",
    "    pipe_3=Pipeline([('scaling', StandardScaler()),\n",
    "                    ('imbalance', ADASYN(random_state=0)),\n",
    "                    ('preprocess', preprocess_step),\n",
    "                   ('classifier', RandomForestClassifier(random_state=0))])\n",
    "    pipe_4=Pipeline([('scaling', RobustScaler()),\n",
    "                    ('imbalance', RandomOverSampler(random_state=0)),\n",
    "                    ('preprocess', preprocess_step),\n",
    "                   ('classifier', RandomForestClassifier(random_state=0))])\n",
    "    pipe_5=Pipeline([('scaling', RobustScaler()),\n",
    "                    ('imbalance', SMOTE(random_state=0)),\n",
    "                    ('preprocess', preprocess_step),\n",
    "                   ('classifier', RandomForestClassifier(random_state=0))])\n",
    "    pipe_6=Pipeline([('scaling', RobustScaler()),\n",
    "                    ('imbalance', ADASYN(random_state=0)),\n",
    "                    ('preprocess', preprocess_step),\n",
    "                   ('classifier', RandomForestClassifier(random_state=0))])\n",
    "    pipe_7=Pipeline([('scaling', MinMaxScaler()),\n",
    "                    ('imbalance', RandomOverSampler(random_state=0)),\n",
    "                    ('preprocess', preprocess_step),\n",
    "                   ('classifier', RandomForestClassifier(random_state=0))])\n",
    "    pipe_8=Pipeline([('scaling', MinMaxScaler()),\n",
    "                    ('imbalance', SMOTE(random_state=0)),\n",
    "                    ('preprocess', preprocess_step),\n",
    "                   ('classifier', RandomForestClassifier(random_state=0))])\n",
    "    pipe_9=Pipeline([('scaling', MinMaxScaler()),\n",
    "                    ('imbalance', ADASYN(random_state=0)),\n",
    "                    ('preprocess', preprocess_step),\n",
    "                   ('classifier', RandomForestClassifier(random_state=0))])\n",
    "    \n",
    "    \n",
    "    param_grid= {\n",
    "                  'classifier__n_estimators': [None, 10, 100, 500],\n",
    "                  'classifier__criterion':['gini'],\n",
    "                  'classifier__max_depth':range(0,50,2),\n",
    "                  'classifier__min_samples_split':range(0,500, 10),\n",
    "                  'classifier__min_samples_leaf':range(0,20, 1),\n",
    "                  'classifier__max_features':['auto','sqrt','log2']\n",
    "               }\n",
    "    \n",
    "    gs_1=RandomizedSearchCV(pipe_1, param_distributions=param_grid, n_iter=10, cv =10, verbose=True, n_jobs=-1, return_train_score=True, refit='f1_score', scoring='f1')\n",
    "    gs_2=RandomizedSearchCV(pipe_2, param_distributions=param_grid, n_iter=10, cv =10, verbose=True, n_jobs=-1, return_train_score=True, refit='f1_score', scoring='f1')\n",
    "    gs_3=RandomizedSearchCV(pipe_3, param_distributions=param_grid, n_iter=10, cv =10, verbose=True, n_jobs=-1, return_train_score=True, refit='f1_score', scoring='f1')\n",
    "    gs_4=RandomizedSearchCV(pipe_4, param_distributions=param_grid, n_iter=10, cv =10, verbose=True, n_jobs=-1, return_train_score=True, refit='f1_score', scoring='f1')\n",
    "    gs_5=RandomizedSearchCV(pipe_5, param_distributions=param_grid, n_iter=10, cv =10, verbose=True, n_jobs=-1, return_train_score=True, refit='f1_score', scoring='f1')\n",
    "    gs_6=RandomizedSearchCV(pipe_6, param_distributions=param_grid, n_iter=10, cv =10, verbose=True, n_jobs=-1, return_train_score=True, refit='f1_score', scoring='f1')\n",
    "    gs_7=RandomizedSearchCV(pipe_7, param_distributions=param_grid, n_iter=10, cv =10, verbose=True, n_jobs=-1, return_train_score=True, refit='f1_score', scoring='f1')\n",
    "    gs_8=RandomizedSearchCV(pipe_8, param_distributions=param_grid, n_iter=10, cv =10, verbose=True, n_jobs=-1, return_train_score=True, refit='f1_score', scoring='f1')\n",
    "    gs_9=RandomizedSearchCV(pipe_9, param_distributions=param_grid, n_iter=10, cv =10, verbose=True, n_jobs=-1, return_train_score=True, refit='f1_score', scoring='f1')\n",
    "    \n",
    "    grids=[gs_1,gs_2,gs_3,gs_4,gs_5,gs_6,gs_7,gs_8,gs_9]\n",
    "    \n",
    "    print('Performing model optimizations...')\n",
    "   \n",
    "    best_f1=0\n",
    "    \n",
    "    for gs in grids:\n",
    "        gs.fit(X_tr, y_tr)\n",
    "        print(\"Best parameters - : \", gs.best_params_)\n",
    "        print(\"Best training accuracy - :\", gs.best_score_)\n",
    "        y_pred = gs.predict(X_ts)\n",
    "        print(\"Test accuracy -  : \", gs.score(X_ts, y_ts))\n",
    "        print (\"Recall Score - : \", metrics.recall_score(y_ts, y_pred, average='weighted'))\n",
    "        print (\"Precison Score - : \", metrics.precision_score(y_ts, y_pred, average='weighted'))\n",
    "        print (\"F1 Score - : \", metrics.f1_score(y_ts, y_pred, average='weighted'))\n",
    "        \n",
    "        if metrics.f1_score(y_ts, y_pred, average='weighted')>best_f1:\n",
    "            best_f1=metrics.f1_score(y_ts, y_pred, average='weighted')\n",
    "            best_gs=gs\n",
    "     \n",
    "    print(\"===========================================================\")\n",
    "    print (\"Best Model values :\")\n",
    "    print(\"Best parameters - : \", best_gs.best_params_)\n",
    "    print(\"Best training accuracy - :\", best_gs.best_score_)\n",
    "    y_pred = best_gs.predict(X_ts)\n",
    "    print(\"Test accuracy -  : \", best_gs.score(X_ts, y_ts))\n",
    "    print (\"Recall Score - : \", metrics.recall_score(y_ts, y_pred, average='weighted'))\n",
    "    print (\"Precison Score - : \", metrics.precision_score(y_ts, y_pred, average='weighted'))\n",
    "    print (\"F1 Score - : \", metrics.f1_score(y_ts, y_pred, average='weighted'))\n",
    "    \n",
    "    y_pred_proba = best_gs.predict_proba(X_ts)\n",
    "    preds = y_pred_proba[:,1]\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_ts, preds)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    print(roc_auc)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label='AUC ROC (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    print(\"===========================================================\")\n",
    "    \n",
    "    return best_gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf_model=process_rf(adv_data_2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rf_param_model', 'wb') as file:  \n",
    "    pickle.dump(best_rf_model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best Random Forest Model information**\n",
    "\n",
    "**Best Model :** best_rf_model\n",
    "\n",
    "**Best parameters - :**  {'classifier__n_estimators': 500, 'classifier__min_samples_split': 20, 'classifier__min_samples_leaf': 2, 'classifier__max_features': 'log2', 'classifier__max_depth': 48, 'classifier__criterion': 'gini'}\n",
    "\n",
    "**Best training accuracy - :** 0.6171399533822614\n",
    "\n",
    "**Test accuracy -  :**  0.6307458143074581\n",
    "\n",
    "**Recall Score - :**  0.7671785028790787\n",
    "\n",
    "**Precison Score - :**  0.8329584077668938\n",
    "\n",
    "**F1 Score - :**  0.7824949126262221\n",
    "\n",
    "**AUC-ROC Score - :** 0.8650290285244923"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section15012></a>\n",
    "#### **15.1.2 Hyper paramete tuning in Gradient Boosting Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to process Gradient Boosting model usimhg hyper parameter tuning\n",
    "def process_gb(data,stratified):\n",
    "    \n",
    "    \"\"\"\n",
    "    data - input data\n",
    "    stratified flag - wether to set the stratified flag to TRUE or FALSE (always FALSE in this case)\n",
    "    \"\"\"\n",
    "\n",
    "    X=data.drop('netgain', axis=1)\n",
    "    y=data['netgain']\n",
    "    \n",
    "    if (True==stratified):\n",
    "        X_tr, X_ts, y_tr, y_ts = train_test_split(X, y, test_size = .20, random_state = 0, stratify=y, shuffle=True)\n",
    "    else:\n",
    "        X_tr, X_ts, y_tr, y_ts = train_test_split(X, y, test_size = .20, random_state = 0)\n",
    "        \n",
    "    preprocess_step=FeatureUnion([('kbest', SelectKBest(score_func=f_classif, k=15))])\n",
    "    \n",
    "    pipe_1=Pipeline([('scaling', StandardScaler()),\n",
    "                    ('imbalance', RandomOverSampler(random_state=0)),\n",
    "                    ('preprocess', preprocess_step),\n",
    "                   ('classifier', GradientBoostingClassifier(random_state=0))])\n",
    "    pipe_2=Pipeline([('scaling', StandardScaler()),\n",
    "                    ('imbalance', SMOTE(random_state=0)),\n",
    "                    ('preprocess', preprocess_step),\n",
    "                   ('classifier', GradientBoostingClassifier(random_state=0))])\n",
    "    pipe_3=Pipeline([('scaling', StandardScaler()),\n",
    "                    ('imbalance', ADASYN(random_state=0)),\n",
    "                    ('preprocess', preprocess_step),\n",
    "                   ('classifier', GradientBoostingClassifier(random_state=0))])\n",
    "    pipe_4=Pipeline([('scaling', RobustScaler()),\n",
    "                    ('imbalance', RandomOverSampler(random_state=0)),\n",
    "                    ('preprocess', preprocess_step),\n",
    "                   ('classifier', GradientBoostingClassifier(random_state=0))])\n",
    "    pipe_5=Pipeline([('scaling', RobustScaler()),\n",
    "                    ('imbalance', SMOTE(random_state=0)),\n",
    "                    ('preprocess', preprocess_step),\n",
    "                   ('classifier', GradientBoostingClassifier(random_state=0))])\n",
    "    pipe_6=Pipeline([('scaling', RobustScaler()),\n",
    "                    ('imbalance', ADASYN(random_state=0)),\n",
    "                    ('preprocess', preprocess_step),\n",
    "                   ('classifier', GradientBoostingClassifier(random_state=0))])\n",
    "    pipe_7=Pipeline([('scaling', MinMaxScaler()),\n",
    "                    ('imbalance', RandomOverSampler(random_state=0)),\n",
    "                    ('preprocess', preprocess_step),\n",
    "                   ('classifier', GradientBoostingClassifier(random_state=0))])\n",
    "    pipe_8=Pipeline([('scaling', MinMaxScaler()),\n",
    "                    ('imbalance', SMOTE(random_state=0)),\n",
    "                    ('preprocess', preprocess_step),\n",
    "                   ('classifier', GradientBoostingClassifier(random_state=0))])\n",
    "    pipe_9=Pipeline([('scaling', MinMaxScaler()),\n",
    "                    ('imbalance', ADASYN(random_state=0)),\n",
    "                    ('preprocess', preprocess_step),\n",
    "                   ('classifier', GradientBoostingClassifier(random_state=0))])\n",
    "    \n",
    "    \n",
    "    param_grid= {\n",
    "                'classifier__n_estimators': [10, 100,250,500,750,1000,1250,1500,1750],\n",
    "                'classifier__learning_rate': [0.15,0.1,0.05,0.01,0.005,0.001],\n",
    "                'classifier__max_depth':range(2,50,2),\n",
    "                'classifier__min_samples_split':range(2,500, 10),\n",
    "                'classifier__min_samples_leaf':range(1,20, 1),\n",
    "                'classifier__max_features':['auto','sqrt','log2'],\n",
    "                'classifier__subsample': [0.5, 0.6, 0.7,0.75,0.8,0.85,0.9,0.95,1]\n",
    "               }\n",
    "\n",
    "    gs_1=RandomizedSearchCV(pipe_1, param_distributions=param_grid, n_iter=10, cv =10, verbose=True, n_jobs=-1, return_train_score=True, refit='f1_score', scoring='f1')\n",
    "    gs_2=RandomizedSearchCV(pipe_2, param_distributions=param_grid, n_iter=10, cv =10, verbose=True, n_jobs=-1, return_train_score=True, refit='f1_score', scoring='f1')\n",
    "    gs_3=RandomizedSearchCV(pipe_3, param_distributions=param_grid, n_iter=10, cv =10, verbose=True, n_jobs=-1, return_train_score=True, refit='f1_score', scoring='f1')\n",
    "    gs_4=RandomizedSearchCV(pipe_4, param_distributions=param_grid, n_iter=10, cv =10, verbose=True, n_jobs=-1, return_train_score=True, refit='f1_score', scoring='f1')\n",
    "    gs_5=RandomizedSearchCV(pipe_5, param_distributions=param_grid, n_iter=10, cv =10, verbose=True, n_jobs=-1, return_train_score=True, refit='f1_score', scoring='f1')\n",
    "    gs_6=RandomizedSearchCV(pipe_6, param_distributions=param_grid, n_iter=10, cv =10, verbose=True, n_jobs=-1, return_train_score=True, refit='f1_score', scoring='f1')\n",
    "    gs_7=RandomizedSearchCV(pipe_7, param_distributions=param_grid, n_iter=10, cv =10, verbose=True, n_jobs=-1, return_train_score=True, refit='f1_score', scoring='f1')\n",
    "    gs_8=RandomizedSearchCV(pipe_8, param_distributions=param_grid, n_iter=10, cv =10, verbose=True, n_jobs=-1, return_train_score=True, refit='f1_score', scoring='f1')\n",
    "    gs_9=RandomizedSearchCV(pipe_9, param_distributions=param_grid, n_iter=10, cv =10, verbose=True, n_jobs=-1, return_train_score=True, refit='f1_score', scoring='f1')\n",
    "    \n",
    "    grids=[gs_1,gs_2,gs_3,gs_4,gs_5,gs_6,gs_7,gs_8,gs_9]\n",
    "    \n",
    "    print('Performing model optimizations...')\n",
    "   \n",
    "    best_f1=0\n",
    "    \n",
    "    for gs in grids:\n",
    "        gs.fit(X_tr, y_tr)\n",
    "        print(\"Best parameters - : \", gs.best_params_)\n",
    "        print(\"Best training accuracy - :\", gs.best_score_)\n",
    "        y_pred = gs.predict(X_ts)\n",
    "        print(\"Test accuracy -  : \", gs.score(X_ts, y_ts))\n",
    "        print (\"Recall Score - : \", metrics.recall_score(y_ts, y_pred, average='weighted'))\n",
    "        print (\"Precison Score - : \", metrics.precision_score(y_ts, y_pred, average='weighted'))\n",
    "        print (\"F1 Score - : \", metrics.f1_score(y_ts, y_pred, average='weighted'))\n",
    "        \n",
    "        if metrics.f1_score(y_ts, y_pred, average='weighted')>best_f1:\n",
    "            best_f1=metrics.f1_score(y_ts, y_pred, average='weighted')\n",
    "            best_gs=gs\n",
    "     \n",
    "    print(\"===========================================================\")\n",
    "    print (\"Best Model values :\")\n",
    "    print(\"Best parameters - : \", best_gs.best_params_)\n",
    "    print(\"Best training accuracy - :\", best_gs.best_score_)\n",
    "    y_pred = best_gs.predict(X_ts)\n",
    "    print(\"Test accuracy -  : \", best_gs.score(X_ts, y_ts))\n",
    "    print (\"Recall Score - : \", metrics.recall_score(y_ts, y_pred, average='weighted'))\n",
    "    print (\"Precison Score - : \", metrics.precision_score(y_ts, y_pred, average='weighted'))\n",
    "    print (\"F1 Score - : \", metrics.f1_score(y_ts, y_pred, average='weighted'))\n",
    "    \n",
    "    y_pred_proba = best_gs.predict_proba(X_ts)\n",
    "    preds = y_pred_proba[:,1]\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_ts, preds)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    print(roc_auc)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label='AUC ROC (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    print(\"===========================================================\")\n",
    "    \n",
    "    return best_gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_gb_model=process_gb(adv_data_2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gb_param_model', 'wb') as file: \n",
    "    pickle.dump(best_gb_model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best Gradient Boosting Model information**\n",
    "\n",
    "**Best Model :** best_gb_model\n",
    "\n",
    "**Best parameters - :**  {'classifier__subsample': 1, 'classifier__n_estimators': 1250, 'classifier__min_samples_split': 412, 'classifier__min_samples_leaf': 9, 'classifier__max_features': 'auto', 'classifier__max_depth': 38, 'classifier__learning_rate': 0.01}\n",
    "\n",
    "**Best training accuracy - :** 0.6238598604165437\n",
    "\n",
    "**Test accuracy -  :**  0.635900700190961\n",
    "\n",
    "**Recall Score - :**  0.7804222648752399\n",
    "\n",
    "**Precison Score - :**  0.8310411097706167\n",
    "\n",
    "**F1 Score - :**  0.7934883805062174\n",
    "\n",
    "**AUC-ROC Score - :** 0.8660632686289025\n",
    "\n",
    "\n",
    "All the parameters value have been increased as compared to Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section15013></a>\n",
    "#### **15.1.3 Hyper parameter tuning for Xtreme Gradient Boosting Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to process Xtreme Gradient Boosting model using hyper parameter tuning\n",
    "def process_xgb(data,stratified):\n",
    "    \n",
    "    \"\"\"\n",
    "    data - input data\n",
    "    stratified flag - wether to set the stratified flag to TRUE or FALSE (always FALSE in this case)\n",
    "    \"\"\"\n",
    "\n",
    "    X=data.drop('netgain', axis=1)\n",
    "    y=data['netgain']\n",
    "    \n",
    "    if (True==stratified):\n",
    "        X_tr, X_ts, y_tr, y_ts = train_test_split(X, y, test_size = .20, random_state = 0, stratify=y, shuffle=True)\n",
    "    else:\n",
    "        X_tr, X_ts, y_tr, y_ts = train_test_split(X, y, test_size = .20, random_state = 0)\n",
    "        \n",
    "    preprocess_step=FeatureUnion([('kbest', SelectKBest(score_func=f_classif, k=15))])\n",
    "    \n",
    "    pipe_1=Pipeline([('scaling', StandardScaler()),\n",
    "                    ('imbalance', RandomOverSampler(random_state=0)),\n",
    "                    ('preprocess', preprocess_step),\n",
    "                   ('classifier', XGBClassifier(random_state=0))])\n",
    "    pipe_2=Pipeline([('scaling', StandardScaler()),\n",
    "                    ('imbalance', SMOTE(random_state=0)),\n",
    "                    ('preprocess', preprocess_step),\n",
    "                   ('classifier', XGBClassifier(random_state=0))])\n",
    "    pipe_3=Pipeline([('scaling', StandardScaler()),\n",
    "                    ('imbalance', ADASYN(random_state=0)),\n",
    "                    ('preprocess', preprocess_step),\n",
    "                   ('classifier', XGBClassifier(random_state=0))])\n",
    "    pipe_4=Pipeline([('scaling', RobustScaler()),\n",
    "                    ('imbalance', RandomOverSampler(random_state=0)),\n",
    "                    ('preprocess', preprocess_step),\n",
    "                   ('classifier', XGBClassifier(random_state=0))])\n",
    "    pipe_5=Pipeline([('scaling', RobustScaler()),\n",
    "                    ('imbalance', SMOTE(random_state=0)),\n",
    "                    ('preprocess', preprocess_step),\n",
    "                   ('classifier', XGBClassifier(random_state=0))])\n",
    "    pipe_6=Pipeline([('scaling', RobustScaler()),\n",
    "                    ('imbalance', ADASYN(random_state=0)),\n",
    "                    ('preprocess', preprocess_step),\n",
    "                   ('classifier', XGBClassifier(random_state=0))])\n",
    "    pipe_7=Pipeline([('scaling', MinMaxScaler()),\n",
    "                    ('imbalance', RandomOverSampler(random_state=0)),\n",
    "                    ('preprocess', preprocess_step),\n",
    "                   ('classifier', XGBClassifier(random_state=0))])\n",
    "    pipe_8=Pipeline([('scaling', MinMaxScaler()),\n",
    "                    ('imbalance', SMOTE(random_state=0)),\n",
    "                    ('preprocess', preprocess_step),\n",
    "                   ('classifier', XGBClassifier(random_state=0))])\n",
    "    pipe_9=Pipeline([('scaling', MinMaxScaler()),\n",
    "                    ('imbalance', ADASYN(random_state=0)),\n",
    "                    ('preprocess', preprocess_step),\n",
    "                   ('classifier', XGBClassifier(random_state=0))])\n",
    "    \n",
    "    \n",
    "    param_grid= {\n",
    "                'classifier__eta': [0.3, 0.2, 0.1 , 0.01, 0.001],\n",
    "                'classifier__max_depth':range(2,20,2),\n",
    "                'classifier__gamma':[i/10.0 for i in range(0,5)],\n",
    "                'classifier__min_child_weight': range(2,20,2),\n",
    "                'classifier__subsample':[0.5, 0.6, 0.7,0.75,0.8,0.85,0.9,0.95,1],\n",
    "                'classifier__reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]\n",
    "               }\n",
    "\n",
    "    gs_1=RandomizedSearchCV(pipe_1, param_distributions=param_grid, n_iter=10, cv =10, verbose=True, n_jobs=-1, return_train_score=True, refit='f1_score', scoring='f1')\n",
    "    gs_2=RandomizedSearchCV(pipe_2, param_distributions=param_grid, n_iter=10, cv =10, verbose=True, n_jobs=-1, return_train_score=True, refit='f1_score', scoring='f1')\n",
    "    gs_3=RandomizedSearchCV(pipe_3, param_distributions=param_grid, n_iter=10, cv =10, verbose=True, n_jobs=-1, return_train_score=True, refit='f1_score', scoring='f1')\n",
    "    gs_4=RandomizedSearchCV(pipe_4, param_distributions=param_grid, n_iter=10, cv =10, verbose=True, n_jobs=-1, return_train_score=True, refit='f1_score', scoring='f1')\n",
    "    gs_5=RandomizedSearchCV(pipe_5, param_distributions=param_grid, n_iter=10, cv =10, verbose=True, n_jobs=-1, return_train_score=True, refit='f1_score', scoring='f1')\n",
    "    gs_6=RandomizedSearchCV(pipe_6, param_distributions=param_grid, n_iter=10, cv =10, verbose=True, n_jobs=-1, return_train_score=True, refit='f1_score', scoring='f1')\n",
    "    gs_7=RandomizedSearchCV(pipe_7, param_distributions=param_grid, n_iter=10, cv =10, verbose=True, n_jobs=-1, return_train_score=True, refit='f1_score', scoring='f1')\n",
    "    gs_8=RandomizedSearchCV(pipe_8, param_distributions=param_grid, n_iter=10, cv =10, verbose=True, n_jobs=-1, return_train_score=True, refit='f1_score', scoring='f1')\n",
    "    gs_9=RandomizedSearchCV(pipe_9, param_distributions=param_grid, n_iter=10, cv =10, verbose=True, n_jobs=-1, return_train_score=True, refit='f1_score', scoring='f1')\n",
    "    \n",
    "    grids=[gs_1,gs_2,gs_3,gs_4,gs_5,gs_6,gs_7,gs_8,gs_9]\n",
    "    \n",
    "    print('Performing model optimizations...')\n",
    "   \n",
    "    best_f1=0\n",
    "    \n",
    "    for gs in grids:\n",
    "        gs.fit(X_tr, y_tr)\n",
    "        print(\"Best parameters - : \", gs.best_params_)\n",
    "        print(\"Best training accuracy - :\", gs.best_score_)\n",
    "        y_pred = gs.predict(X_ts)\n",
    "        print(\"Test accuracy -  : \", gs.score(X_ts, y_ts))\n",
    "        print (\"Recall Score - : \", metrics.recall_score(y_ts, y_pred, average='weighted'))\n",
    "        print (\"Precison Score - : \", metrics.precision_score(y_ts, y_pred, average='weighted'))\n",
    "        print (\"F1 Score - : \", metrics.f1_score(y_ts, y_pred, average='weighted'))\n",
    "        \n",
    "        if metrics.f1_score(y_ts, y_pred, average='weighted')>best_f1:\n",
    "            best_f1=metrics.f1_score(y_ts, y_pred, average='weighted')\n",
    "            best_gs=gs\n",
    "     \n",
    "    print(\"===========================================================\")\n",
    "    print (\"Best Model values :\")\n",
    "    print(\"Best parameters - : \", best_gs.best_params_)\n",
    "    print(\"Best training accuracy - :\", best_gs.best_score_)\n",
    "    y_pred = best_gs.predict(X_ts)\n",
    "    print(\"Test accuracy -  : \", best_gs.score(X_ts, y_ts))\n",
    "    print (\"Recall Score - : \", metrics.recall_score(y_ts, y_pred, average='weighted'))\n",
    "    print (\"Precison Score - : \", metrics.precision_score(y_ts, y_pred, average='weighted'))\n",
    "    print (\"F1 Score - : \", metrics.f1_score(y_ts, y_pred, average='weighted'))\n",
    "    \n",
    "    y_pred_proba = best_gs.predict_proba(X_ts)\n",
    "    preds = y_pred_proba[:,1]\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_ts, preds)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    print(roc_auc)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label='AUC ROC (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    print(\"===========================================================\")\n",
    "    \n",
    "    return best_gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgb_model=process_xgb(adv_data_2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('xgb_param_model', 'wb') as file:  \n",
    "    pickle.dump(best_xgb_model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best Xtreme Gradient Boosting Model information**\n",
    "\n",
    "**Best Model :** best_xgb_model\n",
    "\n",
    "**Best parameters - :**  {'classifier__subsample': 0.8, 'classifier__reg_alpha': 1e-05, 'classifier__min_child_weight': 4, 'classifier__max_depth': 4, 'classifier__gamma': 0.3, 'classifier__eta': 0.3}\n",
    "**Best training accuracy - :** 0.6227975490043983\n",
    "\n",
    "**Test accuracy -  :**  0.6409364125276812\n",
    "\n",
    "**Recall Score - :**  0.7821497120921305\n",
    "\n",
    "**Precison Score - :**  0.8344563671761712\n",
    "\n",
    "**F1 Score - :**  0.7953197635875338\n",
    "\n",
    "**AUC-ROC Score - :** 0.871461222793621\n",
    "\n",
    "\n",
    "All the parameters value have been increased as compared to Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section15014></a>\n",
    "#### **15.1.4 Hyper parameter tuning for LightGBM Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to process Light GBM using hyper parameter tuning\n",
    "def process_lgbm(data,stratified):\n",
    "\n",
    "    \"\"\"\n",
    "    data - input data\n",
    "    stratified flag - wether to set the stratified flag to TRUE or FALSE (always FALSE in this case)\n",
    "    \"\"\"\n",
    "        \n",
    "    X=data.drop('netgain', axis=1)\n",
    "    y=data['netgain']\n",
    "    \n",
    "    if (True==stratified):\n",
    "        X_tr, X_ts, y_tr, y_ts = train_test_split(X, y, test_size = .20, random_state = 0, stratify=y, shuffle=True)\n",
    "    else:\n",
    "        X_tr, X_ts, y_tr, y_ts = train_test_split(X, y, test_size = .20, random_state = 0)\n",
    "        \n",
    "    preprocess_step=FeatureUnion([('kbest', SelectKBest(score_func=f_classif, k=15))])\n",
    "    \n",
    "    params = {'boosting_type': 'gbdt',\n",
    "          'max_depth' : -1,\n",
    "          'objective': 'binary',\n",
    "          'nthread': 3, # Updated from nthread\n",
    "          'num_leaves': 64,\n",
    "          'learning_rate': 0.05,\n",
    "          'max_bin': 512,\n",
    "          'subsample_for_bin': 200,\n",
    "          'subsample': 1,\n",
    "          'subsample_freq': 1,\n",
    "          'colsample_bytree': 0.8,\n",
    "          'reg_alpha': 5,\n",
    "          'reg_lambda': 10,\n",
    "          'min_split_gain': 0.5,\n",
    "          'min_child_weight': 1,\n",
    "          'min_child_samples': 5,\n",
    "          'scale_pos_weight': 1,\n",
    "          'num_class' : 1,\n",
    "          'metric' : 'binary_error'}\n",
    "\n",
    "    lgbmdl = LGBMClassifier(boosting_type= 'gbdt',\n",
    "          objective = 'binary',\n",
    "          n_jobs = -1, \n",
    "          silent = True,\n",
    "          max_depth = params['max_depth'],\n",
    "          max_bin = params['max_bin'],\n",
    "          subsample_for_bin = params['subsample_for_bin'],\n",
    "          subsample = params['subsample'],\n",
    "          subsample_freq = params['subsample_freq'],\n",
    "          min_split_gain = params['min_split_gain'],\n",
    "          min_child_weight = params['min_child_weight'],\n",
    "          min_child_samples = params['min_child_samples'],\n",
    "          scale_pos_weight = params['scale_pos_weight'],\n",
    "        random_state=0)\n",
    "\n",
    "    pipe_1=Pipeline([('scaling', StandardScaler()),\n",
    "                    ('imbalance', RandomOverSampler(random_state=0)),\n",
    "                    ('preprocess', preprocess_step),\n",
    "                   ('classifier', lgbmdl)])\n",
    "    pipe_2=Pipeline([('scaling', StandardScaler()),\n",
    "                    ('imbalance', SMOTE(random_state=0)),\n",
    "                    ('preprocess', preprocess_step),\n",
    "                   ('classifier', lgbmdl)])\n",
    "    pipe_3=Pipeline([('scaling', StandardScaler()),\n",
    "                    ('imbalance', ADASYN(random_state=0)),\n",
    "                    ('preprocess', preprocess_step),\n",
    "                   ('classifier', lgbmdl)])\n",
    "    pipe_4=Pipeline([('scaling', RobustScaler()),\n",
    "                    ('imbalance', RandomOverSampler(random_state=0)),\n",
    "                    ('preprocess', preprocess_step),\n",
    "                   ('classifier', lgbmdl)])\n",
    "    pipe_5=Pipeline([('scaling', RobustScaler()),\n",
    "                    ('imbalance', SMOTE(random_state=0)),\n",
    "                    ('preprocess', preprocess_step),\n",
    "                   ('classifier', lgbmdl)])\n",
    "    pipe_6=Pipeline([('scaling', RobustScaler()),\n",
    "                    ('imbalance', ADASYN(random_state=0)),\n",
    "                    ('preprocess', preprocess_step),\n",
    "                   ('classifier', lgbmdl)])\n",
    "    pipe_7=Pipeline([('scaling', MinMaxScaler()),\n",
    "                    ('imbalance', RandomOverSampler(random_state=0)),\n",
    "                    ('preprocess', preprocess_step),\n",
    "                   ('classifier', lgbmdl)])\n",
    "    pipe_8=Pipeline([('scaling', MinMaxScaler()),\n",
    "                    ('imbalance', SMOTE(random_state=0)),\n",
    "                    ('preprocess', preprocess_step),\n",
    "                   ('classifier', lgbmdl)])\n",
    "    pipe_9=Pipeline([('scaling', MinMaxScaler()),\n",
    "                    ('imbalance', ADASYN(random_state=0)),\n",
    "                    ('preprocess', preprocess_step),\n",
    "                   ('classifier', lgbmdl)])\n",
    "\n",
    "    param_grid = {\n",
    "        'classifier__learning_rate': [0.1, 0.01, 0.005, 0.001],\n",
    "        'classifier__n_estimators': [10, 100,250,500],\n",
    "        'classifier__num_leaves': range(0,20, 1),\n",
    "        'classifier__boosting_type' : ['gbdt'],\n",
    "        'classifier__objective' : ['binary'],\n",
    "        'classifier__colsample_bytree' : [0.65, 0.66],\n",
    "        'classifier__subsample' : [0.5, 0.6, 0.7,0.75,0.8,0.85,0.9,0.95,1],\n",
    "        'classifier__eg_alpha' : [1,1.2],\n",
    "        'classifier__reg_lambda' : [1,1.2,1.4]\n",
    "    }\n",
    "\n",
    "\n",
    "    gs_1=RandomizedSearchCV(pipe_1, param_distributions=param_grid, n_iter=10, cv =10, verbose=True, n_jobs=-1, return_train_score=True, refit='f1_score', scoring='f1')\n",
    "    gs_2=RandomizedSearchCV(pipe_2, param_distributions=param_grid, n_iter=10, cv =10, verbose=True, n_jobs=-1, return_train_score=True, refit='f1_score', scoring='f1')\n",
    "    gs_3=RandomizedSearchCV(pipe_3, param_distributions=param_grid, n_iter=10, cv =10, verbose=True, n_jobs=-1, return_train_score=True, refit='f1_score', scoring='f1')\n",
    "    gs_4=RandomizedSearchCV(pipe_4, param_distributions=param_grid, n_iter=10, cv =10, verbose=True, n_jobs=-1, return_train_score=True, refit='f1_score', scoring='f1')\n",
    "    gs_5=RandomizedSearchCV(pipe_5, param_distributions=param_grid, n_iter=10, cv =10, verbose=True, n_jobs=-1, return_train_score=True, refit='f1_score', scoring='f1')\n",
    "    gs_6=RandomizedSearchCV(pipe_6, param_distributions=param_grid, n_iter=10, cv =10, verbose=True, n_jobs=-1, return_train_score=True, refit='f1_score', scoring='f1')\n",
    "    gs_7=RandomizedSearchCV(pipe_7, param_distributions=param_grid, n_iter=10, cv =10, verbose=True, n_jobs=-1, return_train_score=True, refit='f1_score', scoring='f1')\n",
    "    gs_8=RandomizedSearchCV(pipe_8, param_distributions=param_grid, n_iter=10, cv =10, verbose=True, n_jobs=-1, return_train_score=True, refit='f1_score', scoring='f1')\n",
    "    gs_9=RandomizedSearchCV(pipe_9, param_distributions=param_grid, n_iter=10, cv =10, verbose=True, n_jobs=-1, return_train_score=True, refit='f1_score', scoring='f1')\n",
    "    \n",
    "    grids=[gs_1,gs_2,gs_3,gs_4,gs_5,gs_6,gs_7,gs_8,gs_9]\n",
    "    \n",
    "    print('Performing model optimizations...')\n",
    "   \n",
    "    best_f1=0\n",
    "    \n",
    "    for gs in grids:\n",
    "        gs.fit(X_tr, y_tr)\n",
    "        print(\"Best parameters - : \", gs.best_params_)\n",
    "        print(\"Best training accuracy - :\", gs.best_score_)\n",
    "        y_pred = gs.predict(X_ts)\n",
    "        print(\"Test accuracy -  : \", gs.score(X_ts, y_ts))\n",
    "        print (\"Recall Score - : \", metrics.recall_score(y_ts, y_pred, average='weighted'))\n",
    "        print (\"Precison Score - : \", metrics.precision_score(y_ts, y_pred, average='weighted'))\n",
    "        print (\"F1 Score - : \", metrics.f1_score(y_ts, y_pred, average='weighted'))\n",
    "        \n",
    "        if metrics.f1_score(y_ts, y_pred, average='weighted')>best_f1:\n",
    "            best_f1=metrics.f1_score(y_ts, y_pred, average='weighted')\n",
    "            best_gs=gs\n",
    "     \n",
    "    print(\"===========================================================\")\n",
    "    print (\"Best Model values :\")\n",
    "    print(\"Best parameters - : \", best_gs.best_params_)\n",
    "    print(\"Best training accuracy - :\", best_gs.best_score_)\n",
    "    y_pred = best_gs.predict(X_ts)\n",
    "    print(\"Test accuracy -  : \", best_gs.score(X_ts, y_ts))\n",
    "    print (\"Recall Score - : \", metrics.recall_score(y_ts, y_pred, average='weighted'))\n",
    "    print (\"Precison Score - : \", metrics.precision_score(y_ts, y_pred, average='weighted'))\n",
    "    print (\"F1 Score - : \", metrics.f1_score(y_ts, y_pred, average='weighted'))\n",
    "    \n",
    "    y_pred_proba = best_gs.predict_proba(X_ts)\n",
    "    preds = y_pred_proba[:,1]\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_ts, preds)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    print(roc_auc)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label='AUC ROC (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    print(\"===========================================================\")\n",
    "    \n",
    "    return best_gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lgbm_model=process_lgbm(adv_data_2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lgbm_param_model', 'wb') as file:  \n",
    "    pickle.dump(best_lgbm_model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best Light Gradient Boosting Model information**\n",
    "\n",
    "**Best Model :** best_lgbm_model\n",
    "\n",
    "**Best parameters - :**  {'classifier__subsample': 0.8, 'classifier__reg_lambda': 1, 'classifier__objective': 'binary', 'classifier__num_leaves': 11, 'classifier__n_estimators': 100, 'classifier__learning_rate': 0.1, 'classifier__eg_alpha': 1.2, 'classifier__colsample_bytree': 0.65, 'classifier__boosting_type': 'gbdt'}\n",
    "\n",
    "**Best training accuracy - :** 0.6205174445915582\n",
    "\n",
    "**Test accuracy -  :**  0.6340731853629273\n",
    "\n",
    "**Recall Score - :**  0.7658349328214972\n",
    "\n",
    "**Precison Score - :**  0.8368709448909321\n",
    "\n",
    "**F1 Score - :**  0.7816403922259263\n",
    "\n",
    "**AUC-ROC Score - :** 0.8649997078073866\n",
    "\n",
    "\n",
    "All the parameters value have been decreasedvas compared to Xtreme Gradient Boosting and Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary on hyper parameter tuning of various models**\n",
    "\n",
    "Xtreme Gradient Boosting performed better as compared to other models (compared on the basis of F1 score) with following values\n",
    "\n",
    "**Parameters - :**  {'classifier__subsample': 0.8, 'classifier__reg_alpha': 1e-05, 'classifier__min_child_weight': 4, 'classifier__max_depth': 4, 'classifier__gamma': 0.3, 'classifier__eta': 0.3}\n",
    "**Best training accuracy - :** 0.6227975490043983\n",
    "\n",
    "**Test accuracy -  :**  0.6409364125276812\n",
    "\n",
    "**Recall Score - :**  0.7821497120921305\n",
    "\n",
    "**Precison Score - :**  0.8344563671761712\n",
    "\n",
    "**F1 Score - :**  0.7953197635875338\n",
    "\n",
    "**AUC-ROC Score - :** 0.871461222793621\n",
    "\n",
    "\n",
    "Further ensembling techniques can be applied on all hyper parameter tuned models and best selected baseline model to check if performance enahnces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section1502></a>\n",
    "### **15.2 Ensemble Techniques**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's check all the models to be used in ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xtreme Gradient Boosting Model\n",
    "with open('xgb_param_model', 'rb') as file:  \n",
    "    best_xgb_model = pickle.load(file)\n",
    "    \n",
    "best_xgb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Boosting model\n",
    "with open('gb_param_model', 'rb') as file:  \n",
    "    best_gb_model = pickle.load(file)\n",
    "\n",
    "best_gb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Light Gradient Boosting\n",
    "with open('lgbm_param_model', 'rb') as file:  \n",
    "    best_lgbm_model = pickle.load(file)\n",
    "    \n",
    "best_lgbm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Model\n",
    "with open('rf_param_model', 'rb') as file:  \n",
    "    best_rf_model = pickle.load(file)\n",
    "\n",
    "best_rf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best performing model from baseline models\n",
    "with open('model_adv_data_2_no_stratify', 'rb') as file:  \n",
    "    baseline_model = pickle.load(file)\n",
    "    \n",
    "baseline_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section15021></a>\n",
    "#### **15.2.1 Voting Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=adv_data_2.drop('netgain', axis=1)\n",
    "y=adv_data_2['netgain']\n",
    "\n",
    "X_tr, X_ts, y_tr, y_ts = train_test_split(X, y, test_size = .20, random_state = 0)\n",
    "\n",
    "votingclf=VotingClassifier(estimators=[\n",
    "            ('model1', baseline_model),\n",
    "            ('model2', best_rf_model),\n",
    "            ('model3', best_gb_model),\n",
    "            ('model4', best_xgb_model),\n",
    "            ('model5', best_lgbm_model)], voting='hard', n_jobs=-1)\n",
    "\n",
    "votingclf=votingclf.fit(X_tr,y_tr)\n",
    "\n",
    "print(\"===========================================================\")\n",
    "print (\"Voting Classifier Model values :\")\n",
    "print(\"Training accuracy - :\", votingclf.score(X_tr, y_tr))\n",
    "y_pred = votingclf.predict(X_ts)\n",
    "print(\"Test accuracy -  : \", votingclf.score(X_ts, y_ts))\n",
    "print (\"Recall Score - : \", metrics.recall_score(y_ts, y_pred, average='weighted'))\n",
    "print (\"Precison Score - : \", metrics.precision_score(y_ts, y_pred, average='weighted'))\n",
    "print (\"F1 Score - : \", metrics.f1_score(y_ts, y_pred, average='weighted'))\n",
    "print(\"===========================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('voting_model', 'wb') as file:  \n",
    "    pickle.dump(votingclf, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Voting Classifier Along with the F1 score,Recall and Precsion, Test and Train accuracy has been improved.\n",
    "\n",
    "So, the Voting Classifier can be considered final model from ML perspective, if Stacking Classifier gives lower values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section15022></a>\n",
    "#### **15.2.2 Stacking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=adv_data_2.drop('netgain', axis=1)\n",
    "y=adv_data_2['netgain']\n",
    "\n",
    "X_tr, X_ts, y_tr, y_ts = train_test_split(X, y, test_size = .20, random_state = 0)\n",
    "\n",
    "stackclf=StackingClassifier(estimators=[\n",
    "            ('model1', baseline_model),\n",
    "            ('model2', best_rf_model),\n",
    "            ('model3', best_gb_model),\n",
    "            ('model4', best_xgb_model),\n",
    "            ('model5', best_lgbm_model)], final_estimator=LogisticRegressionCV(cv=5, random_state=0), n_jobs=-1)\n",
    "\n",
    "stackclf=stackclf.fit(X_tr,y_tr)\n",
    "\n",
    "print(\"===========================================================\")\n",
    "print (\"Stacking Classifier Model values :\")\n",
    "print(\"Training accuracy - :\", stackclf.score(X_tr, y_tr))\n",
    "y_pred = stackclf.predict(X_ts)\n",
    "print(\"Test accuracy -  : \", stackclf.score(X_ts, y_ts))\n",
    "print (\"Recall Score - : \", metrics.recall_score(y_ts, y_pred, average='weighted'))\n",
    "print (\"Precison Score - : \", metrics.precision_score(y_ts, y_pred, average='weighted'))\n",
    "print (\"F1 Score - : \", metrics.f1_score(y_ts, y_pred, average='weighted'))\n",
    "   \n",
    "print(\"===========================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stacking_modelfile', 'wb') as file:  \n",
    "    pickle.dump(stackclf, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking further increased the F1 score and accuracy values, so this model, we can consider as final one from ML perspective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary on final model from ML**\n",
    "\n",
    "**Stacking Classifier build on Random Forest, Gradient Boosting Classifier, Xtreme Gradient Boosting classifier and LightGBM**\n",
    "\n",
    "**Training accuracy** - : 0.8229196659948171\n",
    "\n",
    "**Test accuracy** -  :  0.8166986564299424\n",
    "\n",
    "**Recall Score** - :  0.8166986564299424\n",
    "\n",
    "**Precison Score** - :  0.8042796289150841\n",
    "\n",
    "**F1 Score** - :  0.8059686365556438"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section16></a>\n",
    "## **16. Deep Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the final model parameter values from Machine Learning, so main objective is to create DNN model and analyze if it performs better than ML models on the given dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section1601></a>\n",
    "### **16.1  - Check the data distribution and if the data is linearly seperable based on output class**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plotting pairplot to study the data using original dataset(before any feature engoneering or encoding on it)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_data_orig.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "sns.pairplot(data=adv_data_orig, hue='netgain')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above graph, we analyze that data is not linearly seperable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further can plot the encoded data we got earlier after feature engineering and other operations - adv_data and using only important features we analyzed earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp=adv_data[['ratings','runtime','realtionship_status_Married-civ-spouse','industry_Pharma','realtionship_status_Never-married','airtime_Morning','airtime_Primetime','expensive','targeted_sex_Male','industry_Political','industry_Entertainment','industry_Other','netgain']]\n",
    "tmp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "sns.pairplot(data=tmp, hue='netgain')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can analyze that the data is not linearly seperable based on output class, so need activation fucntions in the layers to do the processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section1602></a>\n",
    "### **16.2  - Function to Normalize the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will create and analyze the neural network on adv_data, adv_data_1 and adv_data_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will try to normalize from all the 4 common techniques to analyze which works best with neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting basic X_train,y_train,X_test and y_test before normaliztion for further use if required\n",
    "\n",
    "train_dataset = adv_data.sample(frac=0.8,random_state=0)   \n",
    "test_dataset = adv_data.drop(train_dataset.index)\n",
    "    \n",
    "X_train=train_dataset.drop('netgain', axis=1)\n",
    "y_train=train_dataset['netgain']\n",
    "\n",
    "X_test=test_dataset.drop('netgain', axis=1)\n",
    "y_test=test_dataset['netgain']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normalized_dataset(data, scaling):\n",
    "        \n",
    "    train_dataset = adv_data.sample(frac=0.8,random_state=0)   \n",
    "    test_dataset = adv_data.drop(train_dataset.index)\n",
    "\n",
    "    print(train_dataset.shape)\n",
    "    print(test_dataset.shape)\n",
    "    \n",
    "    X_train=train_dataset.drop('netgain', axis=1)\n",
    "    y_train=train_dataset['netgain']\n",
    "\n",
    "    X_test=test_dataset.drop('netgain', axis=1)\n",
    "    y_test=test_dataset['netgain']\n",
    "    \n",
    "    print(X_train.shape)\n",
    "    print(y_train.shape)\n",
    "    print(X_test.shape)\n",
    "    print(y_test.shape)\n",
    "\n",
    "\n",
    "    if (1==scaling):\n",
    "        norm_clf=StandardScaler()\n",
    "    elif (2==scaling):\n",
    "        norm_clf=MinMaxScaler()\n",
    "    elif (3==scaling):\n",
    "        norm_clf=Normalizer()\n",
    "    elif (4==scaling):\n",
    "        norm_clf=RobustScaler()\n",
    "        \n",
    "    X_train_norm=norm_clf.fit_transform(X_train)\n",
    "    X_test_norm=norm_clf.transform(X_test)\n",
    "    \n",
    "    print (\"\\nNormalized Train data :\\n\")\n",
    "    print (X_train_norm[0])\n",
    "    print (\"\\nNormalized Test data :\\n\")\n",
    "    print (X_test_norm[0])\n",
    "    \n",
    "    return X_train_norm, X_test_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Scaling adv_data using various methods like StandardScaler, MinMaxScaler, RobustScaler and Normalizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling using StandardScaler\n",
    "X_train_norm11, X_test_norm11=get_normalized_dataset(adv_data,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling using MinMaxScaler\n",
    "X_train_norm12, X_test_norm12=get_normalized_dataset(adv_data,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizer\n",
    "X_train_norm13, X_test_norm13=get_normalized_dataset(adv_data,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Robust Scaler\n",
    "X_train_norm14, X_test_norm14=get_normalized_dataset(adv_data,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Scaling adv_data_1 using various methods like StandardScaler, MinMaxScaler, RobustScaler and Normalizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling using StandardScaler\n",
    "X_train_norm21, X_test_norm21=get_normalized_dataset(adv_data_1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling using MinMaxScaler\n",
    "X_train_norm22, X_test_norm22=get_normalized_dataset(adv_data_1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizer\n",
    "X_train_norm23, X_test_norm23=get_normalized_dataset(adv_data_1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Robust Scaler\n",
    "X_train_norm24, X_test_norm24=get_normalized_dataset(adv_data_1,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Scaling adv_data_2 using various methods like StandardScaler, MinMaxScaler, RobustScaler and Normalizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling using StandardScaler\n",
    "X_train_norm31, X_test_norm31=get_normalized_dataset(adv_data_2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling using MinMaxScaler\n",
    "X_train_norm32, X_test_norm32=get_normalized_dataset(adv_data_2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizer\n",
    "X_train_norm33, X_test_norm33=get_normalized_dataset(adv_data_2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Robust Scaler\n",
    "X_train_norm34, X_test_norm34=get_normalized_dataset(adv_data_2,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have below normalized/scaled data set for further experiment\n",
    "\n",
    " - adv_data\n",
    "* X_train_norm11, X_test_norm11\n",
    "* X_train_norm12, X_test_norm12\n",
    "* X_train_norm13, X_test_norm13\n",
    "* X_train_norm14, X_test_norm14\n",
    "\n",
    "\n",
    " - adv_data_1\n",
    "* X_train_norm21, X_test_norm21\n",
    "* X_train_norm22, X_test_norm22\n",
    "* X_train_norm23, X_test_norm23\n",
    "* X_train_norm24, X_test_norm24\n",
    "\n",
    "\n",
    " - adv_data_2\n",
    "* X_train_norm31, X_test_norm31\n",
    "* X_train_norm32, X_test_norm32\n",
    "* X_train_norm33, X_test_norm33\n",
    "* X_train_norm34, X_test_norm34"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section1603></a>\n",
    "### **16.3 Create simple Neural Network first and evaluation with all datasets generated above**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Building Simple Neural Network with only 1 hidden layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_input_nodes=len(X_train.columns)\n",
    "num_input_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=len(np.unique(y_train))\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_nn(num_hidden_neurons):\n",
    "    # initialize model\n",
    "    model = keras.Sequential()\n",
    "    # add an input layer and a hidden layer\n",
    "    model.add(Dense(num_hidden_neurons, input_dim = num_input_nodes))\n",
    "    # add activation layer to add non-linearity\n",
    "    model.add(Activation('relu'))\n",
    "    # add output layer\n",
    "    model.add(Dense(1))\n",
    "    # add softmax layer \n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_hidden_neurons):\n",
    "    model=simple_nn(num_hidden_neurons)\n",
    "    \n",
    "    # Defining the optimizer with a specific learning rate of 0.001\n",
    "    optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
    "    \n",
    "    # Compiling the model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=build_model(63)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_batch = X_train_norm11[:10]\n",
    "example_result = model.predict(example_batch)\n",
    "example_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, num_epochs):\n",
    "\n",
    "    model_info = model.fit(X_train, y_train, epochs=num_epochs, validation_split=0.2,\n",
    "                        verbose=0, callbacks=[tfdocs.modeling.EpochDots()])\n",
    "    model_hist=pd.DataFrame(model_info.history)\n",
    "    model_hist['epochs']=model_info.epoch\n",
    "    model_hist=model_hist.sort_values(by='val_accuracy', ascending=False)\n",
    "    model_hist.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return model_info, model_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for intial experiment with simple neural network, we can try with different epochs - 1000, 500, 2000 for all the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section16031></a>\n",
    "#### **16.3.1 Training the model with 1000 EPOCHS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info,df=train_model(model,X_train_norm11, y_train, 1000)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info,df=train_model(model, X_train_norm12, y_train, 1000)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info,df=train_model(model, X_train_norm13, y_train, 1000)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info,df=train_model(model, X_train_norm14, y_train, 1000)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info,df=train_model(model, X_train_norm21, y_train, 1000)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info,df=train_model(model, X_train_norm22, y_train, 1000)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info,df=train_model(model, X_train_norm23, y_train, 1000)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info,df=train_model(model, X_train_norm24, y_train, 1000)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info,df=train_model(model, X_train_norm31, y_train, 1000)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info,df=train_model(model, X_train_norm32, y_train, 1000)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info,df=train_model(model, X_train_norm33, y_train, 1000)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info,df=train_model(model, X_train_norm34, y_train, 1000)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the epochs of 1000, training data and validation data accuracy seems above .80 in most of the cases, now can try with lower epoch values, if can achieve similar results with lower iterations as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section16032></a>\n",
    "#### **16.3.2 Training the model with 500 EPOCHS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info,df=train_model(model, X_train_norm11, y_train, 500)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info,df=train_model(model, X_train_norm12, y_train, 500)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info,df=train_model(model, X_train_norm13, y_train, 500)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info,df=train_model(model, X_train_norm14, y_train, 500)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info,df=train_model(model, X_train_norm21, y_train, 500)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info,df=train_model(model, X_train_norm22, y_train, 500)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info,df=train_model(model, X_train_norm23, y_train, 500)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info,df=train_model(model, X_train_norm24, y_train, 500)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info,df=train_model(model, X_train_norm31, y_train, 500)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info,df=train_model(model, X_train_norm32, y_train, 500)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info,df=train_model(model, X_train_norm33, y_train, 500)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info,df=train_model(model, X_train_norm34, y_train, 500)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though the experiment with 500 epoch cycles have similar results, but the experiment with 1000 epochs provided marginally higher results, so can try experimenting with 2000 epoch values, if can increasing the epochs increases the acccuracies as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section16033></a>\n",
    "#### **16.3.3 Training the model with 2000 EPOCHS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info,df=train_model(model, X_train_norm11, y_train, 2000)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info,df=train_model(model, X_train_norm12, y_train, 2000)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info,df=train_model(model, X_train_norm13, y_train, 2000)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info,df=train_model(model, X_train_norm14, y_train, 2000)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info,df=train_model(model, X_train_norm21, y_train, 2000)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info,df=train_model(model, X_train_norm22, y_train, 2000)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info,df=train_model(model, X_train_norm23, y_train, 2000)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info,df=train_model(model, X_train_norm24, y_train, 2000)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info,df=train_model(model, X_train_norm31, y_train, 2000)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info,df=train_model(model, X_train_norm32, y_train, 2000)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info,df=train_model(model, X_train_norm33, y_train, 2000)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info,df=train_model(model, X_train_norm34, y_train, 2000)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observed that increasing epochs to 2000 have not increased the accuracy values and experiment with 1000 epochs provided the overall better result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section16034></a>\n",
    "#### **16.3.4 Summary of highest accuracies obtained for all 3 epoch values**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Dataset</th>\n",
    "        <th>Accuracy(500)</th>\n",
    "        <th>Accuracy(1000)</th>\n",
    "        <th>Accuracy(2000)</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>adv_data+StandardScaler</td>\n",
    "        <td>.8131</td>\n",
    "        <td>.8186</td>\n",
    "        <td>.8138</td>\n",
    "    </tr>   \n",
    "    <tr>\n",
    "        <td>adv_data+MinMaxScaler</td>\n",
    "        <td>.8080</td>\n",
    "        <td>.8090</td>\n",
    "        <td>.8114</td>\n",
    "    </tr> \n",
    "    <tr>\n",
    "        <td>adv_data+Normalizer</td>\n",
    "        <td>.7909</td>\n",
    "        <td>.8030</td>\n",
    "        <td>.7675</td>\n",
    "    </tr> \n",
    "    <tr>\n",
    "        <td>adv_data+RobustScaler</td>\n",
    "        <td>.8147</td>\n",
    "        <td>.8131</td>\n",
    "        <td>.8159</td>\n",
    "    </tr> \n",
    "</table>\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Dataset</th>\n",
    "        <th>Accuracy(500)</th>\n",
    "        <th>Accuracy(1000)</th>\n",
    "        <th>Accuracy(2000)</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>adv_data_1+StandardScaler</td>\n",
    "        <td>.8133</td>\n",
    "        <td>.8169</td>\n",
    "        <td>.8138</td>\n",
    "    </tr>   \n",
    "    <tr>\n",
    "        <td>adv_data_1+MinMaxScaler</td>\n",
    "        <td>.8083</td>\n",
    "        <td>.8157</td>\n",
    "        <td>.8083</td>\n",
    "    </tr> \n",
    "    <tr>\n",
    "        <td>adv_data_1+Normalizer</td>\n",
    "        <td>.7927</td>\n",
    "        <td>.8068</td>\n",
    "        <td>.800</td>\n",
    "    </tr> \n",
    "    <tr>\n",
    "        <td>adv_data_1+RobustScaler</td>\n",
    "        <td>.8102</td>\n",
    "        <td>.8073</td>\n",
    "        <td>.8063</td>\n",
    "    </tr> \n",
    "</table>\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Dataset</th>\n",
    "        <th>Accuracy(500)</th>\n",
    "        <th>Accuracy(1000)</th>\n",
    "        <th>Accuracy(2000)</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>adv_data_2+StandardScaler</td>\n",
    "        <td>.8126</td>\n",
    "        <td>.8159</td>\n",
    "        <td>.809</td>\n",
    "    </tr>   \n",
    "    <tr>\n",
    "        <td>adv_data_2+MinMaxScaler</td>\n",
    "        <td>.8080</td>\n",
    "        <td>.8114</td>\n",
    "        <td>.806</td>\n",
    "    </tr> \n",
    "    <tr>\n",
    "        <td>adv_data_2+Normalizer</td>\n",
    "        <td>.7927</td>\n",
    "        <td>.7893</td>\n",
    "        <td>.7972</td>\n",
    "    </tr> \n",
    "    <tr>\n",
    "        <td>adv_data_2+RobustScaler</td>\n",
    "        <td>.814</td>\n",
    "        <td>.8152</td>\n",
    "        <td>.803</td>\n",
    "    </tr> \n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "So, from above tables, can deduce the following\n",
    "\n",
    "- StandardScaler and RobustScaler have given the better values as compared to other techniques\n",
    "- Epoch value of 2000 have not increased the accuracies significantly, so if compare the tradeoff between the training time and accuracy values, we can skip epoch values of 2000 from further analysis\n",
    "- The difference in accuracies between epochs of 500 and 1000 are not very huge, so can experiment with deep neural networks and other optimization techniques with 500 epochs\n",
    "\n",
    "Can also check the test data validation results for following model using simple neural networks for better understanding\n",
    " - adv_data+RobustScaler with epoch  - 1000 and 500\n",
    " - adv_data+StandardScaler with epoch  - 1000 and 500\n",
    " - adv_data_1+RobustScaler with epoch  - 1000 and 500\n",
    " - adv_data_1+StandardScaler with epoch  - 1000 and 500\n",
    " - adv_data_2+RobustScaler with epoch  - 1000 and 500\n",
    " - adv_data_2+StandardScaler with epoch  - 1000 and 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section16035></a>\n",
    "#### **16.3.5 Analyzing validation loss for various data generated above**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracies(X_train, y_train, X_test, y_test, num_epochs):\n",
    "    model=build_model(63)\n",
    "    info, df=train_model(model, X_train, y_train, num_epochs)\n",
    "    _, train_acc = model.evaluate(X_train, y_train, verbose=1)\n",
    "    _, test_acc = model.evaluate(X_test, y_test, verbose=1)\n",
    "    print (\"\\nTrain accuracy - \", train_acc)\n",
    "    print (\"\\nTest accuracy - \", test_acc)\n",
    "    plt.plot(info.history['loss'], label='train')\n",
    "    plt.plot(info.history['val_loss'], label='test')\n",
    "    plt.legend()\n",
    "    plt.title(\"Accuracies\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adv_data + StandardScaler for 500 epochs\n",
    "plot_accuracies(X_train_norm11,y_train, X_test_norm11, y_test, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation loss for test data seems to be much higher than that of training data and also the validation loss seems to have increased with number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adv_data + RobustScaler for 500 epochs\n",
    "plot_accuracies(X_train_norm14,y_train, X_test_norm14, y_test, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation loss for test data seems to be much higher than that of training data and the difference is huge, could be overfitting.\n",
    "\n",
    "Also the loss in training is not changing much but for test data, it increased with the number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adv_data + StandardScaler for 1000 epochs\n",
    "plot_accuracies(X_train_norm11,y_train, X_test_norm11, y_test, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation loss for test data seems to be much higher than that of training data and also the validation loss seems to have increased with number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adv_data + RobustScaler for 1000 epochs\n",
    "plot_accuracies(X_train_norm14,y_train, X_test_norm14, y_test, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation loss for test data seems to be much higher than that of training data and the difference is huge, could be overfitting.\n",
    "\n",
    "Also the loss in training is not changing much but for test data, it increased with the number of epochs             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adv_data_1 + StandardScaler for 500 epochs\n",
    "plot_accuracies(X_train_norm21,y_train, X_test_norm21, y_test, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation loss for test data seems to be much higher than that of training data and also the validation loss seems to have increased with number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adv_data_1 + RobustScaler for 500 epochs\n",
    "plot_accuracies(X_train_norm24,y_train, X_test_norm24, y_test, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Validation loss for test data seems to be much higher than that of training data and the difference is huge, could be overfitting.\n",
    "\n",
    "Also the loss in training is not changing much but for test data, it increased with the number of epochs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adv_data_1 + StandardScaler for 1000 epochs\n",
    "plot_accuracies(X_train_norm21,y_train, X_test_norm21, y_test, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation loss for test data seems to be much higher than that of training data and also the validation loss seems to have increased with number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adv_data_1 + RobustScaler for 1000 epochs\n",
    "plot_accuracies(X_train_norm24,y_train, X_test_norm24, y_test, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation loss for test data seems to be much higher than that of training data and the difference is huge, could be overfitting.\n",
    "\n",
    "Also the loss in training is not changing much but for test data, it increased with the number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adv_data_2 + StandardScaler for 500 epochs\n",
    "plot_accuracies(X_train_norm31,y_train, X_test_norm31, y_test, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation loss for test data seems to be much higher than that of training data and also the validation loss seems to have increased with number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adv_data_2 + RobustScaler for 500 epochs\n",
    "plot_accuracies(X_train_norm34,y_train, X_test_norm34, y_test, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation loss for test data seems to be much higher than that of training data and the difference is huge, could be overfitting.\n",
    "\n",
    "Also the loss in training is not changing much but for test data, it increased with the number of epochs   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adv_data_2 + StandardScaler for 1000 epochs\n",
    "plot_accuracies(X_train_norm31,y_train, X_test_norm31, y_test, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation loss for test data seems to be much higher than that of training data and also the validation loss seems to have increased with number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adv_data_2 + RobustScaler for 1000 epochs\n",
    "plot_accuracies(X_train_norm34,y_train, X_test_norm34, y_test, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation loss for test data seems to be much higher than that of training data and the difference is huge, could be overfitting.\n",
    "\n",
    "Also the loss in training is not changing much but for test data, it increased with the number of epochs             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "- Not much difference in train and test accuracy values in 500 epochs or 1000 epochs, so can continue with epochs=500.\n",
    "- In some cases, the accuracy has stablized and not increased significantly, so can use EarlyStop to halt the processing at appropriate time.\n",
    "- Also, the grah is not smooth, could be due to default batch size of 32, so in further experiment can try with different batch sizes.\n",
    "- Also can further experiment with only adv_data, as not much difference/improvement among the different dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section1604></a>\n",
    "### **16.4 Creating deep neural networks with hyper parameter optimization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to train the model with different parameters like layers, activation functions and dropout probability values to determine which combination gives the better result.\n",
    "\n",
    "For basic analysis, we will run the test on adv_data + StandardScaler data to analyze the behvaiour\n",
    "\n",
    "\n",
    "We are using KerasClassifier and RandomizedSearchCV to check the multiple parameter values and get the best model information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_deep_nn(layers, activations, dropouts):\n",
    "    model=Sequential()\n",
    "    \n",
    "    for i,nodes in enumerate(layers):\n",
    "        if i==0:\n",
    "            model.add(Dense(nodes, input_dim=len(X_train.columns)))\n",
    "        else:\n",
    "            model.add(Dense(nodes))\n",
    "        model.add(Activation(activations))\n",
    "        model.add(Dropout(dropouts))\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=KerasClassifier(build_fn=create_deep_nn, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers=[(63,),(128,),(256,),(128,64),(256,128,64)]\n",
    "activations=['relu','selu','elu','tanh']\n",
    "dropouts=[0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "param_grid=dict(layers=layers, activations=activations, dropouts=dropouts, \n",
    "                batch_size=[32,64,80,128,256], epochs=[50,100,200,300,500])\n",
    "#grid=GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid=RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_jobs=-1, cv=3, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_result=grid.fit(X_train_norm11, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_result.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_result.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can try with different values in param_grid for experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=KerasClassifier(build_fn=create_deep_nn, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers=[(32,),(45,),(64,), (80,),(128,), (40,20),(60,40,20)]\n",
    "activations=['relu','selu','elu','tanh']\n",
    "dropouts=[0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "param_grid=dict(layers=layers, activations=activations, dropouts=dropouts, \n",
    "                batch_size=[64, 80, 100, 128, 180, 256, 300], epochs=[50,100,200,300,500, 1000])\n",
    "#grid=GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid=RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_jobs=-1, cv=10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_result=grid.fit(X_train_norm11, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_result.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_result.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Got better score by changing the parameter grid, so can try with some more values with similar epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=KerasClassifier(build_fn=create_deep_nn, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers=[(32,), (40,), (45,),(64,), (80,), (128,), (40,20), (60,40,20), (20, 40, 40, 20)]\n",
    "activations=['relu','selu','elu','tanh']\n",
    "dropouts=[0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "param_grid=dict(layers=layers, activations=activations, dropouts=dropouts, \n",
    "                batch_size=[64, 80, 100, 128, 180, 256, 300], epochs=[100,200,300,400, 500, 600, 800, 1000])\n",
    "#grid=GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid=RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_jobs=-1, cv=10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_result=grid.fit(X_train_norm11, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_result.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_result.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Grid search on number of layers, number of neurons, activation function and dropout probabilities, we get the few configurations like below, so can try to create neural networks and analyze their score on train and test data and later can optimize them using different optimizers and kernel regularization techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'layers': (63,),\n",
    " 'epochs': 300,\n",
    " 'dropouts': 0.4,\n",
    " 'batch_size': 32,\n",
    " 'activations': 'relu'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'layers': (60, 40, 20),\n",
    " 'epochs': 1000,\n",
    " 'dropouts': 0.5,\n",
    " 'batch_size': 64,\n",
    " 'activations': 'relu'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'layers': (80,),\n",
    " 'epochs': 1000,\n",
    " 'dropouts': 0.6,\n",
    " 'batch_size': 256,\n",
    " 'activations': 'tanh'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above information, we can create few architectures and observe their performance (without batch normalization, changing optimizers or regularization technique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section1605></a>\n",
    "### **16.5 Experiment with selected configuration from hyper parameter tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will try below hidden layers combination with dropout values of (0.2,0.4,0.5,0.6) and activation functions of 'relu','elu' and 'tanh' and optimizer='adam'\n",
    "\n",
    " - 63\n",
    " - 60+40+20\n",
    " - 80\n",
    " - 45+15+45\n",
    " - 40+50+40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model for 1 hidden layer of 63 neurons\n",
    "def build_deep_nn1(dropouts, actfn):\n",
    "\n",
    "    # initialize model\n",
    "    model = keras.Sequential()\n",
    "    # add an input layer and a hidden layer\n",
    "    model.add(Dense(63, input_dim = len(X_train.columns)))\n",
    "    # add activation layer to add non-linearity\n",
    "    model.add(Activation(actfn))\n",
    "    #Add Dropout layer\n",
    "    model.add(Dropout(dropouts))\n",
    "    # add output layer\n",
    "    model.add(Dense(1))\n",
    "    # add softmax layer \n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    # Compiling the model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model for 1 hidden layer of 80 neurons\n",
    "def build_deep_nn2(dropouts, actfn):\n",
    "    \n",
    "    # initialize model\n",
    "    model = keras.Sequential()\n",
    "    # add an input layer and a hidden layer\n",
    "    model.add(Dense(80, input_dim = len(X_train.columns)))\n",
    "    # add activation layer to add non-linearity\n",
    "    model.add(Activation(actfn))\n",
    "    #Add Dropout layer\n",
    "    model.add(Dropout(dropouts))\n",
    "    # add output layer\n",
    "    model.add(Dense(1))\n",
    "    # add softmax layer \n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    # Compiling the model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model for 3 hidden layers of 45,15 and 45 neurons each\n",
    "def build_deep_nn3(dropouts, actfn):\n",
    "    \n",
    "    # initialize model\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # add a hidden layer\n",
    "    model.add(Dense(45, input_dim = len(X_train.columns)))\n",
    "    # add activation layer to add non-linearity\n",
    "    model.add(Activation(actfn))\n",
    "    #Add Dropout layer\n",
    "    model.add(Dropout(dropouts))\n",
    "    \n",
    "    # add a hidden layer\n",
    "    model.add(Dense(15, input_dim = len(X_train.columns)))\n",
    "    # add activation layer to add non-linearity\n",
    "    model.add(Activation(actfn))\n",
    "    #Add Dropout layer\n",
    "    model.add(Dropout(dropouts))\n",
    "    \n",
    "    # add a hidden layer\n",
    "    model.add(Dense(45, input_dim = len(X_train.columns)))\n",
    "    # add activation layer to add non-linearity\n",
    "    model.add(Activation(actfn))\n",
    "    #Add Dropout layer\n",
    "    model.add(Dropout(dropouts))\n",
    "    \n",
    "    \n",
    "    # add output layer\n",
    "    model.add(Dense(1))\n",
    "    # add softmax layer \n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    # Compiling the model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model for 3 hidden layers of 60,40 and 20 neurons each\n",
    "def build_deep_nn4(dropouts, actfn):\n",
    "    \n",
    "    # initialize model\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # add a hidden layer\n",
    "    model.add(Dense(60, input_dim = len(X_train.columns)))\n",
    "    # add activation layer to add non-linearity\n",
    "    model.add(Activation(actfn))\n",
    "    #Add Dropout layer\n",
    "    model.add(Dropout(dropouts))\n",
    "    \n",
    "    # add a hidden layer\n",
    "    model.add(Dense(40, input_dim = len(X_train.columns)))\n",
    "    # add activation layer to add non-linearity\n",
    "    model.add(Activation(actfn))\n",
    "    #Add Dropout layer\n",
    "    model.add(Dropout(dropouts))\n",
    "    \n",
    "    # add a hidden layer\n",
    "    model.add(Dense(20, input_dim = len(X_train.columns)))\n",
    "    # add activation layer to add non-linearity\n",
    "    model.add(Activation(actfn))\n",
    "    #Add Dropout layer\n",
    "    model.add(Dropout(dropouts))\n",
    "    \n",
    "    \n",
    "    # add output layer\n",
    "    model.add(Dense(1))\n",
    "    # add softmax layer \n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model for 3 hidden layers of 40 50 amd 40 neurons each\n",
    "def build_deep_nn5(dropouts, actfn):\n",
    " \n",
    "    # initialize model\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # add a hidden layer\n",
    "    model.add(Dense(40, input_dim = len(X_train.columns)))\n",
    "    # add activation layer to add non-linearity\n",
    "    model.add(Activation(actfn))\n",
    "    #Add Dropout layer\n",
    "    model.add(Dropout(dropouts))\n",
    "    \n",
    "    # add a hidden layer\n",
    "    model.add(Dense(50, input_dim = len(X_train.columns)))\n",
    "    # add activation layer to add non-linearity\n",
    "    model.add(Activation(actfn))\n",
    "    #Add Dropout layer\n",
    "    model.add(Dropout(dropouts))\n",
    "    \n",
    "    # add a hidden layer\n",
    "    model.add(Dense(40, input_dim = len(X_train.columns)))\n",
    "    # add activation layer to add non-linearity\n",
    "    model.add(Activation(actfn))\n",
    "    #Add Dropout layer\n",
    "    model.add(Dropout(dropouts))\n",
    "    \n",
    "    \n",
    "    # add output layer\n",
    "    model.add(Dense(1))\n",
    "    # add softmax layer \n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_train_model(X_train, y_train, X_test, y_test, num_epochs):\n",
    "\n",
    "    data_list=[]\n",
    "\n",
    "    dropout_list=[0.2,0.4,0.5,0.6]\n",
    "    act_fnlist=['relu','selu','tanh']\n",
    "    batch_size=[32,64,128,256, 512]\n",
    "    data_record=[]\n",
    "    fucntion_list=[build_deep_nn1,build_deep_nn2,build_deep_nn3,build_deep_nn4,build_deep_nn5]\n",
    "    function_name=['deep_nn1','deep_nn2','deep_nn3','deep_nn4','deep_nn5']\n",
    "    \n",
    "    for name, fn in zip(function_name,fucntion_list):\n",
    "        for actfn in act_fnlist:\n",
    "            for dropout in dropout_list:\n",
    "                for batch in batch_size:\n",
    "                    data_record=[]\n",
    "                    deep_model=fn(dropout, actfn)\n",
    "                    model_info = deep_model.fit(X_train, y_train, epochs=num_epochs, validation_split=0.2,\n",
    "                                        verbose=0, callbacks=[tfdocs.modeling.EpochDots()], batch_size=batch)\n",
    "                    model_hist=pd.DataFrame(model_info.history)\n",
    "                    model_hist['epochs']=model_info.epoch\n",
    "                    model_hist=model_hist.sort_values(by='val_accuracy', ascending=False)\n",
    "                    model_hist.reset_index(drop=True, inplace=True)\n",
    "                    _, train_acc = deep_model.evaluate(X_train, y_train, verbose=1)\n",
    "                    _, test_acc = deep_model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "                    print(\"\\nActivation - \", actfn)\n",
    "                    print(\"\\nDropout - \", dropout)\n",
    "                    print(\"\\nBatch size - \", batch)\n",
    "                    print(\"\\nModel summary - \\n\", deep_model.summary())\n",
    "                    print(model_hist.head())\n",
    "                    print (\"\\nTrain accuracy - \", train_acc)\n",
    "                    print (\"\\nTest accuracy - \", test_acc)\n",
    "                    plt.plot(model_info.history['loss'], label='train')\n",
    "                    plt.plot(model_info.history['val_loss'], label='test')\n",
    "                    plt.legend()\n",
    "                    plt.title(\"Accuracies\")\n",
    "                    plt.show()\n",
    "                    print(\"=========================================\")\n",
    "                    data_record.append(name)\n",
    "                    data_record.append(actfn)\n",
    "                    data_record.append(dropout)\n",
    "                    data_record.append(batch)\n",
    "                    data_record.append(model_hist.loc[0,'accuracy'])\n",
    "                    data_record.append(model_hist.loc[0,'val_accuracy'])\n",
    "                    data_record.append(model_hist.loc[0,'val_loss'])\n",
    "                    data_record.append(train_acc)\n",
    "                    data_record.append(test_acc)\n",
    "                    data_record.append(deep_model.summary())\n",
    "                    data_record.append(model_hist.loc[0,'epochs'])\n",
    "                    data_record.append(num_epochs)\n",
    "                    data_list.append(tuple(data_record))\n",
    "                    print(\"*************************************************************************************************************************\")\n",
    "                    print(\"*************************************************************************************************************************\")\n",
    "                    \n",
    "                    print(\"\\n\")\n",
    "\n",
    "    deep_model_data=pd.DataFrame(data_list, columns=['model','activation', 'dropout', 'batchsize', 'trainaccuracy', 'testaccuracy', \n",
    "                                          'loss', 'meantrainaccuracy', 'meantestaccuracy', 'summary', 'epoch', 'epochs'])\n",
    "    return deep_model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_500_1=build_train_model(X_train_norm11, y_train, X_test_norm11, y_test, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple configurations of layers and neurons tested with different values of batch size and dropouts with multiple activation functions for epoch of 500 and below is the observation\n",
    "\n",
    "- relu activation fucntions with the batch sizes of 32,64,128,256 with any value of dropout seems to have high variation in loss for training and testing data, could be overfitting\n",
    "- Though relu with batch size of 512 seems more stable and less difference in loss for test and train data.\n",
    "- But the activation fucntions os tanh and selu with dropout of 0.5 and 0.6 and batch size of 256 and 512 seems most stable.\n",
    "\n",
    "Also, in many scenarios, the loss has become constant after certain number of epochs, so early stop callback needs to be included to halt at right time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_500_2=build_train_model(X_train_norm14, y_train, X_test_norm14, y_test, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple configurations of layers and neurons tested with different values of batch size and dropouts with multiple activation functions for epoch of 500 and below is the observation\n",
    "\n",
    "- relu activation fucntions with the batch sizes of 32,64,128,256 with any value of dropout seems to have high variation in loss for training and testing data, could be overfitting\n",
    "- Though relu with batch size of 512 seems more stable and less difference in loss for test and train data.\n",
    "- But the activation fucntions os tanh and selu with dropout of 0.5 and 0.6 and batch size of 256 and 512 seems most stable.\n",
    "\n",
    "Also, in many scenarios, the loss has become constant after certain number of epochs, so early stop callback needs to be included to halt at right time\n",
    "\n",
    "Also, the models with multiple layers seems to be more stable than models with single layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1000_1=build_train_model(X_train_norm11, y_train, X_test_norm11, y_test, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple configurations of layers and neurons tested with different values of batch size and dropouts with multiple activation functions for epoch of 500 and below is the observation\n",
    "\n",
    "- relu activation fucntions with the batch sizes of 32,64,128,256 with any value of dropout seems to have high variation in loss for training and testing data, could be overfitting\n",
    "- Though relu with batch size of 512 seems more stable and less difference in loss for test and train data.\n",
    "- But the activation fucntions os tanh and selu with dropout of 0.5 and 0.6 and batch size of 256 and 512 seems most stable.\n",
    "\n",
    "Also, in many scenarios, the loss has become constant after certain number of epochs, so early stop callback needs to be included to halt at right time\n",
    "\n",
    "Also, the models with multiple layers seems to be more stable than models with single layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1000_2=build_train_model(X_train_norm14, y_train, X_test_norm14, y_test, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple configurations of layers and neurons tested with different values of batch size and dropouts with multiple activation functions for epoch of 500 and below is the observation\n",
    "\n",
    "- relu activation fucntions with the batch sizes of 32,64,128,256 with any value of dropout seems to have high variation in loss for training and testing data, could be overfitting\n",
    "- Though relu with batch size of 512 seems more stable and less difference in loss for test and train data.\n",
    "- But the activation fucntions os tanh and selu with dropout of 0.5 and 0.6 and batch size of 256 and 512 seems most stable.\n",
    "\n",
    "Also, in many scenarios, the loss has become constant after certain number of epochs, so early stop callback needs to be included to halt at right time\n",
    "\n",
    "Also, the models with multiple layers seems to be more stable than models with single layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_500_1.sort_values(by='testaccuracy', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_500_2.sort_values(by='testaccuracy', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1000_1.sort_values(by='testaccuracy', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1000_2.sort_values(by='testaccuracy', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_500_1.to_csv('Dataframe_500_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_500_2.to_csv('Dataframe_500_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1000_1.to_csv('Dataframe_1000_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1000_2.to_csv('Dataframe_1000_2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above data, we analyzed that deep__n5, deep_nn4 and deep_nn3 model performed better and specifically with batch size of 128,256 and 512 and with activation fucntions - relu and selu and epoch=1000 and dropout of 0.4, 0.5 and .6\n",
    "\n",
    "So, we will apply batch normalization, regularization tehnique, callbacks to halt early when accuracy reaches the specificed value and starts decreasing thereafter and various custom metrics like AUC/ROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section1606></a>\n",
    "### **16.6 Batch Normalization and Weight initializer on models selected above along with different optimizers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section16061></a>\n",
    "#### **16.6.1 Using Batch Normalization and early stopping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model for 3 hidden layers of 60,40 and 20 neurons each\n",
    "def build_deep_nn1_withbn(dropouts, actfn):\n",
    "    \n",
    "    # initialize model\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # add a hidden layer\n",
    "    model.add(Dense(60, input_dim = len(X_train.columns)))\n",
    "    model.add(BatchNormalization())\n",
    "    # add activation layer to add non-linearity\n",
    "    model.add(Activation(actfn))\n",
    "    #Add Dropout layer\n",
    "    model.add(Dropout(dropouts))\n",
    "    \n",
    "    # add a hidden layer\n",
    "    model.add(Dense(40, input_dim = len(X_train.columns)))\n",
    "    model.add(BatchNormalization())\n",
    "    # add activation layer to add non-linearity\n",
    "    model.add(Activation(actfn))\n",
    "    #Add Dropout layer\n",
    "    model.add(Dropout(dropouts))\n",
    "    \n",
    "    # add a hidden layer\n",
    "    model.add(Dense(20, input_dim = len(X_train.columns)))\n",
    "    model.add(BatchNormalization())\n",
    "    # add activation layer to add non-linearity\n",
    "    model.add(Activation(actfn))\n",
    "    #Add Dropout layer\n",
    "    model.add(Dropout(dropouts))\n",
    "    \n",
    "    \n",
    "    # add output layer\n",
    "    model.add(Dense(1))\n",
    "    # add softmax layer \n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model for 3 hidden layers of 45,15 and 45 neurons each\n",
    "def build_deep_nn2_withbn(dropouts, actfn):\n",
    "    \n",
    "    # initialize model\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # add a hidden layer\n",
    "    model.add(Dense(45, input_dim = len(X_train.columns)))\n",
    "    model.add(BatchNormalization())\n",
    "    # add activation layer to add non-linearity\n",
    "    model.add(Activation(actfn))\n",
    "    #Add Dropout layer\n",
    "    model.add(Dropout(dropouts))\n",
    "    \n",
    "    # add a hidden layer\n",
    "    model.add(Dense(15, input_dim = len(X_train.columns)))\n",
    "    model.add(BatchNormalization())\n",
    "    # add activation layer to add non-linearity\n",
    "    model.add(Activation(actfn))\n",
    "    #Add Dropout layer\n",
    "    model.add(Dropout(dropouts))\n",
    "    \n",
    "    # add a hidden layer\n",
    "    model.add(Dense(45, input_dim = len(X_train.columns)))\n",
    "    model.add(BatchNormalization())\n",
    "    # add activation layer to add non-linearity\n",
    "    model.add(Activation(actfn))\n",
    "    #Add Dropout layer\n",
    "    model.add(Dropout(dropouts))\n",
    "    \n",
    "    \n",
    "    # add output layer\n",
    "    model.add(Dense(1))\n",
    "    # add softmax layer \n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    # Compiling the model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_deep_nn3_withbn(dropouts, actfn):\n",
    " \n",
    "    # initialize model\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # add a hidden layer\n",
    "    model.add(Dense(40, input_dim = len(X_train.columns)))\n",
    "    model.add(BatchNormalization())\n",
    "    # add activation layer to add non-linearity\n",
    "    model.add(Activation(actfn))\n",
    "    #Add Dropout layer\n",
    "    model.add(Dropout(dropouts))\n",
    "    \n",
    "    # add a hidden layer\n",
    "    model.add(Dense(50, input_dim = len(X_train.columns)))\n",
    "    model.add(BatchNormalization())\n",
    "    # add activation layer to add non-linearity\n",
    "    model.add(Activation(actfn))\n",
    "    #Add Dropout layer\n",
    "    model.add(Dropout(dropouts))\n",
    "    \n",
    "    # add a hidden layer\n",
    "    model.add(Dense(40, input_dim = len(X_train.columns)))\n",
    "    model.add(BatchNormalization())\n",
    "    # add activation layer to add non-linearity\n",
    "    model.add(Activation(actfn))\n",
    "    #Add Dropout layer\n",
    "    model.add(Dropout(dropouts))\n",
    "    \n",
    "    \n",
    "    # add output layer\n",
    "    model.add(Dense(1))\n",
    "    # add softmax layer \n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_train_model_bn(X_train, y_train, X_test, y_test, num_epochs=1000):\n",
    "\n",
    "    data_list=[]\n",
    "\n",
    "    dropout_list=[0.4,0.5,0.6]\n",
    "    act_fnlist=['selu','tanh']\n",
    "    batch_size=[256, 512]\n",
    "    data_record=[]\n",
    "    fucntion_list=[build_deep_nn1_withbn,build_deep_nn2_withbn, build_deep_nn3_withbn]\n",
    "    function_name=['deep_nn1bn','deep_nn2bn','deep_nn3bn']\n",
    "    \n",
    "    for name, fn in zip(function_name,fucntion_list):\n",
    "        for actfn in act_fnlist:\n",
    "            for dropout in dropout_list:\n",
    "                for batch in batch_size:\n",
    "                    data_record=[]\n",
    "                    deep_model=fn(dropout, actfn)\n",
    "                    earlystop = EarlyStopping(monitor='val_accuracy', min_delta=0.001, patience=5, verbose=1, mode='auto')\n",
    "                    callbacks_list = [tfdocs.modeling.EpochDots(), earlystop]\n",
    "                    model_info = deep_model.fit(X_train, y_train, epochs=num_epochs, validation_split=0.2,\n",
    "                                        verbose=0, callbacks=callbacks_list, batch_size=batch)\n",
    "                    model_hist=pd.DataFrame(model_info.history)\n",
    "                    model_hist['epochs']=model_info.epoch\n",
    "                    model_hist=model_hist.sort_values(by='val_accuracy', ascending=False)\n",
    "                    model_hist.reset_index(drop=True, inplace=True)\n",
    "                    _, train_acc = deep_model.evaluate(X_train, y_train, verbose=1)\n",
    "                    _, test_acc = deep_model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "                    print(\"\\nActivation - \", actfn)\n",
    "                    print(\"\\nDropout - \", dropout)\n",
    "                    print(\"\\nBatch size - \", batch)\n",
    "                    print(\"\\nModel summary - \\n\", deep_model.summary())\n",
    "                    print(model_hist.head())\n",
    "                    print (\"\\nTrain accuracy - \", train_acc)\n",
    "                    print (\"\\nTest accuracy - \", test_acc)\n",
    "                    plt.plot(model_info.history['loss'], label='train')\n",
    "                    plt.plot(model_info.history['val_loss'], label='test')\n",
    "                    plt.legend()\n",
    "                    plt.title(\"Validation Loss\")\n",
    "                    plt.show()\n",
    "                    print(\"=========================================\")\n",
    "                    data_record.append(name)\n",
    "                    data_record.append(actfn)\n",
    "                    data_record.append(dropout)\n",
    "                    data_record.append(batch)\n",
    "                    data_record.append(model_hist.loc[0,'accuracy'])\n",
    "                    data_record.append(model_hist.loc[0,'val_accuracy'])\n",
    "                    data_record.append(model_hist.loc[0,'val_loss'])\n",
    "                    data_record.append(train_acc)\n",
    "                    data_record.append(test_acc)\n",
    "                    data_record.append(deep_model.summary())\n",
    "                    data_record.append(model_hist.loc[0,'epochs'])\n",
    "                    data_record.append(num_epochs)\n",
    "                    data_list.append(tuple(data_record))\n",
    "                    print(\"*************************************************************************************************************************\")\n",
    "                    print(\"*************************************************************************************************************************\")\n",
    "                    \n",
    "                    print(\"\\n\")\n",
    "\n",
    "    deep_model_data=pd.DataFrame(data_list, columns=['model','activation', 'dropout', 'batchsize', 'trainaccuracy', 'testaccuracy', \n",
    "                                          'loss', 'meantrainaccuracy', 'meantestaccuracy', 'summary', 'epoch', 'epochs'])\n",
    "    return deep_model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_500_bn_1=build_train_model_bn(X_train_norm11, y_train, X_test_norm11, y_test, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_500_bn_1.sort_values(by='testaccuracy', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_500_bn_1.to_csv('Dataframe_500_bn_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_500_bn_2=build_train_model_bn(X_train_norm14, y_train, X_test_norm14, y_test, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_500_bn_2.sort_values(by='testaccuracy', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_500_bn_2.to_csv('Dataframe_500_bn_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1000_bn_1=build_train_model_bn(X_train_norm11, y_train, X_test_norm11, y_test, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1000_bn_1.sort_values(by='testaccuracy', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1000_bn_1.to_csv('Dataframe_1000_bn_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1000_bn_2=build_train_model_bn(X_train_norm14, y_train, X_test_norm14, y_test, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1000_bn_2.sort_values(by='testaccuracy', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1000_bn_2.to_csv('Dataframe_1000_bn_2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After including BatchNormalization and EarlyStopping on validation loss, it improved the performnace a bit in terms of accuracy values in some cases but decreases the  time to train the model.\n",
    "So, below are onfigurations that performed well with EarlyStopping and BatchNormalization\n",
    "\n",
    "- Robust Scaled data\n",
    "- Epoch=500\n",
    "- Droput - 0.4 and 0.5\n",
    "- Batch size = 256 and 512\n",
    "- activation fn=  tanh\n",
    "- Model - 3 hidden layers of 40 50 amd 40 neurons each"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section16062></a>\n",
    "#### **16.6.2 Applying Kernel initializers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wiil try to add different values for kernel_initializer and bias_initializer and analyze if its enahnce the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model for 3 hidden layers of 60,40 and 20 neurons each\n",
    "def build_deep_nn1_withbninit(dropouts, actfn, kernel_initializer, bias_initializer):\n",
    "    \n",
    "    # initialize model\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # add a hidden layer\n",
    "    model.add(Dense(60, kernel_initializer=kernel_initializer, bias_initializer=bias_initializer, input_dim = len(X_train.columns)))\n",
    "    model.add(BatchNormalization())\n",
    "    # add activation layer to add non-linearity\n",
    "    model.add(Activation(actfn))\n",
    "    #Add Dropout layer\n",
    "    model.add(Dropout(dropouts))\n",
    "    \n",
    "    # add a hidden layer\n",
    "    model.add(Dense(40, kernel_initializer=kernel_initializer, bias_initializer=bias_initializer, input_dim = len(X_train.columns)))\n",
    "    model.add(BatchNormalization())\n",
    "    # add activation layer to add non-linearity\n",
    "    model.add(Activation(actfn))\n",
    "    #Add Dropout layer\n",
    "    model.add(Dropout(dropouts))\n",
    "    \n",
    "    # add a hidden layer\n",
    "    model.add(Dense(20, kernel_initializer=kernel_initializer, bias_initializer=bias_initializer, input_dim = len(X_train.columns)))\n",
    "    model.add(BatchNormalization())\n",
    "    # add activation layer to add non-linearity\n",
    "    model.add(Activation(actfn))\n",
    "    #Add Dropout layer\n",
    "    model.add(Dropout(dropouts))\n",
    "    \n",
    "    \n",
    "    # add output layer\n",
    "    model.add(Dense(1))\n",
    "    # add softmax layer \n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model for 3 hidden layers of 45,15 and 45 neurons each\n",
    "def build_deep_nn2_withbninit(dropouts, actfn, kernel_initializer, bias_initializer):\n",
    "    \n",
    "    # initialize model\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # add a hidden layer\n",
    "    model.add(Dense(45, kernel_initializer=kernel_initializer, bias_initializer=bias_initializer, input_dim = len(X_train.columns)))\n",
    "    model.add(BatchNormalization())\n",
    "    # add activation layer to add non-linearity\n",
    "    model.add(Activation(actfn))\n",
    "    #Add Dropout layer\n",
    "    model.add(Dropout(dropouts))\n",
    "    \n",
    "    # add a hidden layer\n",
    "    model.add(Dense(15, kernel_initializer=kernel_initializer, bias_initializer=bias_initializer, input_dim = len(X_train.columns)))\n",
    "    model.add(BatchNormalization())\n",
    "    # add activation layer to add non-linearity\n",
    "    model.add(Activation(actfn))\n",
    "    #Add Dropout layer\n",
    "    model.add(Dropout(dropouts))\n",
    "    \n",
    "    # add a hidden layer\n",
    "    model.add(Dense(45, kernel_initializer=kernel_initializer, bias_initializer=bias_initializer, input_dim = len(X_train.columns)))\n",
    "    model.add(BatchNormalization())\n",
    "    # add activation layer to add non-linearity\n",
    "    model.add(Activation(actfn))\n",
    "    #Add Dropout layer\n",
    "    model.add(Dropout(dropouts))\n",
    "    \n",
    "    \n",
    "    # add output layer\n",
    "    model.add(Dense(1))\n",
    "    # add softmax layer \n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    # Compiling the model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model for 3 hidden layers of 40 50 amd 40 neurons each\n",
    "def build_deep_nn3_withbninit(dropouts, actfn, kernel_initializer, bias_initializer):\n",
    " \n",
    "    # initialize model\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # add a hidden layer\n",
    "    model.add(Dense(40, kernel_initializer=kernel_initializer, bias_initializer=bias_initializer, input_dim = len(X_train.columns)))\n",
    "    model.add(BatchNormalization())\n",
    "    # add activation layer to add non-linearity\n",
    "    model.add(Activation(actfn))\n",
    "    #Add Dropout layer\n",
    "    model.add(Dropout(dropouts))\n",
    "    \n",
    "    # add a hidden layer\n",
    "    model.add(Dense(50, kernel_initializer=kernel_initializer, bias_initializer=bias_initializer, input_dim = len(X_train.columns)))\n",
    "    model.add(BatchNormalization())\n",
    "    # add activation layer to add non-linearity\n",
    "    model.add(Activation(actfn))\n",
    "    #Add Dropout layer\n",
    "    model.add(Dropout(dropouts))\n",
    "    \n",
    "    # add a hidden layer\n",
    "    model.add(Dense(40, kernel_initializer=kernel_initializer, bias_initializer=bias_initializer, input_dim = len(X_train.columns)))\n",
    "    model.add(BatchNormalization())\n",
    "    # add activation layer to add non-linearity\n",
    "    model.add(Activation(actfn))\n",
    "    #Add Dropout layer\n",
    "    model.add(Dropout(dropouts))\n",
    "    \n",
    "    \n",
    "    # add output layer\n",
    "    model.add(Dense(1))\n",
    "    # add softmax layer \n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_train_model_bninit(X_train, y_train, X_test, y_test, num_epochs=1000):\n",
    "\n",
    "    data_list=[]\n",
    "\n",
    "    dropout_list=[0.4,0.5,0.6]\n",
    "    act_fnlist=['selu','tanh']\n",
    "    batch_size=[256, 512]\n",
    "    data_record=[]\n",
    "    kernel_initializer=[tf.keras.initializers.RandomNormal(mean=0., stddev=1.), tf.keras.initializers.RandomUniform(minval=0., maxval=1.), tf.keras.initializers.TruncatedNormal(mean=0., stddev=1.)]\n",
    "    kname_list=['normal','uniform', 'tnormal']\n",
    "\n",
    "    bias_initializer=[tf.keras.initializers.Zeros(), tf.keras.initializers.Ones(), tf.keras.initializers.RandomNormal(mean=0., stddev=1.), tf.keras.initializers.RandomUniform(minval=0., maxval=1.), tf.keras.initializers.TruncatedNormal(mean=0., stddev=1.)]\n",
    "    bname_list=['zero', 'one', 'normal','uniform', 'tnormal']\n",
    "\n",
    "    fucntion_list=[build_deep_nn1_withbninit,build_deep_nn2_withbninit, build_deep_nn3_withbninit]\n",
    "    function_name=['deep_nn1bn','deep_nn2bn','deep_nn3bn']\n",
    "    \n",
    "    for name, fn in zip(function_name,fucntion_list):\n",
    "        for actfn in act_fnlist:\n",
    "            for dropout in dropout_list:\n",
    "                for batch in batch_size:\n",
    "                    for kname, kinit in zip(kname_list,kernel_initializer):\n",
    "                        for bname, binit in zip(bname_list, bias_initializer):\n",
    "                            data_record=[]\n",
    "                            deep_model=fn(dropout, actfn, kinit, binit)\n",
    "                            earlystop = EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=5, verbose=1, mode='auto')\n",
    "                            model_info = deep_model.fit(X_train, y_train, epochs=num_epochs, validation_split=0.2,\n",
    "                                                verbose=0, callbacks=[tfdocs.modeling.EpochDots(), earlystop],  batch_size=batch)\n",
    "                            model_hist=pd.DataFrame(model_info.history)\n",
    "                            model_hist['epochs']=model_info.epoch\n",
    "                            model_hist=model_hist.sort_values(by='val_accuracy', ascending=False)\n",
    "                            model_hist.reset_index(drop=True, inplace=True)\n",
    "                            _, train_acc = deep_model.evaluate(X_train, y_train, verbose=1)\n",
    "                            _, test_acc = deep_model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "                            print(\"\\nActivation - \", actfn)\n",
    "                            print(\"\\nDropout - \", dropout)\n",
    "                            print(\"\\nBatch size - \", batch)\n",
    "                            print (\"\\nKernel init method - \", kname)\n",
    "                            print(\"\\Bias init method - \", bname)\n",
    "                            print(\"\\nModel summary - \\n\", deep_model.summary())\n",
    "                            print(model_hist.head())\n",
    "                            print (\"\\nTrain accuracy - \", train_acc)\n",
    "                            print (\"\\nTest accuracy - \", test_acc)\n",
    "                            plt.plot(model_info.history['loss'], label='train')\n",
    "                            plt.plot(model_info.history['val_loss'], label='test')\n",
    "                            plt.legend()\n",
    "                            plt.title(\"Validation Loss\")\n",
    "                            plt.show()\n",
    "                            print(\"=========================================\")\n",
    "                            data_record.append(name)\n",
    "                            data_record.append(actfn)\n",
    "                            data_record.append(dropout)\n",
    "                            data_record.append(batch)\n",
    "                            data_record.append(kname)\n",
    "                            data_record.append(bname)\n",
    "                            data_record.append(model_hist.loc[0,'accuracy'])\n",
    "                            data_record.append(model_hist.loc[0,'val_accuracy'])\n",
    "                            data_record.append(model_hist.loc[0,'val_loss'])\n",
    "                            data_record.append(train_acc)\n",
    "                            data_record.append(test_acc)\n",
    "                            data_record.append(deep_model.summary())\n",
    "                            data_record.append(model_hist.loc[0,'epochs'])\n",
    "                            data_record.append(num_epochs)\n",
    "                            data_list.append(tuple(data_record))\n",
    "                            print(\"*************************************************************************************************************************\")\n",
    "                            print(\"*************************************************************************************************************************\")\n",
    "\n",
    "                            print(\"\\n\")\n",
    "\n",
    "    deep_model_data=pd.DataFrame(data_list, columns=['model','activation', 'dropout', 'Kernelinitmethod', 'biasinitimethod', 'batchsize', 'trainaccuracy', 'testaccuracy', \n",
    "                                          'loss', 'meantrainaccuracy', 'meantestaccuracy', 'summary', 'epoch', 'epochs'])\n",
    "    return deep_model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_500_bninit_1=build_train_model_bninit(X_train_norm11, y_train, X_test_norm11, y_test, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_500_bninit_1.sort_values(by='testaccuracy', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No significant improvement in accuracy by including the kernel initializers, however the below configurations has perfomed better than all-\n",
    "\n",
    "- selu/tanh + batch size=256 and kernel initializer = uniform with bias initializer - uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_500_bninit_1.to_csv('Dataframe_500_bninit_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_500_bninit_2=build_train_model_bninit(X_train_norm14, y_train, X_test_norm14, y_test, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_500_bninit_2.sort_values(by='testaccuracy', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simialr bheaviour as above , no significant improvement by addding kernel and bias initializer but selu/tanh with uniform values of both performed better than rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_500_bninit_2.to_csv('Dataframe_500_bninit_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1000_bninit_1=build_train_model_bninit(X_train_norm11, y_train, X_test_norm11, y_test, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1000_bninit_1.sort_values(by='testaccuracy', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simialr bheaviour as above , no significant improvement by addding kernel and bias initializer but tanh with uniform and normal values of both performed better than rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1000_bninit_1.to_csv('Dataframe_1000_bninit_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1000_bninit_2=build_train_model_bninit(X_train_norm14, y_train, X_test_norm14, y_test, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1000_bninit_2.sort_values(by='testaccuracy', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No significant improvement by including kernel and bias initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1000_bninit_2.to_csv('Dataframe_1000_bninit_2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernel initializer not performed well in terms of accuracies improvement but still can experiment further only with kernel initializer once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model for 3 hidden layers of 60,40 and 20 neurons each\n",
    "def build_deep_nn1_withbninit2(dropouts, actfn, kernel_initializer):\n",
    "    \n",
    "    # initialize model\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # add a hidden layer\n",
    "    model.add(Dense(60, kernel_initializer=kernel_initializer, input_dim = len(X_train.columns)))\n",
    "    model.add(BatchNormalization())\n",
    "    # add activation layer to add non-linearity\n",
    "    model.add(Activation(actfn))\n",
    "    #Add Dropout layer\n",
    "    model.add(Dropout(dropouts))\n",
    "    \n",
    "    # add a hidden layer\n",
    "    model.add(Dense(40, kernel_initializer=kernel_initializer, input_dim = len(X_train.columns)))\n",
    "    model.add(BatchNormalization())\n",
    "    # add activation layer to add non-linearity\n",
    "    model.add(Activation(actfn))\n",
    "    #Add Dropout layer\n",
    "    model.add(Dropout(dropouts))\n",
    "    \n",
    "    # add a hidden layer\n",
    "    model.add(Dense(20, kernel_initializer=kernel_initializer,  input_dim = len(X_train.columns)))\n",
    "    model.add(BatchNormalization())\n",
    "    # add activation layer to add non-linearity\n",
    "    model.add(Activation(actfn))\n",
    "    #Add Dropout layer\n",
    "    model.add(Dropout(dropouts))\n",
    "    \n",
    "    \n",
    "    # add output layer\n",
    "    model.add(Dense(1))\n",
    "    # add softmax layer \n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model for 3 hidden layers of 45,15 and 45 neurons each\n",
    "def build_deep_nn2_withbninit2(dropouts, actfn, kernel_initializer):\n",
    "    \n",
    "    # initialize model\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # add a hidden layer\n",
    "    model.add(Dense(45, kernel_initializer=kernel_initializer, input_dim = len(X_train.columns)))\n",
    "    model.add(BatchNormalization())\n",
    "    # add activation layer to add non-linearity\n",
    "    model.add(Activation(actfn))\n",
    "    #Add Dropout layer\n",
    "    model.add(Dropout(dropouts))\n",
    "    \n",
    "    # add a hidden layer\n",
    "    model.add(Dense(15, kernel_initializer=kernel_initializer,  input_dim = len(X_train.columns)))\n",
    "    model.add(BatchNormalization())\n",
    "    # add activation layer to add non-linearity\n",
    "    model.add(Activation(actfn))\n",
    "    #Add Dropout layer\n",
    "    model.add(Dropout(dropouts))\n",
    "    \n",
    "    # add a hidden layer\n",
    "    model.add(Dense(45, kernel_initializer=kernel_initializer,  input_dim = len(X_train.columns)))\n",
    "    model.add(BatchNormalization())\n",
    "    # add activation layer to add non-linearity\n",
    "    model.add(Activation(actfn))\n",
    "    #Add Dropout layer\n",
    "    model.add(Dropout(dropouts))\n",
    "    \n",
    "    \n",
    "    # add output layer\n",
    "    model.add(Dense(1))\n",
    "    # add softmax layer \n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    # Compiling the model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model for 3 hidden layers of 40 50 amd 40 neurons each\n",
    "def build_deep_nn3_withbninit2(dropouts, actfn, kernel_initializer):\n",
    " \n",
    "    # initialize model\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # add a hidden layer\n",
    "    model.add(Dense(40, kernel_initializer=kernel_initializer,  input_dim = len(X_train.columns)))\n",
    "    model.add(BatchNormalization())\n",
    "    # add activation layer to add non-linearity\n",
    "    model.add(Activation(actfn))\n",
    "    #Add Dropout layer\n",
    "    model.add(Dropout(dropouts))\n",
    "    \n",
    "    # add a hidden layer\n",
    "    model.add(Dense(50, kernel_initializer=kernel_initializer, input_dim = len(X_train.columns)))\n",
    "    model.add(BatchNormalization())\n",
    "    # add activation layer to add non-linearity\n",
    "    model.add(Activation(actfn))\n",
    "    #Add Dropout layer\n",
    "    model.add(Dropout(dropouts))\n",
    "    \n",
    "    # add a hidden layer\n",
    "    model.add(Dense(40, kernel_initializer=kernel_initializer,  input_dim = len(X_train.columns)))\n",
    "    model.add(BatchNormalization())\n",
    "    # add activation layer to add non-linearity\n",
    "    model.add(Activation(actfn))\n",
    "    #Add Dropout layer\n",
    "    model.add(Dropout(dropouts))\n",
    "    \n",
    "    \n",
    "    # add output layer\n",
    "    model.add(Dense(1))\n",
    "    # add softmax layer \n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_train_model_bninit2(X_train, y_train, X_test, y_test, num_epochs=1000):\n",
    "\n",
    "    data_list=[]\n",
    "\n",
    "    dropout_list=[0.4,0.5,0.6]\n",
    "    act_fnlist=['selu','tanh']\n",
    "    batch_size=[256, 512]\n",
    "    data_record=[]\n",
    "    kernel_initializer=[tf.keras.initializers.RandomNormal(mean=0., stddev=1.), tf.keras.initializers.RandomUniform(minval=0., maxval=1.), tf.keras.initializers.TruncatedNormal(mean=0., stddev=1.)]\n",
    "    kname_list=['normal','uniform', 'tnormal']\n",
    "\n",
    "    fucntion_list=[build_deep_nn1_withbninit2,build_deep_nn2_withbninit2, build_deep_nn3_withbninit2]\n",
    "    function_name=['deep_nn1bn','deep_nn2bn','deep_nn3bn']\n",
    "    \n",
    "    for name, fn in zip(function_name,fucntion_list):\n",
    "        for actfn in act_fnlist:\n",
    "            for dropout in dropout_list:\n",
    "                for batch in batch_size:\n",
    "                    for kname, kinit in zip(kname_list,kernel_initializer):\n",
    "                        data_record=[]\n",
    "                        deep_model=fn(dropout, actfn, kinit)\n",
    "                        earlystop = EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=5, verbose=1, mode='auto')\n",
    "                        model_info = deep_model.fit(X_train, y_train, epochs=num_epochs, validation_split=0.2,\n",
    "                                            verbose=0, callbacks=[tfdocs.modeling.EpochDots(), earlystop],  batch_size=batch)\n",
    "                        model_hist=pd.DataFrame(model_info.history)\n",
    "                        model_hist['epochs']=model_info.epoch\n",
    "                        model_hist=model_hist.sort_values(by='val_accuracy', ascending=False)\n",
    "                        model_hist.reset_index(drop=True, inplace=True)\n",
    "                        _, train_acc = deep_model.evaluate(X_train, y_train, verbose=1)\n",
    "                        _, test_acc = deep_model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "                        print(\"\\nActivation - \", actfn)\n",
    "                        print(\"\\nDropout - \", dropout)\n",
    "                        print(\"\\nBatch size - \", batch)\n",
    "                        print (\"\\nKernel init method - \", kname)\n",
    "                        print(\"\\nModel summary - \\n\", deep_model.summary())\n",
    "                        print(model_hist.head())\n",
    "                        print (\"\\nTrain accuracy - \", train_acc)\n",
    "                        print (\"\\nTest accuracy - \", test_acc)\n",
    "                        plt.plot(model_info.history['loss'], label='train')\n",
    "                        plt.plot(model_info.history['val_loss'], label='test')\n",
    "                        plt.legend()\n",
    "                        plt.title(\"Validation Loss\")\n",
    "                        plt.show()\n",
    "                        print(\"=========================================\")\n",
    "                        data_record.append(name)\n",
    "                        data_record.append(actfn)\n",
    "                        data_record.append(dropout)\n",
    "                        data_record.append(kname)\n",
    "                        data_record.append(batch)\n",
    "                        data_record.append(model_hist.loc[0,'accuracy'])\n",
    "                        data_record.append(model_hist.loc[0,'val_accuracy'])\n",
    "                        data_record.append(model_hist.loc[0,'val_loss'])\n",
    "                        data_record.append(train_acc)\n",
    "                        data_record.append(test_acc)\n",
    "                        data_record.append(deep_model.summary())\n",
    "                        data_record.append(model_hist.loc[0,'epochs'])\n",
    "                        data_record.append(num_epochs)\n",
    "                        data_list.append(tuple(data_record))\n",
    "                        print(\"*************************************************************************************************************************\")\n",
    "                        print(\"*************************************************************************************************************************\")\n",
    "\n",
    "                        print(\"\\n\")\n",
    "\n",
    "    deep_model_data=pd.DataFrame(data_list, columns=['model','activation', 'dropout', 'Kernelinitmethod', 'batchsize', 'trainaccuracy', 'testaccuracy', \n",
    "                                          'loss', 'meantrainaccuracy', 'meantestaccuracy', 'summary', 'epoch', 'epochs'])\n",
    "    return deep_model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_500_bninit_k=build_train_model_bninit2(X_train_norm14, y_train, X_test_norm14, y_test, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_500_bninit_k.sort_values(by='testaccuracy', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_500_bninit_k.to_csv('Dataframe_500_bninit_k.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel initializer (without bias initializer) has not enahnced the performance in terms of accuracies\n",
    "\n",
    "So further experiment can be done with below configurations of layers and neurons with batch Normalization and EarlyStopping\n",
    " - 3 hidden layers of 40 50 amd 40 neurons each\n",
    " - 3 hidden layers of 60,40 and 20 neurons each\n",
    " - 3 hidden layers of 45,15 and 45 neurons each\n",
    " \n",
    " Epoch  = 500 and Data is RobustScaler and Standrd Scaler with dropouts of .4,.5 and .6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section1607></a>\n",
    "### **16.7 Analyzing with various Optimizers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_deep_nn1_fn(dropouts, actfn, optimizer):\n",
    "    \n",
    "    # initialize model\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # add a hidden layer\n",
    "    model.add(Dense(60, input_dim = len(X_train.columns)))\n",
    "    model.add(BatchNormalization())\n",
    "    # add activation layer to add non-linearity\n",
    "    model.add(Activation(actfn))\n",
    "    #Add Dropout layer\n",
    "    model.add(Dropout(dropouts))\n",
    "    \n",
    "    # add a hidden layer\n",
    "    model.add(Dense(40, input_dim = len(X_train.columns)))\n",
    "    model.add(BatchNormalization())\n",
    "    # add activation layer to add non-linearity\n",
    "    model.add(Activation(actfn))\n",
    "    #Add Dropout layer\n",
    "    model.add(Dropout(dropouts))\n",
    "    \n",
    "    # add a hidden layer\n",
    "    model.add(Dense(20, input_dim = len(X_train.columns)))\n",
    "    model.add(BatchNormalization())\n",
    "    # add activation layer to add non-linearity\n",
    "    model.add(Activation(actfn))\n",
    "    #Add Dropout layer\n",
    "    model.add(Dropout(dropouts))\n",
    "    \n",
    "    \n",
    "    # add output layer\n",
    "    model.add(Dense(1))\n",
    "    # add softmax layer \n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_deep_nn2_fn(dropouts, actfn, optimizer):\n",
    "    \n",
    "    # initialize model\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # add a hidden layer\n",
    "    model.add(Dense(45, input_dim = len(X_train.columns)))\n",
    "    model.add(BatchNormalization())\n",
    "    # add activation layer to add non-linearity\n",
    "    model.add(Activation(actfn))\n",
    "    #Add Dropout layer\n",
    "    model.add(Dropout(dropouts))\n",
    "    \n",
    "    # add a hidden layer\n",
    "    model.add(Dense(15, input_dim = len(X_train.columns)))\n",
    "    model.add(BatchNormalization())\n",
    "    # add activation layer to add non-linearity\n",
    "    model.add(Activation(actfn))\n",
    "    #Add Dropout layer\n",
    "    model.add(Dropout(dropouts))\n",
    "    \n",
    "    # add a hidden layer\n",
    "    model.add(Dense(45, input_dim = len(X_train.columns)))\n",
    "    model.add(BatchNormalization())\n",
    "    # add activation layer to add non-linearity\n",
    "    model.add(Activation(actfn))\n",
    "    #Add Dropout layer\n",
    "    model.add(Dropout(dropouts))\n",
    "    \n",
    "    \n",
    "    # add output layer\n",
    "    model.add(Dense(1))\n",
    "    # add softmax layer \n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    # Compiling the model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_deep_nn3_fn(dropouts, actfn, optimizer):\n",
    " \n",
    "    # initialize model\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # add a hidden layer\n",
    "    model.add(Dense(40, input_dim = len(X_train.columns)))\n",
    "    model.add(BatchNormalization())\n",
    "    # add activation layer to add non-linearity\n",
    "    model.add(Activation(actfn))\n",
    "    #Add Dropout layer\n",
    "    model.add(Dropout(dropouts))\n",
    "    \n",
    "    # add a hidden layer\n",
    "    model.add(Dense(50, input_dim = len(X_train.columns)))\n",
    "    model.add(BatchNormalization())\n",
    "    # add activation layer to add non-linearity\n",
    "    model.add(Activation(actfn))\n",
    "    #Add Dropout layer\n",
    "    model.add(Dropout(dropouts))\n",
    "    \n",
    "    # add a hidden layer\n",
    "    model.add(Dense(40, input_dim = len(X_train.columns)))\n",
    "    model.add(BatchNormalization())\n",
    "    # add activation layer to add non-linearity\n",
    "    model.add(Activation(actfn))\n",
    "    #Add Dropout layer\n",
    "    model.add(Dropout(dropouts))\n",
    "    \n",
    "    \n",
    "    # add output layer\n",
    "    model.add(Dense(1))\n",
    "    # add softmax layer \n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_train_model_fn(X_train, y_train, X_test, y_test, num_epochs=1000):\n",
    "\n",
    "    data_list=[]\n",
    "\n",
    "    dropout_list=[0.4,0.5,0.6]\n",
    "    act_fnlist=['selu','tanh']\n",
    "    batch_size=[256, 512]\n",
    "    data_record=[]\n",
    "\n",
    "    fucntion_list=[build_deep_nn1_fn,build_deep_nn2_fn, build_deep_nn3_fn]\n",
    "    function_name=['deep_nn1bn','deep_nn2bn','deep_nn3bn']\n",
    "\n",
    "    optimizer_list=['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "    \n",
    "    for name, fn in zip(function_name,fucntion_list):\n",
    "        for actfn in act_fnlist:\n",
    "            for dropout in dropout_list:\n",
    "                for batch in batch_size:\n",
    "                    for opt in optimizer_list:\n",
    "                        data_record=[]\n",
    "                        deep_model=fn(dropout, actfn, opt)\n",
    "                        earlystop = EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=10, verbose=1, mode='auto')\n",
    "                        model_info = deep_model.fit(X_train, y_train, epochs=num_epochs, validation_split=0.2,\n",
    "                                            verbose=0, callbacks=[tfdocs.modeling.EpochDots(), earlystop],  batch_size=batch)\n",
    "                        model_hist=pd.DataFrame(model_info.history)\n",
    "                        model_hist['epochs']=model_info.epoch\n",
    "                        model_hist=model_hist.sort_values(by='val_accuracy', ascending=False)\n",
    "                        model_hist.reset_index(drop=True, inplace=True)\n",
    "                        _, train_acc = deep_model.evaluate(X_train, y_train, verbose=1)\n",
    "                        _, test_acc = deep_model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "                        print(\"\\nActivation - \", actfn)\n",
    "                        print(\"\\nDropout - \", dropout)\n",
    "                        print(\"\\nBatch size - \", batch)\n",
    "                        print (\"\\nOptimizer - \", opt)\n",
    "                        print(\"\\nModel summary - \\n\", deep_model.summary())\n",
    "                        print(model_hist.head())\n",
    "                        print (\"\\nTrain accuracy - \", train_acc)\n",
    "                        print (\"\\nTest accuracy - \", test_acc)\n",
    "                        plt.plot(model_info.history['loss'], label='train')\n",
    "                        plt.plot(model_info.history['val_loss'], label='test')\n",
    "                        plt.legend()\n",
    "                        plt.title(\"Validation Loss\")\n",
    "                        plt.show()\n",
    "                        print(\"=========================================\")\n",
    "                        data_record.append(name)\n",
    "                        data_record.append(actfn)\n",
    "                        data_record.append(dropout)\n",
    "                        data_record.append(batch)\n",
    "                        data_record.append(opt)\n",
    "                        data_record.append(model_hist.loc[0,'accuracy'])\n",
    "                        data_record.append(model_hist.loc[0,'val_accuracy'])\n",
    "                        data_record.append(model_hist.loc[0,'val_loss'])\n",
    "                        data_record.append(train_acc)\n",
    "                        data_record.append(test_acc)\n",
    "                        data_record.append(deep_model.summary())\n",
    "                        data_record.append(model_hist.loc[0,'epochs'])\n",
    "                        data_record.append(num_epochs)\n",
    "                        data_list.append(tuple(data_record))\n",
    "                        print(\"*************************************************************************************************************************\")\n",
    "                        print(\"*************************************************************************************************************************\")\n",
    "\n",
    "                        print(\"\\n\")\n",
    "\n",
    "    deep_model_data=pd.DataFrame(data_list, columns=['model','activation', 'dropout', 'batchsize', 'Optimizer', 'trainaccuracy', 'testaccuracy', \n",
    "                                          'loss', 'meantrainaccuracy', 'meantestaccuracy', 'summary', 'epoch', 'epochs'])\n",
    "    return deep_model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_500_opt1=build_train_model_fn(X_train_norm11, y_train, X_test_norm11, y_test, 500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_500_opt1.sort_values(by='testaccuracy', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_500_opt1.to_csv('Dataframe_500_opt1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_500_opt2=build_train_model_fn(X_train_norm14, y_train, X_test_norm14, y_test, 500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_500_opt2.sort_values(by='testaccuracy', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_500_opt2.to_csv('Dataframe_500_opt2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So. after analyzing, the best model is :\n",
    " - 3 hidden layers of 60,40 and 20 neurons each\n",
    " - activation - tanh\n",
    " - dropout - 0.4 or 0.6\n",
    " - optimizer - Adam\n",
    " - Batch size - 256 or 512\n",
    " - Epoch - 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section1608></a>\n",
    "### **16.8 Selecting the final parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_final_model(dropouts):\n",
    "    \n",
    "    # initialize model\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # add a hidden layer\n",
    "    model.add(Dense(60, input_dim = len(X_train.columns)))\n",
    "    model.add(BatchNormalization())\n",
    "    # add activation layer to add non-linearity\n",
    "    model.add(Activation('tanh'))\n",
    "    #Add Dropout layer\n",
    "    model.add(Dropout(dropouts))\n",
    "    \n",
    "    # add a hidden layer\n",
    "    model.add(Dense(40, input_dim = len(X_train.columns)))\n",
    "    model.add(BatchNormalization())\n",
    "    # add activation layer to add non-linearity\n",
    "    model.add(Activation('tanh'))\n",
    "    #Add Dropout layer\n",
    "    model.add(Dropout(dropouts))\n",
    "    \n",
    "    # add a hidden layer\n",
    "    model.add(Dense(20, input_dim = len(X_train.columns)))\n",
    "    model.add(BatchNormalization())\n",
    "    # add activation layer to add non-linearity\n",
    "    model.add(Activation('tanh'))\n",
    "    #Add Dropout layer\n",
    "    model.add(Dropout(dropouts))\n",
    "    \n",
    "    \n",
    "    # add output layer\n",
    "    model.add(Dense(1))\n",
    "    # add softmax layer \n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model=KerasClassifier(build_fn=build_final_model, verbose=1, epochs=500)\n",
    "batch_size=[256,512]\n",
    "dropouts=[0.4,0.6]\n",
    "param_grid=dict(batch_size=batch_size, dropouts=dropouts)\n",
    "grid=RandomizedSearchCV(estimator=final_model, param_distributions=param_grid, n_jobs=-1, cv=10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_result=grid.fit(X_train_norm11, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_result.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_result.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_result1=grid.fit(X_train_norm14, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_result1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_result1.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, final model would be -\n",
    " - Data scaled using StandardScaler\n",
    " - 3 hidden layers of 60,40 and 20 neurons each\n",
    " - activation - tanh\n",
    " - dropout - 0.4\n",
    " - optimizer - Adam\n",
    " - Batch size - 512\n",
    " - Epoch - 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section1609></a>\n",
    "### **16.9 Final Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Data scaled using StandardScaler\n",
    " - 3 hidden layers of 60,40 and 20 neurons each\n",
    " - activation - tanh\n",
    " - dropout - 0.4\n",
    " - optimizer - Adam\n",
    " - Batch size - 512\n",
    " - Epoch - 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_finalized_model():\n",
    "    \n",
    "    # initialize model\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # add a hidden layer\n",
    "    model.add(Dense(60, input_dim = len(X_train.columns)))\n",
    "    model.add(BatchNormalization())\n",
    "    # add activation layer to add non-linearity\n",
    "    model.add(Activation('tanh'))\n",
    "    #Add Dropout layer\n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    # add a hidden layer\n",
    "    model.add(Dense(40, input_dim = len(X_train.columns)))\n",
    "    model.add(BatchNormalization())\n",
    "    # add activation layer to add non-linearity\n",
    "    model.add(Activation('tanh'))\n",
    "    #Add Dropout layer\n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    # add a hidden layer\n",
    "    model.add(Dense(20, input_dim = len(X_train.columns)))\n",
    "    model.add(BatchNormalization())\n",
    "    # add activation layer to add non-linearity\n",
    "    model.add(Activation('tanh'))\n",
    "    #Add Dropout layer\n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    \n",
    "    # add output layer\n",
    "    model.add(Dense(1))\n",
    "    # add softmax layer \n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy',tf.keras.metrics.AUC()])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalized_model=build_finalized_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalized_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystop = EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=10, verbose=1, mode='auto')\n",
    "final_model_info = finalized_model.fit(X_train_norm11, y_train, epochs=500, validation_split=0.2,verbose=0, callbacks=[tfdocs.modeling.EpochDots(), earlystop], batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame(final_model_info.history)\n",
    "final_df['epoch'] = final_model_info.epoch\n",
    "final_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = tfdocs.plots.HistoryPlotter(smoothing_std=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter.plot({'Basic': final_model_info}, metric=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter.plot({'Basic': final_model_info}, metric=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=finalized_model.evaluate(X_test_norm11, y_test, verbose=2)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = finalized_model.predict_classes(X_test_norm11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(zip(y_test, test_predictions.flatten()), columns=['Actual','Predict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary on final model from DL**\n",
    "\n",
    " - 3 hidden layers of 60,40 and 20 neurons each\n",
    " - activation - tanh\n",
    " - dropout - 0.4\n",
    " - optimizer - Adam\n",
    " - Batch size - 512\n",
    " - Epoch - 500\n",
    "\n",
    "**Training accuracy** - : 0.803119\n",
    "\n",
    "**Test accuracy** - : 0.8027\n",
    "\n",
    "**AUC Score** - : 0.8513"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section17></a>\n",
    "## **17. Comparison of Machine Learning Model results and Deep Learning results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From Machine Learning models got below values**\n",
    "\n",
    "- Model - Stacking Classifier build on Random Forest, Gradient Boosting Classifier, Xtreme Gradient Boosting classifier and LightGBM\n",
    "\n",
    "- Training accuracy - : 0.8229196659948171\n",
    "\n",
    "- Test accuracy - : 0.8166986564299424\n",
    "\n",
    "- Recall Score - : 0.8166986564299424\n",
    "\n",
    "- Precison Score - : 0.8042796289150841\n",
    "\n",
    "- F1 Score - : 0.8059686365556438"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From Deep Learning models we got below values**\n",
    "\n",
    " - 3 hidden layers of 60,40 and 20 neurons each\n",
    " - activation - tanh\n",
    " - dropout - 0.4\n",
    " - optimizer - Adam\n",
    " - Batch size - 512\n",
    " - Epoch - 500\n",
    "\n",
    "**Training accuracy** - : 0.803119\n",
    "\n",
    "**Test accuracy** - : 0.8027\n",
    "\n",
    "**AUC Score** - : 0.8513"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracies values might be marginally higher from ML models but might be with more experiments with more configurations in DL may get better values.\n",
    "\n",
    "But if we consider time to train the final model, it was much lesser in DL model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section18></a>\n",
    "## **18. Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we consider tradeoff between accuracy and time, then we can prefer DL model but if accuracy is important then we can prefer ML model in this case as accuracy is marginally higher but time taken was lot higher than DL model in the given dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we can consider the below points for profitable advertisements\n",
    "\n",
    " - Pharma is most profitable industry in terms of advertisement.\n",
    " - Advertisements targeted towards male and especially male living with spouse are more profitable.\n",
    " - Comedy genre is most profitable and for married people and primetime is the most profitable to air the advertisement.\n",
    " - Comedy and Drama with runtime of 40-50 mins is more profitable across the World, but for all categoeries have significant count.\n",
    " - Most profitable run time for Comedy/Drama is 40 mins, followed by 50 mins, 45 mins and 60 mins.\n",
    " - Pharma is mostly profitable for Male audience while Auto and Entertainment for both Male and Female."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Advertise Success Rate.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0030ad715ac24635b2d555a75c3be88c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bbcb610b29194b608f10eb6df74603b2",
      "placeholder": "​",
      "style": "IPY_MODEL_bea86f53d46245e5b391e966d5482c3c",
      "value": " 1/1 [00:07&lt;00:00,  7.36s/it]"
     }
    },
    "0e7d1aa19a7641c6beacc8340bca6c42": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "132696e0e57d4d8a8fca1859584f08df": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "16a78573eba04e49bfda26b1f715e094": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "variables: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6fd4b7911d0d4d68ba8868218ffd8b9b",
      "max": 12,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_eef2d6095fa9421cbf0ce5552813f431",
      "value": 12
     }
    },
    "1762321b21344e2ea404531105d3d203": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "28ff5fec54ef4e06bf4e8926ca7e75fc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2b2ac5bc569d49a189ac28d82a67d2db": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2fb13bc8ffed4a9e985f5c6aeaf04e5f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "30a7391ec5d249a8b535e9df16f135d7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3ac11be1e9104601a5174c4bd17036a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "build report structure: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b19ea9c1fbd74a418bd08ba6c7e98f9f",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3c7745aed29147c4ad9c8a82649ffa23",
      "value": 1
     }
    },
    "3c7745aed29147c4ad9c8a82649ffa23": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "3cd9cc14e44043e68ac6cc89fb8e9410": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3f99c7d20557409f936d051222fd029e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "40d9e82b3546430cbc8833bf61f21f4f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a82bbb213a894ce38735d416be25a3d9",
       "IPY_MODEL_d0daa37114584d3b98a0c748fe6162ae"
      ],
      "layout": "IPY_MODEL_fcb0833e9fc74835bcd1edfe6bf347c4"
     }
    },
    "4e13cfe86fe44d259c0d620caee39111": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ec57249851504bccabb6ce0e9eb039e6",
       "IPY_MODEL_e23ffb8f230a4bd28ec02c9f0585aef3"
      ],
      "layout": "IPY_MODEL_d0a4ad0446ac455f820fa2779125aecd"
     }
    },
    "551ddbd09e284d289cd30c66baea1060": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "interactions [continuous]: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0e7d1aa19a7641c6beacc8340bca6c42",
      "max": 9,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_56eec5d6999b435eaa8da5d5df6193a4",
      "value": 9
     }
    },
    "56eec5d6999b435eaa8da5d5df6193a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "5cf2a3734f6a450dbfdbfe463c7415f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8bd7a420269a467dab4d7f683412d768",
      "placeholder": "​",
      "style": "IPY_MODEL_bbb800ebd2294c499c6234d7909ebce7",
      "value": " 9/9 [00:04&lt;00:00,  1.87it/s]"
     }
    },
    "5ebc77b88fc047b4bb996e5c95c8e454": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "610f8e5bece34ed8800850eb9db18b90": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "617fbb675a724e32ba2aa590200eda4e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_baf2f8fb4d304de8b6f88e72174e95ae",
       "IPY_MODEL_be1d9fc483454222bbf26b79c386bb09"
      ],
      "layout": "IPY_MODEL_cfd7637676f84eb58d8261dca47c1c04"
     }
    },
    "619ab18a8c1e4eb6a3613716843f7c7e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "61c6c2b1717a4024a3dd2706860e19af": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6238b75d5d8c45b1a4e1220308868316": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "68f61c364f91484f99af74adf868f6b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_30a7391ec5d249a8b535e9df16f135d7",
      "placeholder": "​",
      "style": "IPY_MODEL_132696e0e57d4d8a8fca1859584f08df",
      "value": " 3/3 [00:00&lt;00:00, 23.83it/s]"
     }
    },
    "6fd4b7911d0d4d68ba8868218ffd8b9b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "73d1aa4431014e5fa9c760868e400646": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7cd30bb9688c45ffbfb20ed8efe440e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_16a78573eba04e49bfda26b1f715e094",
       "IPY_MODEL_b5e4f2dfea4b40af8759e0aa5c1bf11c"
      ],
      "layout": "IPY_MODEL_b895a13004874030a8a3781281aba775"
     }
    },
    "7ec735aad1d24157917eb4413d659617": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "80189249d21e422e96e8496dfa0d6946": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "857e1e236fc0460ca3b2a75ef3deebdd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "8bd7a420269a467dab4d7f683412d768": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9470e3ca2e64488a98c709e11cd76ae1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "985f06a377f3487c87c0aacfe0e19977": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "99611965e4a3426bb6f033df5604fd0e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_551ddbd09e284d289cd30c66baea1060",
       "IPY_MODEL_5cf2a3734f6a450dbfdbfe463c7415f6"
      ],
      "layout": "IPY_MODEL_e907020be5fe4c1391b88e6bfc8faec3"
     }
    },
    "a2669a858bea40ab9e1930f2a3910f5d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a82bbb213a894ce38735d416be25a3d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "table: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_28ff5fec54ef4e06bf4e8926ca7e75fc",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c1725ba4121c4fa8a20e2a6bcd4c5c6a",
      "value": 1
     }
    },
    "b19ea9c1fbd74a418bd08ba6c7e98f9f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b1dac2ceeb154a0ca1b92ee6a5774cd8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b5e4f2dfea4b40af8759e0aa5c1bf11c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cf76cbb90caa4c0eb9cce0287eaccf2e",
      "placeholder": "​",
      "style": "IPY_MODEL_a2669a858bea40ab9e1930f2a3910f5d",
      "value": " 12/12 [00:31&lt;00:00,  2.65s/it]"
     }
    },
    "b895a13004874030a8a3781281aba775": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "baeec67fc5fd458d9ee6b1e89a7e6023": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_dd2b9f16df4f4376998696337d726459",
       "IPY_MODEL_f37f866eddd3421e91096ae56a77f760"
      ],
      "layout": "IPY_MODEL_9470e3ca2e64488a98c709e11cd76ae1"
     }
    },
    "baf2f8fb4d304de8b6f88e72174e95ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "missing [matrix]: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_73d1aa4431014e5fa9c760868e400646",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_857e1e236fc0460ca3b2a75ef3deebdd",
      "value": 2
     }
    },
    "bbb800ebd2294c499c6234d7909ebce7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bbcb610b29194b608f10eb6df74603b2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "be1d9fc483454222bbf26b79c386bb09": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_61c6c2b1717a4024a3dd2706860e19af",
      "placeholder": "​",
      "style": "IPY_MODEL_e1f206db22574dfabad4a8ccd36a3102",
      "value": " 2/2 [00:03&lt;00:00,  1.54s/it]"
     }
    },
    "bea86f53d46245e5b391e966d5482c3c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c042f8b98ebe45eebddc03a6d251af50": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3ac11be1e9104601a5174c4bd17036a5",
       "IPY_MODEL_0030ad715ac24635b2d555a75c3be88c"
      ],
      "layout": "IPY_MODEL_610f8e5bece34ed8800850eb9db18b90"
     }
    },
    "c1725ba4121c4fa8a20e2a6bcd4c5c6a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "ceb4385988f947a88532afc7c2583255": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "warnings [correlations]: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_985f06a377f3487c87c0aacfe0e19977",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3f99c7d20557409f936d051222fd029e",
      "value": 3
     }
    },
    "cf76cbb90caa4c0eb9cce0287eaccf2e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cfd7637676f84eb58d8261dca47c1c04": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d0a4ad0446ac455f820fa2779125aecd": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d0daa37114584d3b98a0c748fe6162ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2b2ac5bc569d49a189ac28d82a67d2db",
      "placeholder": "​",
      "style": "IPY_MODEL_5ebc77b88fc047b4bb996e5c95c8e454",
      "value": " 1/1 [00:03&lt;00:00,  3.27s/it]"
     }
    },
    "dd2b9f16df4f4376998696337d726459": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "package: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1762321b21344e2ea404531105d3d203",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2fb13bc8ffed4a9e985f5c6aeaf04e5f",
      "value": 1
     }
    },
    "e1f206db22574dfabad4a8ccd36a3102": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e23ffb8f230a4bd28ec02c9f0585aef3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b1dac2ceeb154a0ca1b92ee6a5774cd8",
      "placeholder": "​",
      "style": "IPY_MODEL_80189249d21e422e96e8496dfa0d6946",
      "value": " 6/6 [00:08&lt;00:00,  1.38s/it]"
     }
    },
    "e907020be5fe4c1391b88e6bfc8faec3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ec57249851504bccabb6ce0e9eb039e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "correlations [recoded]: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7ec735aad1d24157917eb4413d659617",
      "max": 6,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_619ab18a8c1e4eb6a3613716843f7c7e",
      "value": 6
     }
    },
    "eef2d6095fa9421cbf0ce5552813f431": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "efa59afe025f4e39baec14ae175dd78d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f0e83e483d55455587416a0989538944": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ceb4385988f947a88532afc7c2583255",
       "IPY_MODEL_68f61c364f91484f99af74adf868f6b8"
      ],
      "layout": "IPY_MODEL_efa59afe025f4e39baec14ae175dd78d"
     }
    },
    "f37f866eddd3421e91096ae56a77f760": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6238b75d5d8c45b1a4e1220308868316",
      "placeholder": "​",
      "style": "IPY_MODEL_3cd9cc14e44043e68ac6cc89fb8e9410",
      "value": " 1/1 [00:00&lt;00:00,  3.49it/s]"
     }
    },
    "fcb0833e9fc74835bcd1edfe6bf347c4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
